{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "36db6533",
   "metadata": {},
   "source": [
    "# Implementing Long-Term Memory in LangGraph\n",
    "\n",
    "> **Learning Outcomes:**\n",
    "> - Understand the concept and mechanisms of Long-Term Memory\n",
    "> - Implement Long-Term Memory in LangGraph\n",
    "> - Extend the implementation to support consolidation\n",
    "\n",
    "## Introduction\n",
    "\n",
    "In this lab, we will implement long-term memory in LangGraph. Long-Term memory is a mechanism that allows the model to remember information from the past. This is particularly useful in tasks that require the model to remember information from the past interactions to tailor their responses to the user.\n",
    "\n",
    "### Memory in Humans\n",
    "\n",
    "In humans, memory has many different components and processes. These include:\n",
    "\n",
    "1. *Encoding:* Converting information into a form that can be stored in memory.\n",
    "2. *Storage:* Storing information in memory.\n",
    "3. *Recall:* Passively remembering information.\n",
    "4. *Retrieval:* Actively trying to remember information.\n",
    "5. *Forgetting:* The loss of information over time.\n",
    "6. *Consolidation:* Stabilizing and organizing memories over time.\n",
    "\n",
    "### The Scenario\n",
    "\n",
    "We will be creating a chatbot that can remember information from past interactions. The chatbot will be able to selectively remember information that is important and forget information that is not. The chatbot will also be able to consolidate memories to make them more stable and organized.\n",
    "\n",
    "The graph will look like this:\n",
    "\n",
    "```mermaid\n",
    "stateDiagram\n",
    "    [*] --> Recall\n",
    "    Recall --> ChatBot\n",
    "    ChatBot --> Encode\n",
    "    Encode --> Consolidate\n",
    "    Consolidate --> Chat\n",
    "```\n",
    "\n",
    "## Getting Started\n",
    "\n",
    "Let's start by installing the required libraries and setting the OpenAI API key. The OpenAI API key is required to access the OpenAI models.\n",
    "\n",
    "Run the following cells to install the required libraries and set the OpenAI API key."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d96a7c5a-d7eb-4013-904c-23356601a7d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install --upgrade pip setuptools wheel\n",
    "%pip install tiktoken --only-binary=:all:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d13ffdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install -qU langchain==0.3.* langchain_openai==0.3.* langgraph==0.5.*\n",
    "\n",
    "import os\n",
    "import getpass\n",
    "import uuid\n",
    "import textwrap\n",
    "from typing import Annotated\n",
    "\n",
    "if 'OPENAI_API_KEY' not in os.environ:\n",
    "    os.environ['OPENAI_API_KEY'] = getpass.getpass(\"Enter your OpenAI API key: \")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9992a4f5",
   "metadata": {},
   "source": [
    "## Core\n",
    "\n",
    "### Step 1 - Building the State\n",
    "\n",
    "We will start by building the state of our graph. The state will contain the messages from the user and the chatbot. The state will also contain retrieved memories from the long-term memory.\n",
    "\n",
    "We will use the `TypedDict` class from the `typing_extensions` module to define the state. This will allow us to specify the types of the state variables. We will also use the `Annotated` class to add metadata to the state variables. This includes the \"add_messages\" metadata for the `messages` field, which specifies that new messages should be added to the existing messages.\n",
    "\n",
    "```python\n",
    "class MessagesState(TypedDict):\n",
    "    messages: Annotated[list[AnyMessage], add_messages]\n",
    "```\n",
    "\n",
    "In the follow cell, define a `ChatbotState` class with the following fields:\n",
    "\n",
    "- `messages`: A list of messages from the user and the chatbot. \n",
    "- `memories`: A list of strings representing the memories stored in the long-term memory.\n",
    "\n",
    "```python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a561c61f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing_extensions import TypedDict\n",
    "from langchain_core.messages import AnyMessage\n",
    "from langgraph.graph import add_messages\n",
    "\n",
    "# YOUR CODE HERE\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1483a248",
   "metadata": {},
   "source": [
    "This is how LangGraph will pass data between nodes in the graph. As the data moves through the graph, it will be stored in a typed dictionary. Fields can be added or updated as the data moves through the graph.\n",
    "\n",
    "### Step 2 - Setting Up the Memory Store\n",
    "\n",
    "Next, we will set up the memory store for the long-term memory. The memory store will use LangGraph's `InMemoryStore`. This memory doesn't refer to \"memory\", but to RAM. So, it will reset and lose all data when the program is restarted.\n",
    "\n",
    "We will also create a `Memories` class that will allow us to structure the outputs returned from a model and make it easier to work with the memories.\n",
    "\n",
    "Run the following cell to define both."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06291986",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.store.memory import InMemoryStore\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from pydantic import BaseModel\n",
    "\n",
    "in_memory_store = InMemoryStore(\n",
    "    index={\n",
    "        \"embed\": OpenAIEmbeddings(model=\"text-embedding-3-small\"),\n",
    "        \"dims\": 1536,\n",
    "    }\n",
    ")\n",
    "\n",
    "class Memories(BaseModel):\n",
    "    list: list[str]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a79e936b",
   "metadata": {},
   "source": [
    "### Step 3 - Implementing the Nodes\n",
    "\n",
    "Now, we will implement the nodes for our graph. To make working with our memory store easier, we will first define a helper function named `get_namespace` that will return a namespace for the memory store.\n",
    "\n",
    "Run the following cell to define the `get_namespace` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2aeba41c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.runnables import RunnableConfig\n",
    "\n",
    "def get_namespace(config: RunnableConfig):\n",
    "    return (\"memories\", config[\"configurable\"][\"user_id\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ed62ce7",
   "metadata": {},
   "source": [
    "#### Recall Node\n",
    "\n",
    "Our first node will be the `Recall` node. This node will retrieve memories from the long-term memory store.\n",
    "\n",
    "You can retrieve memories from the memory store using the `search` method of the memory store. The `get` method takes the namespace and a query as arguments. The query can be a string or a dictionary.\n",
    "\n",
    "```python\n",
    "results = store.search(namespace, query)\n",
    "results[0].value[\"data\"] # <-- This will give you the memory\n",
    "```\n",
    "\n",
    "To simulate passive recall of memories, we will use the user's last message as the query to retrieve memories related to the user's last message.\n",
    "\n",
    "```python\n",
    "last_message = state[\"messages\"][-1].content\n",
    "```\n",
    "\n",
    "In the following cell, implement the `Recall` node. The node should retrieve memories related to the user's last message and update the memroies in the state."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1721b241",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.store.base import BaseStore\n",
    "\n",
    "def recall(state: ChatbotState, config: RunnableConfig, *, store: BaseStore):\n",
    "    # YOUR CODE HERE\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4389d15",
   "metadata": {},
   "source": [
    "#### Chatbot Node\n",
    "\n",
    "Our next node will be the `ChatBot` node. This node will generate a response from the chatbot.\n",
    "\n",
    "In the following cell, write a system message that describes the purpose of the chatbot and add the memories to it. Then invoke the model with the system message and the state messages.\n",
    "\n",
    "Here is a brief example that does not include the memories:\n",
    "\n",
    "```python\n",
    "model.invoke(\n",
    "    [{\"role\": \"system\", \"content\": \"I am a chatbot that can remember information from past interactions.\"}]\n",
    "    + state[\"messages\"]\n",
    ")\n",
    "```\n",
    "\n",
    "Make sure to return the response to add it to the messages!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2796d6a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "model = ChatOpenAI(model=\"gpt-4.1\")\n",
    "\n",
    "def chatbot(state: ChatbotState, config: RunnableConfig, *, store: BaseStore):\n",
    "    # YOUR CODE HERE\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfd7cd5c",
   "metadata": {},
   "source": [
    "#### Encode Node\n",
    "Next, we will implement the `Encode` node. This node will encode the chatbot's response and store it in the long-term memory.\n",
    "\n",
    "This node will function similarly to the chatbot mode. However, it should have an additional user message that asks the model to make a list of things to remember. It should return the `Memories` object with the memories to store. To store the memories, you can use the the following code:\n",
    "\n",
    "```python\n",
    "store.put(namespace, str(uuid.uuid4()), {\"data\": memory})\n",
    "```\n",
    "\n",
    "In the following cell, implement the `Encode` node."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6512328a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode(state: ChatbotState, config: RunnableConfig, *, store: BaseStore):\n",
    "    # Pre-format the memories string to avoid f-string backslash error\n",
    "    existing_memories_str = \"\\n\".join(state[\"memory\"])\n",
    "\n",
    "    memories = model.with_structured_output(Memories).invoke([\n",
    "        {\n",
    "            \"role\": \"system\",\n",
    "            \"content\": textwrap.dedent(\"\"\"\n",
    "                You are a bag person for the assistant.\n",
    "                Help the assistant remember important information.\n",
    "                Format memories as declarative sentence fragments.\n",
    "                Do not include already known info.\n",
    "            \"\"\").strip()\n",
    "        }\n",
    "    ] + state[\"messages\"][-2:] + [\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": textwrap.dedent(f\"\"\"\n",
    "                # Memories\n",
    "                {existing_memories_str}\n",
    "\n",
    "                Make a list of things to remember.\n",
    "            \"\"\")\n",
    "        }\n",
    "    ])\n",
    "\n",
    "    namespace = get_namespace(config)\n",
    "\n",
    "    for memory in memories.list:\n",
    "        print(\"memory:\", memory)\n",
    "        store.put(namespace, str(uuid.uuid4()), {\"data\": memory})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a8263e8",
   "metadata": {},
   "source": [
    "#### Consolidate Node\n",
    "\n",
    "Our final node will be the `Consolidate` node. This node will consolidate the memories in the long-term memory store. We are going to use the internals of the memory store to make it easier to retrieve and delete all the memories.\n",
    "\n",
    "```python\n",
    "store._data[namespace] # <-- This will give you all the memories\n",
    "del store._data[namespace] # <-- This will delete all the memories\n",
    "```\n",
    "\n",
    "In the cell below, implement the `Consolidate` node. The node should consolidate the memories in the long-term memory store and store the consolidated memories in the state.\n",
    "\n",
    "In order to prevent this node from running every time, add a condition to run this node only if there are more than 10 memories in the memory store. Limit the number of memories to 10."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b41e0623",
   "metadata": {},
   "outputs": [],
   "source": [
    "def consolidate(state: ChatbotState, config: RunnableConfig, *, store: BaseStore):\n",
    "    namespace = get_namespace(config)\n",
    "    all_memories = store._data[namespace]\n",
    "\n",
    "    # YOUR CODE HERE\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11fc5c65",
   "metadata": {},
   "source": [
    "### Step 4 - Building the Graph\n",
    "\n",
    "Now we can build the graph using the nodes we implemented. The graph is fairly straightforward and links the nodes in the following order: Recall -> ChatBot -> Encode -> Consolidate. As a reminder, this is how you can build the graph:\n",
    "\n",
    "```python\n",
    "builder = StateGraph()\n",
    "builder.add_node(\"node\", function)\n",
    "builder.add_edge(START, \"node\")\n",
    "builder.add_edge(\"node\", END)\n",
    "```\n",
    "\n",
    "In the cell below, build the graph using the nodes we implemented. At the end, compile it using the following code:\n",
    "\n",
    "```python\n",
    "builder.compile(checkpointer=MemorySaver(), store=in_memory_store)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc4c7b3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.graph import StateGraph, START, END\n",
    "from langgraph.checkpoint.memory import MemorySaver\n",
    "\n",
    "builder = StateGraph(ChatbotState)\n",
    "# YOUR CODE HERE\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b55e69d0",
   "metadata": {},
   "source": [
    "### Step 5 - Running the Graph\n",
    "\n",
    "Finally, we can run the graph. Run the following cell to run the graph and see the chatbot in action!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba870587",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set a user ID and thread ID for the conversation. (Thread is the conversation, not the code thread.)\n",
    "thread_1 = {\"configurable\": {\"thread_id\": \"1\", \"user_id\": \"1\"}}\n",
    "thread_2 = {\"configurable\": {\"thread_id\": \"2\", \"user_id\": \"1\"}}\n",
    "\n",
    "# Start the conversation with a message from the user.\n",
    "input_message = {\n",
    "    \"role\": \"user\",\n",
    "    \"content\": \"Hi! I'm Bob.\"\n",
    "}\n",
    "\n",
    "def run_graph(input_message, config):\n",
    "    for chunk in graph.stream({\"messages\": [input_message]}, config, stream_mode=\"values\"):\n",
    "        final_state = chunk\n",
    "        print(chunk)\n",
    "\n",
    "    chunk[\"messages\"][-1].pretty_print()\n",
    "\n",
    "run_graph(input_message, thread_1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7ce352c",
   "metadata": {},
   "source": [
    "Now let's give the model a lot to remember!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38681ca0",
   "metadata": {},
   "outputs": [],
   "source": [
    "run_graph({\"role\": \"user\", \"content\": \"\"\"\n",
    "I prefer tea over coffee.\n",
    "I have a small scar on my left elbow.\n",
    "I enjoy solving crossword puzzles.\n",
    "I am afraid of heights.\n",
    "I can play the first few bars of \"FÃ¼r Elise\" on piano.\n",
    "I own a collection of vintage postcards.\n",
    "I have traveled to three different continents.\n",
    "I am a night owl.\n",
    "I love the smell of old books.\n",
    "I once won a local pie-baking contest.\n",
    "\n",
    "What should I bake this weekend?\n",
    "\"\"\"}, thread_1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3ef17fd",
   "metadata": {},
   "source": [
    "Run the cell below to see if the chatbot can remember the information across conversations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "561bfbd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "run_graph({\"role\": \"user\", \"content\": \"\"\"\n",
    "What do you remember about me?\n",
    "\"\"\"}, thread_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc9ccb71",
   "metadata": {},
   "source": [
    "## Bonus Challenge\n",
    "Feel free to experiment with the chatbot by sending different messages. You can also modify the code to add more features or improve the existing ones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d7cdbd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de3e91a2",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "In this lab, we implemented long-term memory in LangGraph. We created a chatbot that can remember information from past interactions and consolidate memories to make them more stable and organized. We also learned how to use the memory store to store and retrieve memories."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6966a773-dadb-4d08-b913-6f74d6c6b23d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
