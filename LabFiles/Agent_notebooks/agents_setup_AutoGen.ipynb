{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Microsoft AutoGen: Zero to Hero Guide\n",
    "\n",
    "## Building Conversational Multi-Agent Systems\n",
    "\n",
    "**Objective:** This comprehensive notebook takes you from beginner to advanced AutoGen user. You'll learn how to build single agents, multi-agent teams, and complex conversational workflows.\n",
    "\n",
    "**Target Audience:** Software engineers from complete beginners to experts looking to master AutoGen.\n",
    "\n",
    "---\n",
    "\n",
    "## Table of Contents\n",
    "1. [Introduction & Core Philosophy](#1-introduction--core-philosophy)\n",
    "2. [Prerequisites & Setup](#2-prerequisites--setup)\n",
    "3. [Core Concepts Deep Dive](#3-core-concepts-deep-dive)\n",
    "4. [Your First Agent: Basic Two-Agent Chat](#4-your-first-agent-basic-two-agent-chat)\n",
    "5. [Understanding Agent Configuration](#5-understanding-agent-configuration)\n",
    "6. [Custom System Messages & Personas](#6-custom-system-messages--personas)\n",
    "7. [Human-in-the-Loop Patterns](#7-human-in-the-loop-patterns)\n",
    "8. [Multi-Agent Group Chats](#8-multi-agent-group-chats)\n",
    "9. [Custom Termination Conditions](#9-custom-termination-conditions)\n",
    "10. [Function Calling & Tools](#10-function-calling--tools)\n",
    "11. [Nested Chats & Sequential Workflows](#11-nested-chats--sequential-workflows)\n",
    "12. [Best Practices & Common Pitfalls](#12-best-practices--common-pitfalls)\n",
    "13. [Conclusion & Next Steps](#13-conclusion--next-steps)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 1. Introduction & Core Philosophy\n",
    "\n",
    "### What is AutoGen?\n",
    "\n",
    "**AutoGen** is Microsoft's open-source framework for building multi-agent conversational AI systems. Unlike single-agent frameworks, AutoGen's core insight is that **complex tasks can be solved by a team of specialized agents having conversations**.\n",
    "\n",
    "### Core Philosophy\n",
    "\n",
    "| Principle | Description |\n",
    "|-----------|-------------|\n",
    "| **Conversational** | Agents solve problems through dialogue, not isolated actions |\n",
    "| **Collaborative** | Multiple agents with different roles work together |\n",
    "| **Flexible** | Supports human participation, code execution, and tool use |\n",
    "| **Composable** | Build complex systems from simple, reusable agent patterns |\n",
    "\n",
    "### When to Use AutoGen?\n",
    "\n",
    "‚úÖ **Good for:**\n",
    "- Multi-agent problem solving (coding, research, planning)\n",
    "- Tasks requiring iterative refinement through conversation\n",
    "- Human-in-the-loop workflows\n",
    "- Code generation and execution pipelines\n",
    "\n",
    "‚ùå **Consider alternatives when:**\n",
    "- You need simple single-agent tools (use LangChain)\n",
    "- You need complex state machines (use LangGraph)\n",
    "- You need role-playing crews with specific processes (use CrewAI)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 2. Prerequisites & Setup\n",
    "\n",
    "### Requirements\n",
    "\n",
    "Before starting, ensure you have:\n",
    "\n",
    "- **Python 3.8+** (3.10+ recommended)\n",
    "- **OpenAI API Key** (or compatible API)\n",
    "- Basic understanding of Python and LLMs\n",
    "\n",
    "### Installation\n",
    "\n",
    "```bash\n",
    "# Install AutoGen with all dependencies\n",
    "pip install pyautogen\n",
    "\n",
    "# For additional features (optional)\n",
    "pip install pyautogen[teachable]  # For teachable agents\n",
    "pip install pyautogen[redis]      # For Redis-backed memory\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install dependencies (uncomment to run)\n",
    "# !pip install pyautogen python-dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import warnings\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Suppress warnings for cleaner output\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Load environment variables from .env file\n",
    "load_dotenv()\n",
    "\n",
    "# Verify API key is available\n",
    "if not os.getenv(\"OPENAI_API_KEY\"):\n",
    "    print(\"‚ùå ERROR: OPENAI_API_KEY not found.\")\n",
    "    print(\"Please create a .env file with: OPENAI_API_KEY=your_key_here\")\n",
    "else:\n",
    "    print(\"‚úÖ API Key loaded successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 3. Core Concepts Deep Dive\n",
    "\n",
    "Before writing code, let's understand AutoGen's building blocks:\n",
    "\n",
    "### Agent Types\n",
    "\n",
    "```\n",
    "‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
    "‚îÇ                        AutoGen Agents                          ‚îÇ\n",
    "‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n",
    "‚îÇ   AssistantAgent        ‚îÇ   LLM-powered agent that generates    ‚îÇ\n",
    "‚îÇ                         ‚îÇ   responses, writes code, reasons     ‚îÇ\n",
    "‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n",
    "‚îÇ   UserProxyAgent        ‚îÇ   Proxy for human/automated actions   ‚îÇ\n",
    "‚îÇ                         ‚îÇ   Can execute code and provide input  ‚îÇ\n",
    "‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n",
    "‚îÇ   ConversableAgent      ‚îÇ   Base class for all agents           ‚îÇ\n",
    "‚îÇ                         ‚îÇ   Maximum customization flexibility   ‚îÇ\n",
    "‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n",
    "‚îÇ   GroupChatManager      ‚îÇ   Orchestrates multi-agent group      ‚îÇ\n",
    "‚îÇ                         ‚îÇ   conversations and turn-taking       ‚îÇ\n",
    "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
    "```\n",
    "\n",
    "### The Conversation Loop\n",
    "\n",
    "```\n",
    "User/Initiator\n",
    "      ‚îÇ\n",
    "      ‚ñº\n",
    "‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê     message      ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
    "‚îÇ   Agent A   ‚îÇ ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñ∂  ‚îÇ   Agent B   ‚îÇ\n",
    "‚îÇ (UserProxy) ‚îÇ                  ‚îÇ (Assistant) ‚îÇ\n",
    "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚óÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
    "      ‚îÇ              response           ‚îÇ\n",
    "      ‚îÇ                                 ‚îÇ\n",
    "      ‚îî‚îÄ‚îÄ Execute Code ‚óÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
    "              ‚îÇ\n",
    "              ‚ñº\n",
    "         Observation\n",
    "              ‚îÇ\n",
    "              ‚ñº\n",
    "      Continue or TERMINATE\n",
    "```\n",
    "\n",
    "### Key Configuration Parameters\n",
    "\n",
    "| Parameter | Description | Default |\n",
    "|-----------|-------------|--------|\n",
    "| `llm_config` | LLM model and API configuration | Required |\n",
    "| `system_message` | Agent's persona and instructions | Built-in default |\n",
    "| `human_input_mode` | When to ask for human input | \"ALWAYS\" |\n",
    "| `max_consecutive_auto_reply` | Auto-reply limit before stopping | 100 |\n",
    "| `code_execution_config` | How/where to run code | None |\n",
    "| `is_termination_msg` | Function to detect end of conversation | None |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 4. Your First Agent: Basic Two-Agent Chat\n",
    "\n",
    "Let's start with the simplest AutoGen pattern: a two-agent system where one agent writes code and another executes it.\n",
    "\n",
    "### The Pattern\n",
    "1. **UserProxyAgent** - Acts as the user, can execute code\n",
    "2. **AssistantAgent** - LLM-powered, writes responses/code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import autogen\n",
    "\n",
    "# Step 1: Configure the LLM\n",
    "# AutoGen uses a 'config_list' format for flexibility (supports multiple models/fallbacks)\n",
    "config_list = [\n",
    "    {\n",
    "        'model': 'gpt-4o',\n",
    "        'api_key': os.getenv(\"OPENAI_API_KEY\"),\n",
    "    }\n",
    "]\n",
    "\n",
    "# LLM configuration with additional parameters\n",
    "llm_config = {\n",
    "    \"config_list\": config_list,\n",
    "    \"temperature\": 0,  # More deterministic responses\n",
    "    \"timeout\": 120,    # Timeout in seconds\n",
    "}\n",
    "\n",
    "print(\"‚úÖ LLM Configuration ready!\")\n",
    "print(f\"   Model: {config_list[0]['model']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2: Create the Assistant Agent\n",
    "# This agent uses the LLM to generate responses\n",
    "\n",
    "assistant = autogen.AssistantAgent(\n",
    "    name=\"Assistant\",\n",
    "    llm_config=llm_config,\n",
    "    # Default system message is good for general coding tasks\n",
    ")\n",
    "\n",
    "print(\"‚úÖ Assistant Agent created!\")\n",
    "print(f\"   Name: {assistant.name}\")\n",
    "print(f\"   Type: AssistantAgent (LLM-powered)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3: Create the User Proxy Agent\n",
    "# This agent acts as the user and can execute code\n",
    "\n",
    "user_proxy = autogen.UserProxyAgent(\n",
    "    name=\"UserProxy\",\n",
    "    human_input_mode=\"NEVER\",  # Fully automated (no human input required)\n",
    "    max_consecutive_auto_reply=5,  # Limit conversation turns\n",
    "    is_termination_msg=lambda x: x.get(\"content\", \"\").rstrip().endswith(\"TERMINATE\"),\n",
    "    code_execution_config={\n",
    "        \"work_dir\": \"autogen_workspace\",  # Directory for code execution\n",
    "        \"use_docker\": False,  # Set True for sandboxed execution\n",
    "    },\n",
    ")\n",
    "\n",
    "print(\"‚úÖ User Proxy Agent created!\")\n",
    "print(f\"   Name: {user_proxy.name}\")\n",
    "print(f\"   Human Input Mode: NEVER (fully automated)\")\n",
    "print(f\"   Code Execution: Enabled in 'autogen_workspace/'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 4: Start the conversation!\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"üöÄ STARTING TWO-AGENT CONVERSATION\")\n",
    "print(\"=\"*60 + \"\\n\")\n",
    "\n",
    "# The user_proxy initiates the chat with a task\n",
    "result = user_proxy.initiate_chat(\n",
    "    assistant,\n",
    "    message=\"What is today's date? Write Python code to find and print it.\",\n",
    ")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"‚úÖ CONVERSATION COMPLETE\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 5: Analyze the conversation history\n",
    "\n",
    "print(\"\\nüìä CONVERSATION ANALYSIS\")\n",
    "print(\"-\" * 40)\n",
    "print(f\"Total messages: {len(result.chat_history)}\")\n",
    "print(f\"Summary: {result.summary[:200]}...\" if len(result.summary) > 200 else f\"Summary: {result.summary}\")\n",
    "print(f\"Cost: {result.cost}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üéØ Key Takeaways from Basic Example\n",
    "\n",
    "1. **AssistantAgent** generates code when asked\n",
    "2. **UserProxyAgent** automatically executes the code\n",
    "3. The conversation continues until `TERMINATE` is detected\n",
    "4. All code runs in the specified `work_dir`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 5. Understanding Agent Configuration\n",
    "\n",
    "Let's explore the key configuration options in detail."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demonstration of different LLM configurations\n",
    "\n",
    "# Configuration 1: Basic OpenAI\n",
    "config_basic = {\n",
    "    \"config_list\": [{\"model\": \"gpt-4o\", \"api_key\": os.getenv(\"OPENAI_API_KEY\")}],\n",
    "}\n",
    "\n",
    "# Configuration 2: With fallback models\n",
    "config_with_fallback = {\n",
    "    \"config_list\": [\n",
    "        {\"model\": \"gpt-4o\", \"api_key\": os.getenv(\"OPENAI_API_KEY\")},\n",
    "        {\"model\": \"gpt-4o-mini\", \"api_key\": os.getenv(\"OPENAI_API_KEY\")},  # Fallback\n",
    "    ],\n",
    "    \"timeout\": 60,\n",
    "}\n",
    "\n",
    "# Configuration 3: With caching (reduces API calls during development)\n",
    "config_with_cache = {\n",
    "    \"config_list\": [{\"model\": \"gpt-4o\", \"api_key\": os.getenv(\"OPENAI_API_KEY\")}],\n",
    "    \"cache_seed\": 42,  # Same seed = same cached responses\n",
    "}\n",
    "\n",
    "print(\"‚úÖ Configuration examples ready!\")\n",
    "print(\"   - Basic: Single model\")\n",
    "print(\"   - Fallback: Primary + backup model\")\n",
    "print(\"   - Cached: Repeatable results during development\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demonstration of Human Input Modes\n",
    "\n",
    "# Mode 1: NEVER - Fully automated\n",
    "auto_proxy = autogen.UserProxyAgent(\n",
    "    name=\"AutoProxy\",\n",
    "    human_input_mode=\"NEVER\",\n",
    "    code_execution_config={\"work_dir\": \"auto_workspace\", \"use_docker\": False},\n",
    ")\n",
    "\n",
    "# Mode 2: ALWAYS - Human approves every response\n",
    "interactive_proxy = autogen.UserProxyAgent(\n",
    "    name=\"InteractiveProxy\",\n",
    "    human_input_mode=\"ALWAYS\",  # Will prompt for input\n",
    "    code_execution_config={\"work_dir\": \"interactive_workspace\", \"use_docker\": False},\n",
    ")\n",
    "\n",
    "# Mode 3: TERMINATE - Human input only at termination\n",
    "semi_auto_proxy = autogen.UserProxyAgent(\n",
    "    name=\"SemiAutoProxy\",\n",
    "    human_input_mode=\"TERMINATE\",  # Auto until termination signal\n",
    "    code_execution_config={\"work_dir\": \"semiauto_workspace\", \"use_docker\": False},\n",
    ")\n",
    "\n",
    "print(\"\\nüìã HUMAN INPUT MODES\")\n",
    "print(\"-\" * 40)\n",
    "print(\"NEVER:     Fully automated, no human interaction\")\n",
    "print(\"ALWAYS:    Human approval required for each turn\")\n",
    "print(\"TERMINATE: Automated until conversation ends\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 6. Custom System Messages & Personas\n",
    "\n",
    "The `system_message` defines your agent's personality, expertise, and behavior. This is crucial for specialized agents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example 1: Python Expert Agent\n",
    "python_expert = autogen.AssistantAgent(\n",
    "    name=\"PythonExpert\",\n",
    "    system_message=\"\"\"You are a senior Python developer with 15 years of experience.\n",
    "    \n",
    "Your expertise includes:\n",
    "- Writing clean, PEP-8 compliant code\n",
    "- Performance optimization\n",
    "- Design patterns and best practices\n",
    "- Testing and debugging\n",
    "\n",
    "When writing code:\n",
    "1. Always include type hints\n",
    "2. Add comprehensive docstrings\n",
    "3. Handle edge cases and errors\n",
    "4. Suggest improvements when appropriate\n",
    "\n",
    "End your response with TERMINATE when the task is complete.\"\"\",\n",
    "    llm_config=llm_config,\n",
    ")\n",
    "\n",
    "print(\"‚úÖ Python Expert Agent created!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example 2: Code Reviewer Agent\n",
    "code_reviewer = autogen.AssistantAgent(\n",
    "    name=\"CodeReviewer\",\n",
    "    system_message=\"\"\"You are a meticulous code reviewer focused on quality and security.\n",
    "\n",
    "When reviewing code, evaluate:\n",
    "1. **Correctness**: Does the code do what it's supposed to?\n",
    "2. **Security**: Are there potential vulnerabilities?\n",
    "3. **Performance**: Can it be optimized?\n",
    "4. **Readability**: Is the code clean and maintainable?\n",
    "5. **Best Practices**: Does it follow conventions?\n",
    "\n",
    "Provide feedback in this format:\n",
    "- ‚úÖ APPROVED: If code meets all criteria\n",
    "- üîÑ REVISION NEEDED: If improvements are required (list them)\n",
    "- ‚ùå REJECTED: If there are critical issues\n",
    "\n",
    "Be constructive and specific in your feedback.\"\"\",\n",
    "    llm_config=llm_config,\n",
    ")\n",
    "\n",
    "print(\"‚úÖ Code Reviewer Agent created!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the Python Expert\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"üêç PYTHON EXPERT DEMONSTRATION\")\n",
    "print(\"=\"*60 + \"\\n\")\n",
    "\n",
    "result = user_proxy.initiate_chat(\n",
    "    python_expert,\n",
    "    message=\"Write a function to calculate the Fibonacci sequence up to n terms with memoization.\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 7. Human-in-the-Loop Patterns\n",
    "\n",
    "Real-world applications often need human oversight. AutoGen provides several patterns for this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pattern 1: Human Approval for Sensitive Operations\n",
    "\n",
    "human_admin = autogen.UserProxyAgent(\n",
    "    name=\"HumanAdmin\",\n",
    "    system_message=\"A human administrator who approves operations.\",\n",
    "    human_input_mode=\"ALWAYS\",  # Always ask for human input\n",
    "    max_consecutive_auto_reply=0,  # Never auto-reply\n",
    "    code_execution_config=False,  # No code execution\n",
    ")\n",
    "\n",
    "print(\"‚úÖ Human Admin Agent created!\")\n",
    "print(\"   This agent always waits for human input before proceeding.\")\n",
    "print(\"   Use for: Approvals, sensitive decisions, quality gates\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pattern 2: Human Override with Auto-Continue\n",
    "\n",
    "def should_request_human_input(message):\n",
    "    \"\"\"Custom logic to determine when human input is needed.\"\"\"\n",
    "    keywords = [\"delete\", \"deploy\", \"production\", \"database\", \"critical\"]\n",
    "    content = message.get(\"content\", \"\").lower()\n",
    "    return any(keyword in content for keyword in keywords)\n",
    "\n",
    "# Note: This is a conceptual example - actual implementation may vary\n",
    "print(\"‚úÖ Custom human input logic defined!\")\n",
    "print(\"   Triggers on keywords: delete, deploy, production, database, critical\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 8. Multi-Agent Group Chats\n",
    "\n",
    "AutoGen's `GroupChat` enables multiple agents to collaborate on complex tasks. This is where AutoGen truly shines!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a development team with specialized roles\n",
    "\n",
    "# Product Manager\n",
    "product_manager = autogen.AssistantAgent(\n",
    "    name=\"ProductManager\",\n",
    "    system_message=\"\"\"You are a Product Manager who breaks down requirements.\n",
    "    \n",
    "Your responsibilities:\n",
    "- Clarify user requirements\n",
    "- Define acceptance criteria\n",
    "- Prioritize features\n",
    "- Ensure the team understands the goal\n",
    "\n",
    "Keep your responses concise and actionable.\"\"\",\n",
    "    llm_config=llm_config,\n",
    ")\n",
    "\n",
    "# Developer\n",
    "developer = autogen.AssistantAgent(\n",
    "    name=\"Developer\",\n",
    "    system_message=\"\"\"You are a Senior Developer who writes clean, efficient code.\n",
    "    \n",
    "Your responsibilities:\n",
    "- Write production-quality code\n",
    "- Follow best practices and design patterns\n",
    "- Consider edge cases and error handling\n",
    "- Document your code\n",
    "\n",
    "Always provide working code, not pseudocode.\"\"\",\n",
    "    llm_config=llm_config,\n",
    ")\n",
    "\n",
    "# QA Engineer\n",
    "qa_engineer = autogen.AssistantAgent(\n",
    "    name=\"QAEngineer\",\n",
    "    system_message=\"\"\"You are a QA Engineer who ensures code quality.\n",
    "    \n",
    "Your responsibilities:\n",
    "- Review code for bugs and issues\n",
    "- Write test cases\n",
    "- Verify edge cases are handled\n",
    "- Approve code with 'APPROVED' or request changes\n",
    "\n",
    "Be thorough but constructive in your feedback.\"\"\",\n",
    "    llm_config=llm_config,\n",
    ")\n",
    "\n",
    "# User Proxy (executes code and represents the user)\n",
    "executor = autogen.UserProxyAgent(\n",
    "    name=\"Executor\",\n",
    "    human_input_mode=\"NEVER\",\n",
    "    max_consecutive_auto_reply=3,\n",
    "    is_termination_msg=lambda x: \"APPROVED\" in x.get(\"content\", \"\"),\n",
    "    code_execution_config={\"work_dir\": \"team_workspace\", \"use_docker\": False},\n",
    ")\n",
    "\n",
    "print(\"‚úÖ Development Team created!\")\n",
    "print(\"   - ProductManager: Requirements & planning\")\n",
    "print(\"   - Developer: Code implementation\")\n",
    "print(\"   - QAEngineer: Review & testing\")\n",
    "print(\"   - Executor: Code execution\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up the Group Chat\n",
    "\n",
    "groupchat = autogen.GroupChat(\n",
    "    agents=[executor, product_manager, developer, qa_engineer],\n",
    "    messages=[],\n",
    "    max_round=12,  # Maximum conversation rounds\n",
    "    speaker_selection_method=\"auto\",  # LLM decides who speaks next\n",
    ")\n",
    "\n",
    "# The manager orchestrates the conversation\n",
    "manager = autogen.GroupChatManager(\n",
    "    groupchat=groupchat,\n",
    "    llm_config=llm_config,\n",
    ")\n",
    "\n",
    "print(\"‚úÖ Group Chat configured!\")\n",
    "print(f\"   Max rounds: {groupchat.max_round}\")\n",
    "print(f\"   Speaker selection: {groupchat.speaker_selection_method}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the Group Chat!\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"üë• DEVELOPMENT TEAM GROUP CHAT\")\n",
    "print(\"=\"*60 + \"\\n\")\n",
    "\n",
    "task = \"\"\"\n",
    "Build a Python function that validates email addresses.\n",
    "Requirements:\n",
    "- Check for valid email format\n",
    "- Handle common edge cases\n",
    "- Return True/False with optional error message\n",
    "\"\"\"\n",
    "\n",
    "result = executor.initiate_chat(\n",
    "    manager,\n",
    "    message=task,\n",
    ")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"‚úÖ GROUP CHAT COMPLETE\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 9. Custom Termination Conditions\n",
    "\n",
    "Control when conversations end with custom termination logic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example termination conditions\n",
    "\n",
    "# 1. Simple keyword termination\n",
    "def terminate_on_keyword(msg):\n",
    "    content = msg.get(\"content\", \"\")\n",
    "    return content.rstrip().endswith(\"TERMINATE\")\n",
    "\n",
    "# 2. Multiple termination keywords\n",
    "def terminate_on_multiple_keywords(msg):\n",
    "    content = msg.get(\"content\", \"\").upper()\n",
    "    keywords = [\"TERMINATE\", \"DONE\", \"COMPLETE\", \"APPROVED\"]\n",
    "    return any(keyword in content for keyword in keywords)\n",
    "\n",
    "# 3. Conditional termination (e.g., after successful execution)\n",
    "def terminate_on_success(msg):\n",
    "    content = msg.get(\"content\", \"\")\n",
    "    # Check for successful code execution\n",
    "    if \"exitcode: 0\" in content:\n",
    "        return True\n",
    "    if \"TERMINATE\" in content:\n",
    "        return True\n",
    "    return False\n",
    "\n",
    "# 4. Max cost termination (prevent runaway costs)\n",
    "class CostTracker:\n",
    "    def __init__(self, max_cost=1.0):\n",
    "        self.total_cost = 0\n",
    "        self.max_cost = max_cost\n",
    "    \n",
    "    def check_and_update(self, msg):\n",
    "        # This is simplified - actual implementation would track API costs\n",
    "        self.total_cost += 0.01  # Estimate per message\n",
    "        if self.total_cost >= self.max_cost:\n",
    "            return True\n",
    "        return \"TERMINATE\" in msg.get(\"content\", \"\")\n",
    "\n",
    "print(\"‚úÖ Termination condition examples defined!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demonstrate custom termination\n",
    "\n",
    "smart_proxy = autogen.UserProxyAgent(\n",
    "    name=\"SmartProxy\",\n",
    "    human_input_mode=\"NEVER\",\n",
    "    max_consecutive_auto_reply=10,\n",
    "    is_termination_msg=terminate_on_success,  # Custom termination\n",
    "    code_execution_config={\"work_dir\": \"smart_workspace\", \"use_docker\": False},\n",
    ")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"üéØ CUSTOM TERMINATION DEMO\")\n",
    "print(\"=\"*60 + \"\\n\")\n",
    "\n",
    "result = smart_proxy.initiate_chat(\n",
    "    assistant,\n",
    "    message=\"Print 'Hello World' in Python. The code should run successfully.\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 10. Function Calling & Tools\n",
    "\n",
    "AutoGen agents can call custom Python functions as tools. This extends their capabilities beyond text generation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Annotated, Literal\n",
    "\n",
    "# Define custom tools/functions\n",
    "\n",
    "def get_weather(\n",
    "    city: Annotated[str, \"The city to get weather for\"],\n",
    ") -> str:\n",
    "    \"\"\"Get the current weather for a city.\"\"\"\n",
    "    # Simulated weather data\n",
    "    weather_data = {\n",
    "        \"new york\": \"72¬∞F, Partly Cloudy\",\n",
    "        \"london\": \"59¬∞F, Rainy\",\n",
    "        \"tokyo\": \"68¬∞F, Sunny\",\n",
    "        \"sydney\": \"77¬∞F, Clear\",\n",
    "    }\n",
    "    city_lower = city.lower()\n",
    "    return weather_data.get(city_lower, f\"Weather data not available for {city}\")\n",
    "\n",
    "def calculate(\n",
    "    operation: Annotated[Literal[\"add\", \"subtract\", \"multiply\", \"divide\"], \"The math operation\"],\n",
    "    a: Annotated[float, \"First number\"],\n",
    "    b: Annotated[float, \"Second number\"],\n",
    ") -> str:\n",
    "    \"\"\"Perform a mathematical calculation.\"\"\"\n",
    "    if operation == \"add\":\n",
    "        return str(a + b)\n",
    "    elif operation == \"subtract\":\n",
    "        return str(a - b)\n",
    "    elif operation == \"multiply\":\n",
    "        return str(a * b)\n",
    "    elif operation == \"divide\":\n",
    "        if b == 0:\n",
    "            return \"Error: Division by zero\"\n",
    "        return str(a / b)\n",
    "\n",
    "print(\"‚úÖ Custom tools defined: get_weather, calculate\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an agent with function calling capabilities\n",
    "\n",
    "tool_assistant = autogen.AssistantAgent(\n",
    "    name=\"ToolAssistant\",\n",
    "    system_message=\"\"\"You are a helpful assistant with access to weather and calculator tools.\n",
    "    Use the tools when appropriate to answer user questions.\n",
    "    Reply TERMINATE when the task is complete.\"\"\",\n",
    "    llm_config=llm_config,\n",
    ")\n",
    "\n",
    "tool_user = autogen.UserProxyAgent(\n",
    "    name=\"ToolUser\",\n",
    "    human_input_mode=\"NEVER\",\n",
    "    max_consecutive_auto_reply=5,\n",
    "    is_termination_msg=lambda x: \"TERMINATE\" in x.get(\"content\", \"\"),\n",
    "    code_execution_config=False,  # No code execution, just function calls\n",
    ")\n",
    "\n",
    "# Register the functions with both agents\n",
    "# The assistant decides when to call them, the user executes them\n",
    "tool_assistant.register_for_llm(name=\"get_weather\", description=\"Get weather for a city\")(get_weather)\n",
    "tool_assistant.register_for_llm(name=\"calculate\", description=\"Perform math calculations\")(calculate)\n",
    "\n",
    "tool_user.register_for_execution(name=\"get_weather\")(get_weather)\n",
    "tool_user.register_for_execution(name=\"calculate\")(calculate)\n",
    "\n",
    "print(\"‚úÖ Tool-enabled agents created!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test function calling\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"üîß FUNCTION CALLING DEMO\")\n",
    "print(\"=\"*60 + \"\\n\")\n",
    "\n",
    "result = tool_user.initiate_chat(\n",
    "    tool_assistant,\n",
    "    message=\"What's the weather in Tokyo? Also, what is 25 multiplied by 4?\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 11. Nested Chats & Sequential Workflows\n",
    "\n",
    "For complex workflows, you can chain conversations or nest them within each other."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sequential Workflow: Research -> Write -> Review\n",
    "\n",
    "researcher = autogen.AssistantAgent(\n",
    "    name=\"Researcher\",\n",
    "    system_message=\"\"\"You are a researcher who gathers information.\n",
    "    Summarize key points concisely. End with 'RESEARCH COMPLETE'.\"\"\",\n",
    "    llm_config=llm_config,\n",
    ")\n",
    "\n",
    "writer = autogen.AssistantAgent(\n",
    "    name=\"Writer\",\n",
    "    system_message=\"\"\"You are a technical writer.\n",
    "    Take research findings and create clear documentation.\n",
    "    End with 'WRITING COMPLETE'.\"\"\",\n",
    "    llm_config=llm_config,\n",
    ")\n",
    "\n",
    "editor = autogen.AssistantAgent(\n",
    "    name=\"Editor\",\n",
    "    system_message=\"\"\"You are an editor who reviews and improves content.\n",
    "    Provide the final polished version. End with 'TERMINATE'.\"\"\",\n",
    "    llm_config=llm_config,\n",
    ")\n",
    "\n",
    "print(\"‚úÖ Sequential workflow agents created!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run sequential workflow manually\n",
    "\n",
    "coordinator = autogen.UserProxyAgent(\n",
    "    name=\"Coordinator\",\n",
    "    human_input_mode=\"NEVER\",\n",
    "    max_consecutive_auto_reply=2,\n",
    "    code_execution_config=False,\n",
    ")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"üìù SEQUENTIAL WORKFLOW: Research -> Write -> Edit\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Step 1: Research\n",
    "print(\"\\n--- STEP 1: RESEARCH ---\")\n",
    "research_result = coordinator.initiate_chat(\n",
    "    researcher,\n",
    "    message=\"Research the benefits of using multi-agent AI systems in software development.\",\n",
    "    max_turns=2,\n",
    ")\n",
    "research_summary = research_result.chat_history[-1][\"content\"]\n",
    "\n",
    "# Step 2: Write\n",
    "print(\"\\n--- STEP 2: WRITE ---\")\n",
    "write_result = coordinator.initiate_chat(\n",
    "    writer,\n",
    "    message=f\"Based on this research, write a brief technical overview:\\n\\n{research_summary}\",\n",
    "    max_turns=2,\n",
    ")\n",
    "written_content = write_result.chat_history[-1][\"content\"]\n",
    "\n",
    "# Step 3: Edit\n",
    "print(\"\\n--- STEP 3: EDIT ---\")\n",
    "edit_result = coordinator.initiate_chat(\n",
    "    editor,\n",
    "    message=f\"Please review and polish this content:\\n\\n{written_content}\",\n",
    "    max_turns=2,\n",
    ")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"‚úÖ SEQUENTIAL WORKFLOW COMPLETE\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 12. Best Practices & Common Pitfalls\n",
    "\n",
    "### ‚úÖ Best Practices\n",
    "\n",
    "1. **Clear System Messages**: Be specific about roles, capabilities, and expected behavior\n",
    "2. **Appropriate Termination**: Always define clear termination conditions\n",
    "3. **Cost Management**: Use caching during development, set max_consecutive_auto_reply\n",
    "4. **Security**: Use Docker for code execution in production\n",
    "5. **Logging**: Enable verbose mode for debugging, disable in production\n",
    "\n",
    "### ‚ùå Common Pitfalls\n",
    "\n",
    "1. **Infinite Loops**: Forgetting termination conditions\n",
    "2. **Vague Roles**: Agents don't know when to act without clear system messages\n",
    "3. **Unsandboxed Execution**: Running untrusted code without Docker\n",
    "4. **No Fallbacks**: Not handling API errors or timeouts\n",
    "5. **Over-complexity**: Starting with too many agents instead of iterating"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Production-ready configuration template\n",
    "\n",
    "production_llm_config = {\n",
    "    \"config_list\": [\n",
    "        {\"model\": \"gpt-4o\", \"api_key\": os.getenv(\"OPENAI_API_KEY\")},\n",
    "        {\"model\": \"gpt-4o-mini\", \"api_key\": os.getenv(\"OPENAI_API_KEY\")},  # Fallback\n",
    "    ],\n",
    "    \"temperature\": 0,\n",
    "    \"timeout\": 120,\n",
    "    # \"cache_seed\": None,  # Disable caching in production\n",
    "}\n",
    "\n",
    "production_user_proxy = autogen.UserProxyAgent(\n",
    "    name=\"ProductionProxy\",\n",
    "    human_input_mode=\"TERMINATE\",\n",
    "    max_consecutive_auto_reply=10,\n",
    "    is_termination_msg=terminate_on_multiple_keywords,\n",
    "    code_execution_config={\n",
    "        \"work_dir\": \"production_workspace\",\n",
    "        \"use_docker\": True,  # Sandboxed execution\n",
    "        \"timeout\": 60,\n",
    "        \"last_n_messages\": 3,\n",
    "    },\n",
    ")\n",
    "\n",
    "print(\"‚úÖ Production-ready configuration template created!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 13. Conclusion & Next Steps\n",
    "\n",
    "### What You've Learned\n",
    "\n",
    "| Topic | Key Takeaway |\n",
    "|-------|-------------|\n",
    "| Core Concepts | AutoGen enables conversational multi-agent systems |\n",
    "| Agent Types | AssistantAgent (LLM) + UserProxyAgent (execution) |\n",
    "| Configuration | llm_config, system_message, human_input_mode |\n",
    "| Group Chats | Multiple agents collaborating with GroupChatManager |\n",
    "| Function Calling | Extend agents with custom Python tools |\n",
    "| Workflows | Sequential and nested conversation patterns |\n",
    "\n",
    "### Next Steps\n",
    "\n",
    "1. **Practice**: Build a multi-agent system for your use case\n",
    "2. **Explore**: Try AutoGen Studio for visual agent building\n",
    "3. **Compare**: See how LangGraph handles similar workflows differently\n",
    "4. **Production**: Implement proper error handling and monitoring\n",
    "\n",
    "### Resources\n",
    "\n",
    "- [AutoGen Documentation](https://microsoft.github.io/autogen/)\n",
    "- [AutoGen GitHub](https://github.com/microsoft/autogen)\n",
    "- [AutoGen Studio](https://microsoft.github.io/autogen/docs/autogen-studio/getting-started)\n",
    "\n",
    "---\n",
    "\n",
    "**Congratulations!** You've completed the AutoGen Zero to Hero guide! üéâ"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
