{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Agentic AI Implementation: GenAI Use-Case Evaluation & Recommendation Engine\n",
    "\n",
    "This notebook demonstrates a comprehensive agentic AI solution for evaluating whether a use case should use Generative AI, deterministic logic, or a hybrid approach. The implementation progressively applies all 12 lab techniques from the GAI-3101 course.\n",
    "\n",
    "## Use Case Overview\n",
    "\n",
    "**Problem**: AI architects spend ~2.75 hours per use case manually evaluating whether GenAI is appropriate, applying decision trees, and documenting recommendations.\n",
    "\n",
    "**Solution**: Multi-agent system that automates data collection, decision-tree evaluation, and recommendation report generation.\n",
    "\n",
    "**Business Impact**:\n",
    "- Lead time reduction: 13.75 hours → 4 hours (71% improvement)\n",
    "- Manual effort savings: 2.75 hours → 1 hour per use case (64% reduction)\n",
    "- Annual cost savings: $67,200 (40 evaluations/month at $80/hour expert rate)\n",
    "- ROI: Breakeven in Year 1, $47,200/year net benefit from Year 2\n",
    "\n",
    "## Workflow Steps\n",
    "\n",
    "1. **Collect Evaluation Data** (45 min → 15 min)\n",
    "2. **Apply Decision Tree & Draft Verdict** (60 min → 15 min)\n",
    "3. **Generate Recommendation & Documentation** (60 min → 30 min)\n",
    "\n",
    "## Lab Techniques Applied\n",
    "\n",
    "- **Lab 1**: Simple Python Agent\n",
    "- **Lab 2**: Round Robin Communication\n",
    "- **Lab 4**: Deliberative Agent (LangGraph)\n",
    "- **Lab 6**: Observation Tools\n",
    "- **Lab 7**: Action Tools\n",
    "- **Lab 8**: Hierarchical Planning\n",
    "- **Lab 9**: Rule-Based Reasoning\n",
    "- **Lab 12**: Error Recovery\n",
    "- **Lab 11**: Complete End-to-End System"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Environment Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages\n",
    "!pip install -q openai pyautogen langchain langchain-openai langgraph python-dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure OpenAI API Key\n",
    "import os\n",
    "from getpass import getpass\n",
    "\n",
    "if \"OPENAI_API_KEY\" not in os.environ:\n",
    "    os.environ[\"OPENAI_API_KEY\"] = getpass(\"Enter your OpenAI API key: \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize OpenAI client\n",
    "from openai import OpenAI\n",
    "import json\n",
    "from typing import Dict, List, Any, Optional\n",
    "from datetime import datetime\n",
    "from enum import Enum\n",
    "\n",
    "client = OpenAI(api_key=os.environ[\"OPENAI_API_KEY\"])\n",
    "print(\"✓ OpenAI client initialized\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define recommendation types\n",
    "class RecommendationType(Enum):\n",
    "    DETERMINISTIC = \"deterministic\"\n",
    "    GENAI = \"genai\"\n",
    "    HYBRID = \"hybrid\"\n",
    "\n",
    "# Sample use case submissions for testing\n",
    "sample_use_cases = [\n",
    "    {\n",
    "        \"use_case_id\": \"UC-2025-001\",\n",
    "        \"title\": \"Customer Email Response Generator\",\n",
    "        \"description\": \"Generate personalized email responses to customer inquiries based on their account status and query type.\",\n",
    "        \"submitted_by\": \"customer_service_team\",\n",
    "        \"submitted_at\": \"2025-11-25\",\n",
    "        \"input_data_types\": [\"customer_email_text\", \"account_data\", \"query_history\"],\n",
    "        \"expected_output\": \"Natural language email response\",\n",
    "        \"volume\": \"500 emails/day\",\n",
    "        \"compliance_requirements\": [\"GDPR\", \"brand_guidelines\"]\n",
    "    },\n",
    "    {\n",
    "        \"use_case_id\": \"UC-2025-002\",\n",
    "        \"title\": \"Invoice Validation System\",\n",
    "        \"description\": \"Validate incoming invoices against predefined business rules: check totals match line items, verify tax calculations, confirm vendor is approved.\",\n",
    "        \"submitted_by\": \"finance_team\",\n",
    "        \"submitted_at\": \"2025-11-25\",\n",
    "        \"input_data_types\": [\"structured_invoice_data\", \"vendor_list\", \"tax_rules\"],\n",
    "        \"expected_output\": \"Pass/Fail validation result with error codes\",\n",
    "        \"volume\": \"200 invoices/day\",\n",
    "        \"compliance_requirements\": [\"SOX\", \"audit_trail\"]\n",
    "    },\n",
    "    {\n",
    "        \"use_case_id\": \"UC-2025-003\",\n",
    "        \"title\": \"Contract Clause Analyzer\",\n",
    "        \"description\": \"Analyze legal contracts to extract key clauses, identify risks, and generate summary with specific rule-based flags for non-standard terms.\",\n",
    "        \"submitted_by\": \"legal_team\",\n",
    "        \"submitted_at\": \"2025-11-25\",\n",
    "        \"input_data_types\": [\"pdf_contracts\", \"clause_library\", \"risk_rules\"],\n",
    "        \"expected_output\": \"Structured extraction + natural language risk summary\",\n",
    "        \"volume\": \"50 contracts/week\",\n",
    "        \"compliance_requirements\": [\"legal_review\", \"audit_requirements\"]\n",
    "    }\n",
    "]\n",
    "\n",
    "print(\"✓ Sample use cases loaded\")\n",
    "print(f\"   - {sample_use_cases[0]['title']} (likely GenAI)\")\n",
    "print(f\"   - {sample_use_cases[1]['title']} (likely Deterministic)\")\n",
    "print(f\"   - {sample_use_cases[2]['title']} (likely Hybrid)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Foundation – Simple Python Agent (Lab 1)\n",
    "\n",
    "We create a base `Agent` class and 3 specialized agents for the evaluation workflow:\n",
    "\n",
    "1. **InfoCollectionAgent**: Gathers and structures use case information\n",
    "2. **EvaluationAgent**: Applies decision tree and generates verdict\n",
    "3. **ReportGenerationAgent**: Creates final recommendation with trade-offs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Base Agent class (Lab 1 - Simple Python Agent)\n",
    "class Agent:\n",
    "    \"\"\"Base agent class with action selection and execution.\"\"\"\n",
    "    \n",
    "    def __init__(self, name: str, role: str):\n",
    "        self.name = name\n",
    "        self.role = role\n",
    "        self.state = {}\n",
    "        \n",
    "    def _select_action(self, observation: Dict[str, Any]) -> str:\n",
    "        \"\"\"Select action based on observation. Override in subclasses.\"\"\"\n",
    "        raise NotImplementedError(\"Subclasses must implement _select_action\")\n",
    "        \n",
    "    def act(self, observation: Dict[str, Any]) -> Dict[str, Any]:\n",
    "        \"\"\"Execute action based on observation.\"\"\"\n",
    "        action = self._select_action(observation)\n",
    "        result = self._execute_action(action, observation)\n",
    "        return result\n",
    "        \n",
    "    def _execute_action(self, action: str, observation: Dict[str, Any]) -> Dict[str, Any]:\n",
    "        \"\"\"Execute the selected action.\"\"\"\n",
    "        return {\n",
    "            \"agent\": self.name,\n",
    "            \"action\": action,\n",
    "            \"status\": \"completed\",\n",
    "            \"observation\": observation\n",
    "        }\n",
    "\n",
    "print(\"✓ Base Agent class defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specialized Evaluation Agents\n",
    "\n",
    "class InfoCollectionAgent(Agent):\n",
    "    \"\"\"Agent for collecting and structuring use case information (Step 1).\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        super().__init__(\"InfoCollectionAgent\", \"Collect and structure use case data\")\n",
    "        self.required_fields = [\n",
    "            \"title\", \"description\", \"input_data_types\", \n",
    "            \"expected_output\", \"volume\", \"compliance_requirements\"\n",
    "        ]\n",
    "        self.decision_questions = [\n",
    "            {\n",
    "                \"id\": \"q1_deterministic_rules\",\n",
    "                \"question\": \"Can the problem be solved with CLEAR DETERMINISTIC RULES?\",\n",
    "                \"guidance\": \"If all inputs map to specific outputs via known rules, answer YES\"\n",
    "            },\n",
    "            {\n",
    "                \"id\": \"q2_natural_language_output\",\n",
    "                \"question\": \"Does the desired output require NATURAL LANGUAGE generation?\",\n",
    "                \"guidance\": \"If output is free-form text, summaries, or conversational, answer YES\"\n",
    "            },\n",
    "            {\n",
    "                \"id\": \"q3_unstructured_data\",\n",
    "                \"question\": \"Must UNSTRUCTURED DATA be analyzed (text, images, audio)?\",\n",
    "                \"guidance\": \"If inputs include documents, images, or audio for understanding, answer YES\"\n",
    "            },\n",
    "            {\n",
    "                \"id\": \"q4_mixed_requirements\",\n",
    "                \"question\": \"Are there MIXED requirements (some deterministic, some generative)?\",\n",
    "                \"guidance\": \"If workflow combines rule-based validation with generative outputs, answer YES\"\n",
    "            }\n",
    "        ]\n",
    "        \n",
    "    def _select_action(self, observation: Dict[str, Any]) -> str:\n",
    "        if \"use_case\" in observation:\n",
    "            return \"collect_and_structure\"\n",
    "        return \"request_information\"\n",
    "        \n",
    "    def collect_and_structure(self, use_case: Dict) -> Dict[str, Any]:\n",
    "        \"\"\"Collect use case info and apply decision-tree questions.\"\"\"\n",
    "        \n",
    "        # Check completeness\n",
    "        missing_fields = [f for f in self.required_fields if f not in use_case or not use_case[f]]\n",
    "        \n",
    "        # Analyze use case to infer answers to decision questions\n",
    "        criteria_answers = self._infer_criteria_answers(use_case)\n",
    "        \n",
    "        # Build structured summary\n",
    "        structured_summary = {\n",
    "            \"use_case_id\": use_case.get(\"use_case_id\", \"unknown\"),\n",
    "            \"title\": use_case.get(\"title\", \"Untitled\"),\n",
    "            \"description\": use_case.get(\"description\", \"\"),\n",
    "            \"data_characteristics\": {\n",
    "                \"input_types\": use_case.get(\"input_data_types\", []),\n",
    "                \"has_unstructured_data\": self._has_unstructured_data(use_case.get(\"input_data_types\", [])),\n",
    "                \"output_type\": use_case.get(\"expected_output\", \"\"),\n",
    "                \"requires_natural_language\": self._requires_nlg(use_case.get(\"expected_output\", \"\"))\n",
    "            },\n",
    "            \"operational_context\": {\n",
    "                \"volume\": use_case.get(\"volume\", \"unknown\"),\n",
    "                \"compliance\": use_case.get(\"compliance_requirements\", [])\n",
    "            },\n",
    "            \"criteria_answers\": criteria_answers,\n",
    "            \"completeness\": {\n",
    "                \"is_complete\": len(missing_fields) == 0,\n",
    "                \"missing_fields\": missing_fields\n",
    "            },\n",
    "            \"collected_at\": datetime.now().isoformat()\n",
    "        }\n",
    "        \n",
    "        return {\n",
    "            \"agent\": self.name,\n",
    "            \"action\": \"collect_and_structure\",\n",
    "            \"status\": \"success\",\n",
    "            \"structured_summary\": structured_summary,\n",
    "            \"message\": f\"Collected data for use case: {use_case.get('title', 'unknown')}\"\n",
    "        }\n",
    "        \n",
    "    def _infer_criteria_answers(self, use_case: Dict) -> Dict[str, Any]:\n",
    "        \"\"\"Infer answers to decision-tree questions based on use case data.\"\"\"\n",
    "        input_types = use_case.get(\"input_data_types\", [])\n",
    "        output = use_case.get(\"expected_output\", \"\").lower()\n",
    "        description = use_case.get(\"description\", \"\").lower()\n",
    "        \n",
    "        # Heuristics for inferring answers\n",
    "        has_unstructured = self._has_unstructured_data(input_types)\n",
    "        requires_nlg = self._requires_nlg(output)\n",
    "        \n",
    "        # Check for deterministic indicators\n",
    "        deterministic_keywords = [\"validate\", \"check\", \"verify\", \"match\", \"compare\", \"calculate\", \"rule\"]\n",
    "        has_deterministic = any(kw in description for kw in deterministic_keywords)\n",
    "        \n",
    "        return {\n",
    "            \"q1_deterministic_rules\": has_deterministic and not has_unstructured and not requires_nlg,\n",
    "            \"q2_natural_language_output\": requires_nlg,\n",
    "            \"q3_unstructured_data\": has_unstructured,\n",
    "            \"q4_mixed_requirements\": has_deterministic and (has_unstructured or requires_nlg)\n",
    "        }\n",
    "        \n",
    "    def _has_unstructured_data(self, input_types: List[str]) -> bool:\n",
    "        \"\"\"Check if input types include unstructured data.\"\"\"\n",
    "        unstructured_indicators = [\"text\", \"email\", \"document\", \"pdf\", \"image\", \"audio\", \"video\", \"contract\"]\n",
    "        return any(\n",
    "            any(ind in input_type.lower() for ind in unstructured_indicators)\n",
    "            for input_type in input_types\n",
    "        )\n",
    "        \n",
    "    def _requires_nlg(self, output: str) -> bool:\n",
    "        \"\"\"Check if output requires natural language generation.\"\"\"\n",
    "        nlg_indicators = [\"natural language\", \"email\", \"response\", \"summary\", \"narrative\", \"text\", \"description\"]\n",
    "        return any(ind in output.lower() for ind in nlg_indicators)\n",
    "\n",
    "\n",
    "class EvaluationAgent(Agent):\n",
    "    \"\"\"Agent for applying decision tree and generating verdict (Step 2).\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        super().__init__(\"EvaluationAgent\", \"Apply decision tree and recommend approach\")\n",
    "        \n",
    "    def _select_action(self, observation: Dict[str, Any]) -> str:\n",
    "        if \"structured_summary\" in observation:\n",
    "            return \"evaluate_and_recommend\"\n",
    "        return \"insufficient_data\"\n",
    "        \n",
    "    def evaluate_and_recommend(self, structured_summary: Dict) -> Dict[str, Any]:\n",
    "        \"\"\"Apply decision tree and generate preliminary verdict.\"\"\"\n",
    "        \n",
    "        criteria = structured_summary.get(\"criteria_answers\", {})\n",
    "        \n",
    "        # Apply decision tree logic\n",
    "        decision_path = []\n",
    "        \n",
    "        # Question 1: Clear deterministic rules?\n",
    "        if criteria.get(\"q1_deterministic_rules\", False):\n",
    "            decision_path.append(\"Q1: YES - Problem can be solved with deterministic rules\")\n",
    "            recommendation = RecommendationType.DETERMINISTIC\n",
    "        else:\n",
    "            decision_path.append(\"Q1: NO - Cannot be fully solved with deterministic rules\")\n",
    "            \n",
    "            # Question 2: Natural language output?\n",
    "            if criteria.get(\"q2_natural_language_output\", False):\n",
    "                decision_path.append(\"Q2: YES - Requires natural language generation\")\n",
    "                recommendation = RecommendationType.GENAI\n",
    "            else:\n",
    "                decision_path.append(\"Q2: NO - Does not require natural language generation\")\n",
    "                \n",
    "                # Question 3: Unstructured data?\n",
    "                if criteria.get(\"q3_unstructured_data\", False):\n",
    "                    decision_path.append(\"Q3: YES - Must analyze unstructured data\")\n",
    "                    recommendation = RecommendationType.GENAI\n",
    "                else:\n",
    "                    decision_path.append(\"Q3: NO - No unstructured data analysis needed\")\n",
    "                    recommendation = RecommendationType.DETERMINISTIC\n",
    "        \n",
    "        # Check for hybrid scenarios\n",
    "        if criteria.get(\"q4_mixed_requirements\", False):\n",
    "            decision_path.append(\"Q4: YES - Mixed requirements detected\")\n",
    "            recommendation = RecommendationType.HYBRID\n",
    "            \n",
    "        # Generate confidence score based on clarity of decision\n",
    "        confidence = self._calculate_confidence(criteria, recommendation)\n",
    "        \n",
    "        evaluation_result = {\n",
    "            \"use_case_id\": structured_summary.get(\"use_case_id\", \"unknown\"),\n",
    "            \"title\": structured_summary.get(\"title\", \"unknown\"),\n",
    "            \"recommendation\": recommendation.value,\n",
    "            \"confidence\": confidence,\n",
    "            \"decision_path\": decision_path,\n",
    "            \"criteria_evaluated\": criteria,\n",
    "            \"preliminary_rationale\": self._generate_rationale(recommendation, criteria),\n",
    "            \"evaluated_at\": datetime.now().isoformat()\n",
    "        }\n",
    "        \n",
    "        return {\n",
    "            \"agent\": self.name,\n",
    "            \"action\": \"evaluate_and_recommend\",\n",
    "            \"status\": \"success\",\n",
    "            \"evaluation\": evaluation_result,\n",
    "            \"message\": f\"Recommendation: {recommendation.value.upper()} (confidence: {confidence}%)\"\n",
    "        }\n",
    "        \n",
    "    def _calculate_confidence(self, criteria: Dict, recommendation: RecommendationType) -> int:\n",
    "        \"\"\"Calculate confidence score for the recommendation.\"\"\"\n",
    "        # Higher confidence when criteria clearly point one direction\n",
    "        if recommendation == RecommendationType.DETERMINISTIC:\n",
    "            if criteria.get(\"q1_deterministic_rules\") and not criteria.get(\"q2_natural_language_output\") and not criteria.get(\"q3_unstructured_data\"):\n",
    "                return 95\n",
    "            return 75\n",
    "        elif recommendation == RecommendationType.GENAI:\n",
    "            if criteria.get(\"q2_natural_language_output\") or criteria.get(\"q3_unstructured_data\"):\n",
    "                return 90\n",
    "            return 70\n",
    "        else:  # HYBRID\n",
    "            return 85  # Hybrid is typically well-justified when mixed requirements exist\n",
    "            \n",
    "    def _generate_rationale(self, recommendation: RecommendationType, criteria: Dict) -> str:\n",
    "        \"\"\"Generate preliminary rationale for the recommendation.\"\"\"\n",
    "        if recommendation == RecommendationType.DETERMINISTIC:\n",
    "            return \"The use case can be addressed with deterministic logic because the problem has clear, rule-based solutions and does not require natural language generation or unstructured data analysis.\"\n",
    "        elif recommendation == RecommendationType.GENAI:\n",
    "            reasons = []\n",
    "            if criteria.get(\"q2_natural_language_output\"):\n",
    "                reasons.append(\"natural language output is required\")\n",
    "            if criteria.get(\"q3_unstructured_data\"):\n",
    "                reasons.append(\"unstructured data must be analyzed\")\n",
    "            return f\"GenAI is recommended because {' and '.join(reasons)}. These capabilities are not achievable with traditional deterministic approaches.\"\n",
    "        else:  # HYBRID\n",
    "            return \"A hybrid approach is recommended because the use case has both deterministic components (rule-based validation/processing) and generative components (natural language or unstructured data). Combining both approaches optimizes for accuracy and auditability.\"\n",
    "\n",
    "\n",
    "class ReportGenerationAgent(Agent):\n",
    "    \"\"\"Agent for generating final recommendation report (Step 3).\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        super().__init__(\"ReportGenerationAgent\", \"Generate final recommendation with trade-offs\")\n",
    "        \n",
    "    def _select_action(self, observation: Dict[str, Any]) -> str:\n",
    "        if \"structured_summary\" in observation and \"evaluation\" in observation:\n",
    "            return \"generate_report\"\n",
    "        return \"insufficient_data\"\n",
    "        \n",
    "    def generate_report(self, structured_summary: Dict, evaluation: Dict) -> Dict[str, Any]:\n",
    "        \"\"\"Generate comprehensive recommendation report with trade-offs.\"\"\"\n",
    "        \n",
    "        recommendation = evaluation.get(\"recommendation\", \"unknown\")\n",
    "        \n",
    "        # Generate trade-off analysis\n",
    "        trade_offs = self._generate_trade_offs(recommendation, structured_summary)\n",
    "        \n",
    "        # Generate warnings and considerations\n",
    "        warnings = self._generate_warnings(recommendation, structured_summary)\n",
    "        \n",
    "        # Generate implementation guidance\n",
    "        implementation_guidance = self._generate_implementation_guidance(recommendation)\n",
    "        \n",
    "        # Build final report\n",
    "        report = {\n",
    "            \"report_id\": f\"REC-{evaluation.get('use_case_id', 'unknown')}\",\n",
    "            \"generated_at\": datetime.now().isoformat(),\n",
    "            \n",
    "            \"executive_summary\": self._generate_executive_summary(evaluation, structured_summary),\n",
    "            \n",
    "            \"use_case_details\": {\n",
    "                \"id\": structured_summary.get(\"use_case_id\"),\n",
    "                \"title\": structured_summary.get(\"title\"),\n",
    "                \"description\": structured_summary.get(\"description\")\n",
    "            },\n",
    "            \n",
    "            \"recommendation\": {\n",
    "                \"approach\": recommendation,\n",
    "                \"confidence\": evaluation.get(\"confidence\"),\n",
    "                \"rationale\": evaluation.get(\"preliminary_rationale\")\n",
    "            },\n",
    "            \n",
    "            \"decision_analysis\": {\n",
    "                \"decision_path\": evaluation.get(\"decision_path\"),\n",
    "                \"criteria_evaluated\": evaluation.get(\"criteria_evaluated\")\n",
    "            },\n",
    "            \n",
    "            \"trade_off_analysis\": trade_offs,\n",
    "            \n",
    "            \"warnings_and_considerations\": warnings,\n",
    "            \n",
    "            \"implementation_guidance\": implementation_guidance,\n",
    "            \n",
    "            \"next_steps\": self._generate_next_steps(recommendation)\n",
    "        }\n",
    "        \n",
    "        return {\n",
    "            \"agent\": self.name,\n",
    "            \"action\": \"generate_report\",\n",
    "            \"status\": \"success\",\n",
    "            \"report\": report,\n",
    "            \"message\": f\"Report {report['report_id']} generated successfully\"\n",
    "        }\n",
    "        \n",
    "    def _generate_executive_summary(self, evaluation: Dict, summary: Dict) -> str:\n",
    "        \"\"\"Generate executive summary.\"\"\"\n",
    "        title = summary.get(\"title\", \"Unknown\")\n",
    "        rec = evaluation.get(\"recommendation\", \"unknown\").upper()\n",
    "        conf = evaluation.get(\"confidence\", 0)\n",
    "        \n",
    "        return f\"\"\"Use case \\\"{title}\\\" has been evaluated using the GenAI Decision Framework.\n",
    "\n",
    "**Recommendation: {rec} APPROACH** (Confidence: {conf}%)\n",
    "\n",
    "{evaluation.get('preliminary_rationale', '')}\n",
    "\n",
    "This recommendation is based on analysis of the use case requirements, data characteristics, and expected outputs. See detailed trade-off analysis below.\"\"\"\n",
    "        \n",
    "    def _generate_trade_offs(self, recommendation: str, summary: Dict) -> Dict[str, Any]:\n",
    "        \"\"\"Generate trade-off analysis for the recommendation.\"\"\"\n",
    "        \n",
    "        trade_offs = {\n",
    "            \"deterministic\": {\n",
    "                \"pros\": [\n",
    "                    \"100% predictable and deterministic outputs\",\n",
    "                    \"Full auditability and explainability\",\n",
    "                    \"Lower operational costs\",\n",
    "                    \"Easier compliance and testing\",\n",
    "                    \"No model drift or prompt engineering needed\"\n",
    "                ],\n",
    "                \"cons\": [\n",
    "                    \"Cannot handle unstructured data effectively\",\n",
    "                    \"Limited flexibility for edge cases\",\n",
    "                    \"Rules must be manually maintained\",\n",
    "                    \"Cannot generate natural language outputs\"\n",
    "                ]\n",
    "            },\n",
    "            \"genai\": {\n",
    "                \"pros\": [\n",
    "                    \"Can process unstructured data (text, images, audio)\",\n",
    "                    \"Generates natural, contextual language outputs\",\n",
    "                    \"Handles edge cases and ambiguity well\",\n",
    "                    \"Faster development for complex NLP tasks\"\n",
    "                ],\n",
    "                \"cons\": [\n",
    "                    \"Non-deterministic outputs (variability)\",\n",
    "                    \"Harder to audit and explain decisions\",\n",
    "                    \"Higher operational costs (token usage)\",\n",
    "                    \"Potential hallucinations and errors\",\n",
    "                    \"Model drift and prompt maintenance required\"\n",
    "                ]\n",
    "            },\n",
    "            \"hybrid\": {\n",
    "                \"pros\": [\n",
    "                    \"Best of both worlds\",\n",
    "                    \"Deterministic validation with generative flexibility\",\n",
    "                    \"Clear separation of concerns\",\n",
    "                    \"Better auditability for rule-based components\"\n",
    "                ],\n",
    "                \"cons\": [\n",
    "                    \"More complex architecture\",\n",
    "                    \"Higher development and maintenance effort\",\n",
    "                    \"Need to define clear boundaries\",\n",
    "                    \"Testing complexity increases\"\n",
    "                ]\n",
    "            }\n",
    "        }\n",
    "        \n",
    "        return {\n",
    "            \"recommended_approach\": trade_offs.get(recommendation, {}),\n",
    "            \"alternatives\": {\n",
    "                k: v for k, v in trade_offs.items() if k != recommendation\n",
    "            }\n",
    "        }\n",
    "        \n",
    "    def _generate_warnings(self, recommendation: str, summary: Dict) -> List[str]:\n",
    "        \"\"\"Generate warnings and considerations.\"\"\"\n",
    "        warnings = []\n",
    "        compliance = summary.get(\"operational_context\", {}).get(\"compliance\", [])\n",
    "        \n",
    "        if recommendation == \"genai\":\n",
    "            warnings.append(\"⚠️ GenAI outputs are non-deterministic; implement output validation\")\n",
    "            warnings.append(\"⚠️ Consider explainability requirements for audit purposes\")\n",
    "            if \"SOX\" in compliance or \"audit\" in str(compliance).lower():\n",
    "                warnings.append(\"⚠️ Compliance requirement detected: Ensure GenAI decisions can be explained and traced\")\n",
    "                \n",
    "        if recommendation == \"hybrid\":\n",
    "            warnings.append(\"⚠️ Define clear boundaries between deterministic and GenAI components\")\n",
    "            warnings.append(\"⚠️ Ensure deterministic validation runs before/after GenAI processing\")\n",
    "            \n",
    "        if \"GDPR\" in compliance:\n",
    "            warnings.append(\"⚠️ GDPR compliance: Ensure no PII is sent to external LLM APIs\")\n",
    "            \n",
    "        return warnings\n",
    "        \n",
    "    def _generate_implementation_guidance(self, recommendation: str) -> Dict[str, Any]:\n",
    "        \"\"\"Generate implementation guidance.\"\"\"\n",
    "        guidance = {\n",
    "            \"deterministic\": {\n",
    "                \"suggested_technologies\": [\"Python/Java rule engine\", \"SQL-based validation\", \"Business rules management system\"],\n",
    "                \"architecture_pattern\": \"Rule-based processing pipeline\",\n",
    "                \"key_considerations\": [\n",
    "                    \"Document all rules clearly\",\n",
    "                    \"Implement comprehensive unit tests\",\n",
    "                    \"Version control rule changes\",\n",
    "                    \"Build monitoring for rule hit rates\"\n",
    "                ]\n",
    "            },\n",
    "            \"genai\": {\n",
    "                \"suggested_technologies\": [\"OpenAI GPT-4\", \"Claude\", \"LangChain\", \"Custom fine-tuned models\"],\n",
    "                \"architecture_pattern\": \"LLM-powered processing with validation layer\",\n",
    "                \"key_considerations\": [\n",
    "                    \"Implement prompt versioning and testing\",\n",
    "                    \"Add output validation and guardrails\",\n",
    "                    \"Monitor for model drift and hallucinations\",\n",
    "                    \"Implement fallback mechanisms\",\n",
    "                    \"Consider token cost optimization\"\n",
    "                ]\n",
    "            },\n",
    "            \"hybrid\": {\n",
    "                \"suggested_technologies\": [\"Rule engine + LangChain\", \"Python + OpenAI API\", \"Orchestration framework\"],\n",
    "                \"architecture_pattern\": \"Layered architecture: Deterministic validation → GenAI processing → Deterministic post-validation\",\n",
    "                \"key_considerations\": [\n",
    "                    \"Define clear interface between components\",\n",
    "                    \"Use deterministic pre-processing for data validation\",\n",
    "                    \"Apply deterministic post-processing for output validation\",\n",
    "                    \"Document which component handles which logic\"\n",
    "                ]\n",
    "            }\n",
    "        }\n",
    "        \n",
    "        return guidance.get(recommendation, {})\n",
    "        \n",
    "    def _generate_next_steps(self, recommendation: str) -> List[str]:\n",
    "        \"\"\"Generate recommended next steps.\"\"\"\n",
    "        base_steps = [\n",
    "            \"Review this recommendation with the requesting team\",\n",
    "            \"Validate decision criteria answers with stakeholders\",\n",
    "            \"Assess implementation complexity and timeline\"\n",
    "        ]\n",
    "        \n",
    "        if recommendation == \"genai\":\n",
    "            base_steps.extend([\n",
    "                \"Conduct prompt engineering proof-of-concept\",\n",
    "                \"Evaluate LLM provider options and costs\",\n",
    "                \"Define output validation strategy\"\n",
    "            ])\n",
    "        elif recommendation == \"hybrid\":\n",
    "            base_steps.extend([\n",
    "                \"Design component boundary specification\",\n",
    "                \"Define data flow between deterministic and GenAI layers\",\n",
    "                \"Plan integration testing strategy\"\n",
    "            ])\n",
    "        else:\n",
    "            base_steps.extend([\n",
    "                \"Document complete rule set\",\n",
    "                \"Design rule engine architecture\",\n",
    "                \"Plan rule maintenance process\"\n",
    "            ])\n",
    "            \n",
    "        return base_steps\n",
    "\n",
    "print(\"✓ All 3 specialized evaluation agents defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test Specialized Agents with sample use cases\n",
    "\n",
    "# Test with GenAI-likely use case (Customer Email Response)\n",
    "print(\"=\" * 80)\n",
    "print(\"Testing with: Customer Email Response Generator\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# 1. Info Collection Agent\n",
    "info_agent = InfoCollectionAgent()\n",
    "info_result = info_agent.collect_and_structure(sample_use_cases[0])\n",
    "print(f\"\\n1. InfoCollectionAgent: {info_result['message']}\")\n",
    "print(f\"   Has unstructured data: {info_result['structured_summary']['data_characteristics']['has_unstructured_data']}\")\n",
    "print(f\"   Requires NLG: {info_result['structured_summary']['data_characteristics']['requires_natural_language']}\")\n",
    "\n",
    "# 2. Evaluation Agent\n",
    "eval_agent = EvaluationAgent()\n",
    "eval_result = eval_agent.evaluate_and_recommend(info_result['structured_summary'])\n",
    "print(f\"\\n2. EvaluationAgent: {eval_result['message']}\")\n",
    "print(f\"   Decision Path:\")\n",
    "for step in eval_result['evaluation']['decision_path']:\n",
    "    print(f\"      - {step}\")\n",
    "\n",
    "# 3. Report Generation Agent\n",
    "report_agent = ReportGenerationAgent()\n",
    "report_result = report_agent.generate_report(info_result['structured_summary'], eval_result['evaluation'])\n",
    "print(f\"\\n3. ReportGenerationAgent: {report_result['message']}\")\n",
    "print(f\"   Recommendation: {report_result['report']['recommendation']['approach'].upper()}\")\n",
    "print(f\"   Confidence: {report_result['report']['recommendation']['confidence']}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test with Deterministic-likely use case (Invoice Validation)\n",
    "print(\"=\" * 80)\n",
    "print(\"Testing with: Invoice Validation System\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "info_result_2 = info_agent.collect_and_structure(sample_use_cases[1])\n",
    "print(f\"\\n1. InfoCollectionAgent: {info_result_2['message']}\")\n",
    "print(f\"   Has unstructured data: {info_result_2['structured_summary']['data_characteristics']['has_unstructured_data']}\")\n",
    "print(f\"   Requires NLG: {info_result_2['structured_summary']['data_characteristics']['requires_natural_language']}\")\n",
    "\n",
    "eval_result_2 = eval_agent.evaluate_and_recommend(info_result_2['structured_summary'])\n",
    "print(f\"\\n2. EvaluationAgent: {eval_result_2['message']}\")\n",
    "\n",
    "report_result_2 = report_agent.generate_report(info_result_2['structured_summary'], eval_result_2['evaluation'])\n",
    "print(f\"\\n3. ReportGenerationAgent: {report_result_2['message']}\")\n",
    "print(f\"   Recommendation: {report_result_2['report']['recommendation']['approach'].upper()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test with Hybrid-likely use case (Contract Clause Analyzer)\n",
    "print(\"=\" * 80)\n",
    "print(\"Testing with: Contract Clause Analyzer\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "info_result_3 = info_agent.collect_and_structure(sample_use_cases[2])\n",
    "print(f\"\\n1. InfoCollectionAgent: {info_result_3['message']}\")\n",
    "print(f\"   Has unstructured data: {info_result_3['structured_summary']['data_characteristics']['has_unstructured_data']}\")\n",
    "print(f\"   Mixed requirements: {info_result_3['structured_summary']['criteria_answers']['q4_mixed_requirements']}\")\n",
    "\n",
    "eval_result_3 = eval_agent.evaluate_and_recommend(info_result_3['structured_summary'])\n",
    "print(f\"\\n2. EvaluationAgent: {eval_result_3['message']}\")\n",
    "\n",
    "report_result_3 = report_agent.generate_report(info_result_3['structured_summary'], eval_result_3['evaluation'])\n",
    "print(f\"\\n3. ReportGenerationAgent: {report_result_3['message']}\")\n",
    "print(f\"   Recommendation: {report_result_3['report']['recommendation']['approach'].upper()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3: Multi-Agent Communication (Lab 2)\n",
    "\n",
    "Use AutoGen's `RoundRobinGroupChat` to orchestrate sequential communication between evaluation agents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize model client\n",
    "from autogen import ConversableAgent, GroupChat, GroupChatManager\n",
    "\n",
    "llm_config = {\n",
    "    \"model\": \"gpt-4\",\n",
    "    \"api_key\": os.environ[\"OPENAI_API_KEY\"],\n",
    "    \"temperature\": 0.7\n",
    "}\n",
    "\n",
    "print(\"✓ AutoGen LLM config initialized\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define AutoGen agents for GenAI evaluation workflow\n",
    "\n",
    "evaluation_coordinator = ConversableAgent(\n",
    "    name=\"EvaluationCoordinator\",\n",
    "    system_message=\"\"\"You are the GenAI Use-Case Evaluation coordinator. You receive use case submissions \n",
    "    and orchestrate the evaluation workflow through specialized agents: \n",
    "    InfoCollector → DecisionTreeEvaluator → ReportWriter.\n",
    "    Ensure the final recommendation is clear: DETERMINISTIC, GENAI, or HYBRID.\"\"\",\n",
    "    llm_config=llm_config,\n",
    "    human_input_mode=\"NEVER\"\n",
    ")\n",
    "\n",
    "info_collector = ConversableAgent(\n",
    "    name=\"InfoCollector\",\n",
    "    system_message=\"\"\"You collect and structure use case information. Analyze the submission to answer:\n",
    "    1. Can this be solved with clear deterministic rules?\n",
    "    2. Does it require natural language generation?\n",
    "    3. Must unstructured data be analyzed?\n",
    "    4. Are there mixed requirements?\n",
    "    Provide a structured summary with these answers.\"\"\",\n",
    "    llm_config=llm_config,\n",
    "    human_input_mode=\"NEVER\"\n",
    ")\n",
    "\n",
    "decision_evaluator = ConversableAgent(\n",
    "    name=\"DecisionTreeEvaluator\",\n",
    "    system_message=\"\"\"You apply the GenAI decision tree:\n",
    "    - If deterministic rules suffice → DETERMINISTIC\n",
    "    - If NLG or unstructured data needed → GENAI\n",
    "    - If mixed requirements → HYBRID\n",
    "    Provide the decision path and preliminary verdict with confidence level.\"\"\",\n",
    "    llm_config=llm_config,\n",
    "    human_input_mode=\"NEVER\"\n",
    ")\n",
    "\n",
    "report_writer = ConversableAgent(\n",
    "    name=\"ReportWriter\",\n",
    "    system_message=\"\"\"You generate the final recommendation report with:\n",
    "    - Executive summary\n",
    "    - Trade-off analysis (pros/cons of recommended approach vs alternatives)\n",
    "    - Warnings and compliance considerations\n",
    "    - Implementation guidance\n",
    "    - Next steps\n",
    "    Make the report actionable and clear.\"\"\",\n",
    "    llm_config=llm_config,\n",
    "    human_input_mode=\"NEVER\"\n",
    ")\n",
    "\n",
    "print(\"✓ AutoGen agents for evaluation workflow defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Round-Robin Group Chat\n",
    "\n",
    "evaluation_group_chat = GroupChat(\n",
    "    agents=[evaluation_coordinator, info_collector, decision_evaluator, report_writer],\n",
    "    messages=[],\n",
    "    max_round=8,\n",
    "    speaker_selection_method=\"round_robin\"\n",
    ")\n",
    "\n",
    "evaluation_manager = GroupChatManager(\n",
    "    groupchat=evaluation_group_chat,\n",
    "    llm_config=llm_config\n",
    ")\n",
    "\n",
    "print(\"✓ Evaluation Round-Robin Group Chat created\")\n",
    "print(\"Workflow: Coordinator → InfoCollector → DecisionEvaluator → ReportWriter\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 4: Deliberative Agent with LangGraph (Lab 4)\n",
    "\n",
    "Implement a deliberative workflow using LangGraph's StateGraph for the evaluation process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize LangChain LLM\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "\n",
    "llm = ChatOpenAI(model=\"gpt-4\", temperature=0.7, api_key=os.environ[\"OPENAI_API_KEY\"])\n",
    "print(\"✓ LangChain LLM initialized\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define State Schema\n",
    "from typing import TypedDict, Annotated\n",
    "from langgraph.graph import StateGraph, END\n",
    "import operator\n",
    "\n",
    "class EvaluationWorkflowState(TypedDict):\n",
    "    \"\"\"State for GenAI evaluation workflow.\"\"\"\n",
    "    use_case: Dict[str, Any]\n",
    "    structured_summary: Optional[Dict[str, Any]]\n",
    "    evaluation: Optional[Dict[str, Any]]\n",
    "    report: Optional[Dict[str, Any]]\n",
    "    messages: Annotated[List[str], operator.add]\n",
    "    next_step: str\n",
    "\n",
    "print(\"✓ Evaluation workflow state schema defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define workflow nodes\n",
    "\n",
    "def collect_info_node(state: EvaluationWorkflowState) -> EvaluationWorkflowState:\n",
    "    \"\"\"Collect and structure use case information.\"\"\"\n",
    "    agent = InfoCollectionAgent()\n",
    "    result = agent.collect_and_structure(state[\"use_case\"])\n",
    "    state[\"structured_summary\"] = result[\"structured_summary\"]\n",
    "    state[\"messages\"].append(f\"[InfoCollector] {result['message']}\")\n",
    "    state[\"next_step\"] = \"evaluate\"\n",
    "    return state\n",
    "\n",
    "def evaluate_node(state: EvaluationWorkflowState) -> EvaluationWorkflowState:\n",
    "    \"\"\"Apply decision tree and generate verdict.\"\"\"\n",
    "    agent = EvaluationAgent()\n",
    "    result = agent.evaluate_and_recommend(state[\"structured_summary\"])\n",
    "    state[\"evaluation\"] = result[\"evaluation\"]\n",
    "    state[\"messages\"].append(f\"[Evaluator] {result['message']}\")\n",
    "    state[\"next_step\"] = \"generate_report\"\n",
    "    return state\n",
    "\n",
    "def generate_report_node(state: EvaluationWorkflowState) -> EvaluationWorkflowState:\n",
    "    \"\"\"Generate final recommendation report.\"\"\"\n",
    "    agent = ReportGenerationAgent()\n",
    "    result = agent.generate_report(state[\"structured_summary\"], state[\"evaluation\"])\n",
    "    state[\"report\"] = result[\"report\"]\n",
    "    state[\"messages\"].append(f\"[ReportGenerator] {result['message']}\")\n",
    "    state[\"next_step\"] = \"end\"\n",
    "    return state\n",
    "\n",
    "print(\"✓ Workflow nodes defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build the workflow graph\n",
    "\n",
    "workflow = StateGraph(EvaluationWorkflowState)\n",
    "\n",
    "# Add nodes\n",
    "workflow.add_node(\"collect_info\", collect_info_node)\n",
    "workflow.add_node(\"evaluate\", evaluate_node)\n",
    "workflow.add_node(\"generate_report\", generate_report_node)\n",
    "\n",
    "# Define edges\n",
    "workflow.add_edge(\"collect_info\", \"evaluate\")\n",
    "workflow.add_edge(\"evaluate\", \"generate_report\")\n",
    "workflow.add_edge(\"generate_report\", END)\n",
    "\n",
    "# Set entry point\n",
    "workflow.set_entry_point(\"collect_info\")\n",
    "\n",
    "# Compile\n",
    "evaluation_workflow_app = workflow.compile()\n",
    "\n",
    "print(\"✓ LangGraph workflow compiled\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Execute workflow for all sample use cases\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"EXECUTING LANGGRAPH WORKFLOW FOR ALL USE CASES\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "workflow_results = []\n",
    "\n",
    "for use_case in sample_use_cases:\n",
    "    print(f\"\\n--- Evaluating: {use_case['title']} ---\")\n",
    "    \n",
    "    initial_state = EvaluationWorkflowState(\n",
    "        use_case=use_case,\n",
    "        structured_summary=None,\n",
    "        evaluation=None,\n",
    "        report=None,\n",
    "        messages=[],\n",
    "        next_step=\"collect_info\"\n",
    "    )\n",
    "    \n",
    "    final_state = evaluation_workflow_app.invoke(initial_state)\n",
    "    workflow_results.append(final_state)\n",
    "    \n",
    "    print(f\"Workflow Steps:\")\n",
    "    for msg in final_state[\"messages\"]:\n",
    "        print(f\"  {msg}\")\n",
    "    print(f\"Final Recommendation: {final_state['report']['recommendation']['approach'].upper()}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"WORKFLOW EXECUTION COMPLETE\")\n",
    "print(\"=\" * 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 5: Observation & Action Tools (Labs 6-7)\n",
    "\n",
    "Implement observation tools (data retrieval) and action tools (persistence) for the evaluation workflow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Observation Tools (Lab 6)\n",
    "\n",
    "# Simulated knowledge base of past evaluations\n",
    "CASE_KNOWLEDGE_BASE = [\n",
    "    {\n",
    "        \"case_id\": \"HIST-001\",\n",
    "        \"title\": \"Chatbot for FAQ\",\n",
    "        \"recommendation\": \"genai\",\n",
    "        \"rationale\": \"Required natural language understanding and generation\"\n",
    "    },\n",
    "    {\n",
    "        \"case_id\": \"HIST-002\",\n",
    "        \"title\": \"Transaction Fraud Rules\",\n",
    "        \"recommendation\": \"deterministic\",\n",
    "        \"rationale\": \"Clear threshold-based rules for fraud detection\"\n",
    "    },\n",
    "    {\n",
    "        \"case_id\": \"HIST-003\",\n",
    "        \"title\": \"Document Summarization with Validation\",\n",
    "        \"recommendation\": \"hybrid\",\n",
    "        \"rationale\": \"Combine GenAI for summarization with rules for validation\"\n",
    "    }\n",
    "]\n",
    "\n",
    "def search_similar_cases(use_case_title: str, use_case_description: str) -> List[Dict]:\n",
    "    \"\"\"\n",
    "    Search knowledge base for similar past evaluations.\n",
    "    \n",
    "    Args:\n",
    "        use_case_title: Title of the use case\n",
    "        use_case_description: Description of the use case\n",
    "        \n",
    "    Returns:\n",
    "        List of similar past cases\n",
    "    \"\"\"\n",
    "    # In production, use embeddings and vector search\n",
    "    # For demo, return all cases as \"similar\"\n",
    "    return {\n",
    "        \"query\": use_case_title,\n",
    "        \"similar_cases\": CASE_KNOWLEDGE_BASE,\n",
    "        \"search_timestamp\": datetime.now().isoformat()\n",
    "    }\n",
    "\n",
    "def get_evaluation_criteria() -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Retrieve the current evaluation criteria and decision tree.\n",
    "    \n",
    "    Returns:\n",
    "        Evaluation criteria configuration\n",
    "    \"\"\"\n",
    "    return {\n",
    "        \"version\": \"1.0\",\n",
    "        \"decision_tree\": [\n",
    "            {\n",
    "                \"question_id\": \"q1\",\n",
    "                \"question\": \"Can the problem be solved with CLEAR DETERMINISTIC RULES?\",\n",
    "                \"yes_action\": \"recommend_deterministic\",\n",
    "                \"no_action\": \"continue_to_q2\"\n",
    "            },\n",
    "            {\n",
    "                \"question_id\": \"q2\",\n",
    "                \"question\": \"Does the desired output require NATURAL LANGUAGE generation?\",\n",
    "                \"yes_action\": \"recommend_genai\",\n",
    "                \"no_action\": \"continue_to_q3\"\n",
    "            },\n",
    "            {\n",
    "                \"question_id\": \"q3\",\n",
    "                \"question\": \"Must UNSTRUCTURED DATA be analyzed?\",\n",
    "                \"yes_action\": \"recommend_genai\",\n",
    "                \"no_action\": \"recommend_deterministic\"\n",
    "            },\n",
    "            {\n",
    "                \"question_id\": \"q4\",\n",
    "                \"question\": \"Are there MIXED requirements?\",\n",
    "                \"yes_action\": \"recommend_hybrid\",\n",
    "                \"no_action\": \"use_previous_recommendation\"\n",
    "            }\n",
    "        ],\n",
    "        \"retrieved_at\": datetime.now().isoformat()\n",
    "    }\n",
    "\n",
    "def get_compliance_requirements(compliance_codes: List[str]) -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Retrieve compliance requirement details.\n",
    "    \n",
    "    Args:\n",
    "        compliance_codes: List of compliance requirement codes\n",
    "        \n",
    "    Returns:\n",
    "        Compliance requirement details\n",
    "    \"\"\"\n",
    "    compliance_details = {\n",
    "        \"GDPR\": {\n",
    "            \"name\": \"General Data Protection Regulation\",\n",
    "            \"genai_considerations\": [\"No PII to external LLMs\", \"Right to explanation\", \"Data minimization\"],\n",
    "            \"risk_level\": \"HIGH\"\n",
    "        },\n",
    "        \"SOX\": {\n",
    "            \"name\": \"Sarbanes-Oxley Act\",\n",
    "            \"genai_considerations\": [\"Full audit trail required\", \"Decision explainability\", \"Version control\"],\n",
    "            \"risk_level\": \"HIGH\"\n",
    "        },\n",
    "        \"audit_trail\": {\n",
    "            \"name\": \"Audit Trail Requirements\",\n",
    "            \"genai_considerations\": [\"Log all inputs and outputs\", \"Trace decision paths\"],\n",
    "            \"risk_level\": \"MEDIUM\"\n",
    "        },\n",
    "        \"brand_guidelines\": {\n",
    "            \"name\": \"Brand Guidelines\",\n",
    "            \"genai_considerations\": [\"Output validation for brand consistency\", \"Tone monitoring\"],\n",
    "            \"risk_level\": \"LOW\"\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    return {\n",
    "        \"requested_codes\": compliance_codes,\n",
    "        \"details\": {code: compliance_details.get(code, {\"name\": \"Unknown\", \"genai_considerations\": [], \"risk_level\": \"UNKNOWN\"}) for code in compliance_codes}\n",
    "    }\n",
    "\n",
    "print(\"✓ Observation tools defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Action Tools (Lab 7)\n",
    "\n",
    "# Simulated persistent storage\n",
    "EVALUATION_DATABASE = []\n",
    "\n",
    "def save_evaluation_to_kb(report: Dict, structured_summary: Dict) -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Save evaluation report to knowledge base.\n",
    "    \n",
    "    Args:\n",
    "        report: Final recommendation report\n",
    "        structured_summary: Structured use case summary\n",
    "        \n",
    "    Returns:\n",
    "        Save confirmation\n",
    "    \"\"\"\n",
    "    record = {\n",
    "        \"record_id\": f\"KB-{len(EVALUATION_DATABASE) + 1:04d}\",\n",
    "        \"report_id\": report.get(\"report_id\"),\n",
    "        \"use_case_id\": structured_summary.get(\"use_case_id\"),\n",
    "        \"title\": structured_summary.get(\"title\"),\n",
    "        \"recommendation\": report.get(\"recommendation\", {}).get(\"approach\"),\n",
    "        \"confidence\": report.get(\"recommendation\", {}).get(\"confidence\"),\n",
    "        \"criteria_answers\": structured_summary.get(\"criteria_answers\"),\n",
    "        \"saved_at\": datetime.now().isoformat()\n",
    "    }\n",
    "    \n",
    "    EVALUATION_DATABASE.append(record)\n",
    "    \n",
    "    return {\n",
    "        \"status\": \"SAVED\",\n",
    "        \"record_id\": record[\"record_id\"],\n",
    "        \"saved_at\": record[\"saved_at\"]\n",
    "    }\n",
    "\n",
    "def notify_stakeholders(use_case_id: str, recommendation: str, submitted_by: str) -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Notify stakeholders of evaluation completion.\n",
    "    \n",
    "    Args:\n",
    "        use_case_id: Use case identifier\n",
    "        recommendation: Final recommendation\n",
    "        submitted_by: Original requester\n",
    "        \n",
    "    Returns:\n",
    "        Notification confirmation\n",
    "    \"\"\"\n",
    "    # In production, send actual notifications\n",
    "    return {\n",
    "        \"notification_id\": f\"NOTIF-{datetime.now().strftime('%Y%m%d%H%M%S')}\",\n",
    "        \"use_case_id\": use_case_id,\n",
    "        \"recipients\": [submitted_by, \"ai_governance_team\"],\n",
    "        \"message\": f\"Evaluation complete: {recommendation.upper()} approach recommended\",\n",
    "        \"sent_at\": datetime.now().isoformat(),\n",
    "        \"status\": \"SENT\"\n",
    "    }\n",
    "\n",
    "def export_report(report: Dict, format: str = \"json\") -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Export recommendation report.\n",
    "    \n",
    "    Args:\n",
    "        report: Report to export\n",
    "        format: Export format (json, pdf, etc.)\n",
    "        \n",
    "    Returns:\n",
    "        Export confirmation with location\n",
    "    \"\"\"\n",
    "    # In production, generate actual file\n",
    "    return {\n",
    "        \"report_id\": report.get(\"report_id\"),\n",
    "        \"format\": format,\n",
    "        \"location\": f\"/exports/{report.get('report_id')}.{format}\",\n",
    "        \"exported_at\": datetime.now().isoformat(),\n",
    "        \"status\": \"EXPORTED\"\n",
    "    }\n",
    "\n",
    "print(\"✓ Action tools defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test observation and action tools\n",
    "\n",
    "print(\"=== Testing Observation Tools ===\")\n",
    "\n",
    "print(\"\\n1. Search Similar Cases:\")\n",
    "similar = search_similar_cases(\"Customer Email Generator\", \"Generate email responses\")\n",
    "print(f\"   Found {len(similar['similar_cases'])} similar cases\")\n",
    "for case in similar['similar_cases']:\n",
    "    print(f\"     - {case['title']}: {case['recommendation'].upper()}\")\n",
    "\n",
    "print(\"\\n2. Get Evaluation Criteria:\")\n",
    "criteria = get_evaluation_criteria()\n",
    "print(f\"   Version: {criteria['version']}\")\n",
    "print(f\"   Questions in decision tree: {len(criteria['decision_tree'])}\")\n",
    "\n",
    "print(\"\\n3. Get Compliance Requirements:\")\n",
    "compliance = get_compliance_requirements([\"GDPR\", \"SOX\"])\n",
    "for code, details in compliance['details'].items():\n",
    "    print(f\"   {code}: {details['name']} (Risk: {details['risk_level']})\")\n",
    "\n",
    "print(\"\\n=== Testing Action Tools ===\")\n",
    "\n",
    "print(\"\\n1. Save Evaluation to Knowledge Base:\")\n",
    "save_result = save_evaluation_to_kb(\n",
    "    workflow_results[0]['report'],\n",
    "    workflow_results[0]['structured_summary']\n",
    ")\n",
    "print(f\"   Status: {save_result['status']}\")\n",
    "print(f\"   Record ID: {save_result['record_id']}\")\n",
    "\n",
    "print(\"\\n2. Notify Stakeholders:\")\n",
    "notification = notify_stakeholders(\n",
    "    sample_use_cases[0]['use_case_id'],\n",
    "    workflow_results[0]['report']['recommendation']['approach'],\n",
    "    sample_use_cases[0]['submitted_by']\n",
    ")\n",
    "print(f\"   Status: {notification['status']}\")\n",
    "print(f\"   Recipients: {notification['recipients']}\")\n",
    "\n",
    "print(\"\\n3. Export Report:\")\n",
    "export = export_report(workflow_results[0]['report'], \"json\")\n",
    "print(f\"   Status: {export['status']}\")\n",
    "print(f\"   Location: {export['location']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 6: Rule-Based Reasoning (Lab 9)\n",
    "\n",
    "Implement deterministic rule-based validation for evaluation quality and completeness."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rule-Based Validator for Evaluation Quality\n",
    "\n",
    "class RuleBasedEvaluationValidator:\n",
    "    \"\"\"Deterministic rule-based validation for use case evaluations.\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.validation_rules = [\n",
    "            self.rule_required_use_case_fields,\n",
    "            self.rule_decision_tree_completeness,\n",
    "            self.rule_recommendation_consistency,\n",
    "            self.rule_confidence_threshold,\n",
    "            self.rule_trade_offs_present,\n",
    "            self.rule_compliance_addressed\n",
    "        ]\n",
    "        \n",
    "    def validate_all(self, use_case: Dict, structured_summary: Dict, evaluation: Dict, report: Optional[Dict] = None) -> Dict[str, Any]:\n",
    "        \"\"\"Run all validation rules.\"\"\"\n",
    "        results = []\n",
    "        \n",
    "        for rule in self.validation_rules:\n",
    "            result = rule(use_case, structured_summary, evaluation, report)\n",
    "            results.append(result)\n",
    "            \n",
    "        passed = sum(1 for r in results if r[\"passed\"])\n",
    "        failed = len(results) - passed\n",
    "        \n",
    "        return {\n",
    "            \"validation_summary\": {\n",
    "                \"total_rules\": len(results),\n",
    "                \"passed\": passed,\n",
    "                \"failed\": failed,\n",
    "                \"overall_status\": \"PASS\" if failed == 0 else \"FAIL\"\n",
    "            },\n",
    "            \"rule_results\": results\n",
    "        }\n",
    "        \n",
    "    def rule_required_use_case_fields(self, use_case: Dict, summary: Dict, evaluation: Dict, report: Optional[Dict]) -> Dict:\n",
    "        \"\"\"Validate required use case fields are present.\"\"\"\n",
    "        required_fields = [\"title\", \"description\", \"input_data_types\", \"expected_output\"]\n",
    "        missing = [f for f in required_fields if f not in use_case or not use_case[f]]\n",
    "        \n",
    "        return {\n",
    "            \"rule\": \"required_use_case_fields\",\n",
    "            \"passed\": len(missing) == 0,\n",
    "            \"message\": \"All required fields present\" if len(missing) == 0 else f\"Missing fields: {missing}\"\n",
    "        }\n",
    "        \n",
    "    def rule_decision_tree_completeness(self, use_case: Dict, summary: Dict, evaluation: Dict, report: Optional[Dict]) -> Dict:\n",
    "        \"\"\"Validate all decision tree questions were answered.\"\"\"\n",
    "        criteria = summary.get(\"criteria_answers\", {})\n",
    "        required_questions = [\"q1_deterministic_rules\", \"q2_natural_language_output\", \"q3_unstructured_data\", \"q4_mixed_requirements\"]\n",
    "        answered = [q for q in required_questions if q in criteria and criteria[q] is not None]\n",
    "        \n",
    "        return {\n",
    "            \"rule\": \"decision_tree_completeness\",\n",
    "            \"passed\": len(answered) == len(required_questions),\n",
    "            \"message\": f\"Decision tree complete: {len(answered)}/{len(required_questions)} questions answered\"\n",
    "        }\n",
    "        \n",
    "    def rule_recommendation_consistency(self, use_case: Dict, summary: Dict, evaluation: Dict, report: Optional[Dict]) -> Dict:\n",
    "        \"\"\"Validate recommendation is consistent with criteria answers.\"\"\"\n",
    "        criteria = summary.get(\"criteria_answers\", {})\n",
    "        rec = evaluation.get(\"recommendation\", \"\")\n",
    "        \n",
    "        # Check consistency\n",
    "        is_consistent = True\n",
    "        issues = []\n",
    "        \n",
    "        if rec == \"deterministic\":\n",
    "            if criteria.get(\"q2_natural_language_output\") or criteria.get(\"q3_unstructured_data\"):\n",
    "                is_consistent = False\n",
    "                issues.append(\"Deterministic recommended but NLG or unstructured data detected\")\n",
    "                \n",
    "        elif rec == \"genai\":\n",
    "            if criteria.get(\"q1_deterministic_rules\") and not criteria.get(\"q2_natural_language_output\") and not criteria.get(\"q3_unstructured_data\"):\n",
    "                is_consistent = False\n",
    "                issues.append(\"GenAI recommended but problem appears deterministic\")\n",
    "                \n",
    "        return {\n",
    "            \"rule\": \"recommendation_consistency\",\n",
    "            \"passed\": is_consistent,\n",
    "            \"message\": \"Recommendation consistent with criteria\" if is_consistent else f\"Inconsistencies: {issues}\"\n",
    "        }\n",
    "        \n",
    "    def rule_confidence_threshold(self, use_case: Dict, summary: Dict, evaluation: Dict, report: Optional[Dict]) -> Dict:\n",
    "        \"\"\"Validate confidence meets minimum threshold.\"\"\"\n",
    "        confidence = evaluation.get(\"confidence\", 0)\n",
    "        threshold = 60  # Minimum 60% confidence required\n",
    "        \n",
    "        return {\n",
    "            \"rule\": \"confidence_threshold\",\n",
    "            \"passed\": confidence >= threshold,\n",
    "            \"message\": f\"Confidence {confidence}% meets threshold ({threshold}%)\" if confidence >= threshold else f\"Confidence {confidence}% below threshold ({threshold}%)\"\n",
    "        }\n",
    "        \n",
    "    def rule_trade_offs_present(self, use_case: Dict, summary: Dict, evaluation: Dict, report: Optional[Dict]) -> Dict:\n",
    "        \"\"\"Validate trade-off analysis is present in report.\"\"\"\n",
    "        if not report:\n",
    "            return {\"rule\": \"trade_offs_present\", \"passed\": True, \"message\": \"Skipped (no report)\"}\n",
    "            \n",
    "        trade_offs = report.get(\"trade_off_analysis\", {})\n",
    "        has_trade_offs = (\n",
    "            \"recommended_approach\" in trade_offs and\n",
    "            \"pros\" in trade_offs.get(\"recommended_approach\", {}) and\n",
    "            \"cons\" in trade_offs.get(\"recommended_approach\", {})\n",
    "        )\n",
    "        \n",
    "        return {\n",
    "            \"rule\": \"trade_offs_present\",\n",
    "            \"passed\": has_trade_offs,\n",
    "            \"message\": \"Trade-off analysis complete\" if has_trade_offs else \"Trade-off analysis missing or incomplete\"\n",
    "        }\n",
    "        \n",
    "    def rule_compliance_addressed(self, use_case: Dict, summary: Dict, evaluation: Dict, report: Optional[Dict]) -> Dict:\n",
    "        \"\"\"Validate compliance requirements are addressed.\"\"\"\n",
    "        compliance_reqs = use_case.get(\"compliance_requirements\", [])\n",
    "        \n",
    "        if not compliance_reqs:\n",
    "            return {\"rule\": \"compliance_addressed\", \"passed\": True, \"message\": \"No compliance requirements specified\"}\n",
    "            \n",
    "        if not report:\n",
    "            return {\"rule\": \"compliance_addressed\", \"passed\": True, \"message\": \"Skipped (no report)\"}\n",
    "            \n",
    "        warnings = report.get(\"warnings_and_considerations\", [])\n",
    "        has_compliance_warnings = any(\"compliance\" in str(w).lower() or any(req.lower() in str(w).lower() for req in compliance_reqs) for w in warnings)\n",
    "        \n",
    "        return {\n",
    "            \"rule\": \"compliance_addressed\",\n",
    "            \"passed\": has_compliance_warnings or len(compliance_reqs) == 0,\n",
    "            \"message\": \"Compliance requirements addressed in warnings\" if has_compliance_warnings else f\"Compliance requirements {compliance_reqs} not addressed in warnings\"\n",
    "        }\n",
    "\n",
    "print(\"✓ Rule-based evaluation validator defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test rule-based validation\n",
    "\n",
    "validator = RuleBasedEvaluationValidator()\n",
    "\n",
    "print(\"=== Rule-Based Validation Results ===\")\n",
    "\n",
    "for i, result in enumerate(workflow_results):\n",
    "    print(f\"\\n--- Use Case: {sample_use_cases[i]['title']} ---\")\n",
    "    \n",
    "    validation = validator.validate_all(\n",
    "        sample_use_cases[i],\n",
    "        result['structured_summary'],\n",
    "        result['evaluation'],\n",
    "        result['report']\n",
    "    )\n",
    "    \n",
    "    print(f\"Overall Status: {validation['validation_summary']['overall_status']}\")\n",
    "    print(f\"Passed: {validation['validation_summary']['passed']}/{validation['validation_summary']['total_rules']}\")\n",
    "    \n",
    "    for rule_result in validation['rule_results']:\n",
    "        status_icon = \"✓\" if rule_result['passed'] else \"✗\"\n",
    "        print(f\"  {status_icon} {rule_result['rule']}: {rule_result['message']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 7: Hierarchical Planning (Lab 8)\n",
    "\n",
    "Implement hierarchical task decomposition for the evaluation workflow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hierarchical Planner for Evaluation Workflow\n",
    "\n",
    "class HierarchicalEvaluationPlanner:\n",
    "    \"\"\"Hierarchical planner for use case evaluation workflows.\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.tasks = self._define_tasks()\n",
    "        self.task_dependencies = self._define_dependencies()\n",
    "        self.agent_assignments = self._define_agent_assignments()\n",
    "        \n",
    "    def _define_tasks(self) -> Dict[str, Dict]:\n",
    "        \"\"\"Define hierarchical task structure.\"\"\"\n",
    "        return {\n",
    "            \"genai_evaluation\": {\n",
    "                \"description\": \"Complete GenAI use-case evaluation\",\n",
    "                \"type\": \"composite\",\n",
    "                \"subtasks\": [\"data_collection\", \"decision_analysis\", \"documentation\"]\n",
    "            },\n",
    "            \"data_collection\": {\n",
    "                \"description\": \"Collect and structure use case information\",\n",
    "                \"type\": \"composite\",\n",
    "                \"subtasks\": [\"extract_use_case_info\", \"answer_decision_questions\", \"search_similar_cases\"]\n",
    "            },\n",
    "            \"extract_use_case_info\": {\n",
    "                \"description\": \"Extract basic use case information\",\n",
    "                \"type\": \"primitive\",\n",
    "                \"estimated_time_min\": 5\n",
    "            },\n",
    "            \"answer_decision_questions\": {\n",
    "                \"description\": \"Infer answers to decision tree questions\",\n",
    "                \"type\": \"primitive\",\n",
    "                \"estimated_time_min\": 5\n",
    "            },\n",
    "            \"search_similar_cases\": {\n",
    "                \"description\": \"Search knowledge base for similar past evaluations\",\n",
    "                \"type\": \"primitive\",\n",
    "                \"estimated_time_min\": 3\n",
    "            },\n",
    "            \"decision_analysis\": {\n",
    "                \"description\": \"Apply decision tree and generate verdict\",\n",
    "                \"type\": \"composite\",\n",
    "                \"subtasks\": [\"apply_decision_tree\", \"calculate_confidence\", \"generate_rationale\"]\n",
    "            },\n",
    "            \"apply_decision_tree\": {\n",
    "                \"description\": \"Execute decision tree logic\",\n",
    "                \"type\": \"primitive\",\n",
    "                \"estimated_time_min\": 3\n",
    "            },\n",
    "            \"calculate_confidence\": {\n",
    "                \"description\": \"Calculate recommendation confidence score\",\n",
    "                \"type\": \"primitive\",\n",
    "                \"estimated_time_min\": 2\n",
    "            },\n",
    "            \"generate_rationale\": {\n",
    "                \"description\": \"Generate preliminary rationale for recommendation\",\n",
    "                \"type\": \"primitive\",\n",
    "                \"estimated_time_min\": 5\n",
    "            },\n",
    "            \"documentation\": {\n",
    "                \"description\": \"Generate final report and persist to KB\",\n",
    "                \"type\": \"composite\",\n",
    "                \"subtasks\": [\"generate_trade_offs\", \"generate_warnings\", \"assemble_report\", \"save_to_kb\"]\n",
    "            },\n",
    "            \"generate_trade_offs\": {\n",
    "                \"description\": \"Generate trade-off analysis\",\n",
    "                \"type\": \"primitive\",\n",
    "                \"estimated_time_min\": 8\n",
    "            },\n",
    "            \"generate_warnings\": {\n",
    "                \"description\": \"Generate compliance warnings and considerations\",\n",
    "                \"type\": \"primitive\",\n",
    "                \"estimated_time_min\": 5\n",
    "            },\n",
    "            \"assemble_report\": {\n",
    "                \"description\": \"Assemble final recommendation report\",\n",
    "                \"type\": \"primitive\",\n",
    "                \"estimated_time_min\": 10\n",
    "            },\n",
    "            \"save_to_kb\": {\n",
    "                \"description\": \"Save evaluation to knowledge base\",\n",
    "                \"type\": \"primitive\",\n",
    "                \"estimated_time_min\": 2\n",
    "            }\n",
    "        }\n",
    "        \n",
    "    def _define_dependencies(self) -> Dict[str, List[str]]:\n",
    "        \"\"\"Define task dependencies.\"\"\"\n",
    "        return {\n",
    "            \"answer_decision_questions\": [\"extract_use_case_info\"],\n",
    "            \"search_similar_cases\": [\"extract_use_case_info\"],\n",
    "            \"apply_decision_tree\": [\"answer_decision_questions\"],\n",
    "            \"calculate_confidence\": [\"apply_decision_tree\"],\n",
    "            \"generate_rationale\": [\"apply_decision_tree\", \"search_similar_cases\"],\n",
    "            \"generate_trade_offs\": [\"generate_rationale\"],\n",
    "            \"generate_warnings\": [\"generate_rationale\", \"extract_use_case_info\"],\n",
    "            \"assemble_report\": [\"generate_trade_offs\", \"generate_warnings\", \"calculate_confidence\"],\n",
    "            \"save_to_kb\": [\"assemble_report\"]\n",
    "        }\n",
    "        \n",
    "    def _define_agent_assignments(self) -> Dict[str, str]:\n",
    "        \"\"\"Define which agent handles each task.\"\"\"\n",
    "        return {\n",
    "            \"extract_use_case_info\": \"InfoCollectionAgent\",\n",
    "            \"answer_decision_questions\": \"InfoCollectionAgent\",\n",
    "            \"search_similar_cases\": \"InfoCollectionAgent\",\n",
    "            \"apply_decision_tree\": \"EvaluationAgent\",\n",
    "            \"calculate_confidence\": \"EvaluationAgent\",\n",
    "            \"generate_rationale\": \"EvaluationAgent\",\n",
    "            \"generate_trade_offs\": \"ReportGenerationAgent\",\n",
    "            \"generate_warnings\": \"ReportGenerationAgent\",\n",
    "            \"assemble_report\": \"ReportGenerationAgent\",\n",
    "            \"save_to_kb\": \"ReportGenerationAgent\"\n",
    "        }\n",
    "        \n",
    "    def create_execution_plan(self, root_task: str = \"genai_evaluation\") -> Dict[str, Any]:\n",
    "        \"\"\"Create hierarchical execution plan.\"\"\"\n",
    "        \n",
    "        def get_all_primitive_tasks(task_name: str) -> List[str]:\n",
    "            task = self.tasks[task_name]\n",
    "            if task[\"type\"] == \"primitive\":\n",
    "                return [task_name]\n",
    "            else:\n",
    "                primitives = []\n",
    "                for subtask in task.get(\"subtasks\", []):\n",
    "                    primitives.extend(get_all_primitive_tasks(subtask))\n",
    "                return primitives\n",
    "                \n",
    "        primitive_tasks = get_all_primitive_tasks(root_task)\n",
    "        ordered_tasks = self._topological_sort(primitive_tasks)\n",
    "        \n",
    "        execution_plan = {\n",
    "            \"root_task\": root_task,\n",
    "            \"total_tasks\": len(ordered_tasks),\n",
    "            \"estimated_total_time_min\": sum(\n",
    "                self.tasks[task].get(\"estimated_time_min\", 0)\n",
    "                for task in ordered_tasks\n",
    "            ),\n",
    "            \"execution_sequence\": [\n",
    "                {\n",
    "                    \"order\": idx + 1,\n",
    "                    \"task_name\": task,\n",
    "                    \"description\": self.tasks[task][\"description\"],\n",
    "                    \"agent\": self.agent_assignments.get(task, \"Unknown\"),\n",
    "                    \"estimated_time_min\": self.tasks[task].get(\"estimated_time_min\", 0),\n",
    "                    \"dependencies\": self.task_dependencies.get(task, [])\n",
    "                }\n",
    "                for idx, task in enumerate(ordered_tasks)\n",
    "            ]\n",
    "        }\n",
    "        \n",
    "        return execution_plan\n",
    "        \n",
    "    def _topological_sort(self, tasks: List[str]) -> List[str]:\n",
    "        \"\"\"Topological sort of tasks based on dependencies.\"\"\"\n",
    "        in_degree = {task: 0 for task in tasks}\n",
    "        adj_list = {task: [] for task in tasks}\n",
    "        \n",
    "        for task in tasks:\n",
    "            deps = self.task_dependencies.get(task, [])\n",
    "            for dep in deps:\n",
    "                if dep in tasks:\n",
    "                    adj_list[dep].append(task)\n",
    "                    in_degree[task] += 1\n",
    "                    \n",
    "        queue = [task for task in tasks if in_degree[task] == 0]\n",
    "        sorted_tasks = []\n",
    "        \n",
    "        while queue:\n",
    "            current = queue.pop(0)\n",
    "            sorted_tasks.append(current)\n",
    "            \n",
    "            for neighbor in adj_list[current]:\n",
    "                in_degree[neighbor] -= 1\n",
    "                if in_degree[neighbor] == 0:\n",
    "                    queue.append(neighbor)\n",
    "                    \n",
    "        return sorted_tasks\n",
    "\n",
    "print(\"✓ Hierarchical evaluation planner defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test hierarchical planning\n",
    "\n",
    "planner = HierarchicalEvaluationPlanner()\n",
    "execution_plan = planner.create_execution_plan()\n",
    "\n",
    "print(\"=== Hierarchical Execution Plan ===\")\n",
    "print(f\"\\nRoot Task: {execution_plan['root_task']}\")\n",
    "print(f\"Total Tasks: {execution_plan['total_tasks']}\")\n",
    "print(f\"Estimated Total Time: {execution_plan['estimated_total_time_min']} minutes\")\n",
    "\n",
    "print(\"\\nExecution Sequence:\")\n",
    "for step in execution_plan['execution_sequence']:\n",
    "    deps_str = f\" (depends on: {', '.join(step['dependencies'])})\" if step['dependencies'] else \"\"\n",
    "    print(f\"  {step['order']}. {step['task_name']} [{step['agent']}] - {step['estimated_time_min']}min{deps_str}\")\n",
    "    print(f\"     {step['description']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 8: Error Recovery (Lab 12)\n",
    "\n",
    "Implement robust error handling with retry logic and circuit breaker patterns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Error Recovery Mechanisms\n",
    "\n",
    "import time\n",
    "from functools import wraps\n",
    "\n",
    "class CircuitState(Enum):\n",
    "    CLOSED = \"closed\"\n",
    "    OPEN = \"open\"\n",
    "    HALF_OPEN = \"half_open\"\n",
    "\n",
    "class CircuitBreaker:\n",
    "    \"\"\"Circuit breaker pattern for fault tolerance.\"\"\"\n",
    "    \n",
    "    def __init__(self, failure_threshold: int = 3, timeout: int = 60):\n",
    "        self.failure_threshold = failure_threshold\n",
    "        self.timeout = timeout\n",
    "        self.failure_count = 0\n",
    "        self.last_failure_time = None\n",
    "        self.state = CircuitState.CLOSED\n",
    "        \n",
    "    def call(self, func, *args, **kwargs):\n",
    "        if self.state == CircuitState.OPEN:\n",
    "            if time.time() - self.last_failure_time >= self.timeout:\n",
    "                self.state = CircuitState.HALF_OPEN\n",
    "            else:\n",
    "                raise Exception(\"Circuit breaker OPEN - service unavailable\")\n",
    "                \n",
    "        try:\n",
    "            result = func(*args, **kwargs)\n",
    "            self._on_success()\n",
    "            return result\n",
    "        except Exception as e:\n",
    "            self._on_failure()\n",
    "            raise e\n",
    "            \n",
    "    def _on_success(self):\n",
    "        self.failure_count = 0\n",
    "        self.state = CircuitState.CLOSED\n",
    "        \n",
    "    def _on_failure(self):\n",
    "        self.failure_count += 1\n",
    "        self.last_failure_time = time.time()\n",
    "        if self.failure_count >= self.failure_threshold:\n",
    "            self.state = CircuitState.OPEN\n",
    "\n",
    "def with_retry(max_attempts: int = 3, backoff_factor: float = 2.0):\n",
    "    \"\"\"Retry decorator with exponential backoff.\"\"\"\n",
    "    def decorator(func):\n",
    "        @wraps(func)\n",
    "        def wrapper(*args, **kwargs):\n",
    "            attempt = 0\n",
    "            while attempt < max_attempts:\n",
    "                try:\n",
    "                    return func(*args, **kwargs)\n",
    "                except Exception as e:\n",
    "                    attempt += 1\n",
    "                    if attempt >= max_attempts:\n",
    "                        raise e\n",
    "                    wait_time = backoff_factor ** attempt\n",
    "                    print(f\"  [Retry] Attempt {attempt} failed: {str(e)}. Retrying in {wait_time}s...\")\n",
    "                    time.sleep(wait_time)\n",
    "        return wrapper\n",
    "    return decorator\n",
    "\n",
    "class ResilientEvaluationAgent:\n",
    "    \"\"\"Evaluation agent with error recovery capabilities.\"\"\"\n",
    "    \n",
    "    def __init__(self, base_agent: Agent):\n",
    "        self.base_agent = base_agent\n",
    "        self.circuit_breaker = CircuitBreaker(failure_threshold=3, timeout=60)\n",
    "        self.error_log = []\n",
    "        \n",
    "    @with_retry(max_attempts=3, backoff_factor=2.0)\n",
    "    def execute_with_recovery(self, method_name: str, *args, **kwargs) -> Dict[str, Any]:\n",
    "        try:\n",
    "            method = getattr(self.base_agent, method_name)\n",
    "            result = self.circuit_breaker.call(method, *args, **kwargs)\n",
    "            return {\n",
    "                \"status\": \"success\",\n",
    "                \"result\": result,\n",
    "                \"agent\": self.base_agent.name\n",
    "            }\n",
    "        except Exception as e:\n",
    "            self.error_log.append({\n",
    "                \"timestamp\": datetime.now().isoformat(),\n",
    "                \"agent\": self.base_agent.name,\n",
    "                \"method\": method_name,\n",
    "                \"error\": str(e)\n",
    "            })\n",
    "            return {\n",
    "                \"status\": \"degraded\",\n",
    "                \"result\": self._graceful_degradation(method_name),\n",
    "                \"agent\": self.base_agent.name,\n",
    "                \"error\": str(e)\n",
    "            }\n",
    "            \n",
    "    def _graceful_degradation(self, method_name: str) -> Dict[str, Any]:\n",
    "        return {\n",
    "            \"degraded\": True,\n",
    "            \"message\": f\"Service degraded - using default recommendation\",\n",
    "            \"fallback_recommendation\": \"hybrid\"  # Default to hybrid as safest option\n",
    "        }\n",
    "        \n",
    "    def get_error_summary(self) -> Dict[str, Any]:\n",
    "        return {\n",
    "            \"total_errors\": len(self.error_log),\n",
    "            \"circuit_state\": self.circuit_breaker.state.value,\n",
    "            \"recent_errors\": self.error_log[-5:] if self.error_log else []\n",
    "        }\n",
    "\n",
    "print(\"✓ Error recovery mechanisms defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test error recovery\n",
    "\n",
    "info_agent = InfoCollectionAgent()\n",
    "resilient_agent = ResilientEvaluationAgent(info_agent)\n",
    "\n",
    "print(\"=== Testing Error Recovery ===\")\n",
    "\n",
    "print(\"\\n1. Successful execution:\")\n",
    "result = resilient_agent.execute_with_recovery(\"collect_and_structure\", sample_use_cases[0])\n",
    "print(f\"   Status: {result['status']}\")\n",
    "print(f\"   Agent: {result['agent']}\")\n",
    "\n",
    "print(\"\\n2. Error summary:\")\n",
    "error_summary = resilient_agent.get_error_summary()\n",
    "print(f\"   Total Errors: {error_summary['total_errors']}\")\n",
    "print(f\"   Circuit State: {error_summary['circuit_state']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 9: Complete End-to-End System (Lab 11)\n",
    "\n",
    "Integrate all components into a production-ready evaluation system."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Complete GenAI Evaluation System\n",
    "\n",
    "class GenAIEvaluationSystem:\n",
    "    \"\"\"Production-ready GenAI use-case evaluation system.\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        # Initialize agents with resilience\n",
    "        self.info_agent = ResilientEvaluationAgent(InfoCollectionAgent())\n",
    "        self.eval_agent = ResilientEvaluationAgent(EvaluationAgent())\n",
    "        self.report_agent = ResilientEvaluationAgent(ReportGenerationAgent())\n",
    "        \n",
    "        # Initialize supporting components\n",
    "        self.validator = RuleBasedEvaluationValidator()\n",
    "        self.planner = HierarchicalEvaluationPlanner()\n",
    "        \n",
    "        # System state\n",
    "        self.processing_log = []\n",
    "        \n",
    "    def evaluate_use_case(self, use_case: Dict) -> Dict[str, Any]:\n",
    "        \"\"\"Evaluate a use case end-to-end.\"\"\"\n",
    "        start_time = time.time()\n",
    "        self.processing_log = []\n",
    "        \n",
    "        try:\n",
    "            # Step 0: Create execution plan\n",
    "            self._log(f\"Creating execution plan for use case: {use_case.get('title', 'unknown')}\")\n",
    "            execution_plan = self.planner.create_execution_plan()\n",
    "            \n",
    "            # Step 1: Collect and structure information\n",
    "            self._log(\"Collecting and structuring use case information\")\n",
    "            info_result = self.info_agent.execute_with_recovery(\"collect_and_structure\", use_case)\n",
    "            if info_result['status'] != \"success\":\n",
    "                return self._create_error_response(use_case, \"Info collection failed\", info_result)\n",
    "            structured_summary = info_result['result']['structured_summary']\n",
    "            \n",
    "            # Step 1b: Search similar cases\n",
    "            self._log(\"Searching for similar past evaluations\")\n",
    "            similar_cases = search_similar_cases(use_case.get('title', ''), use_case.get('description', ''))\n",
    "            \n",
    "            # Step 2: Apply decision tree\n",
    "            self._log(\"Applying decision tree and generating verdict\")\n",
    "            eval_result = self.eval_agent.execute_with_recovery(\"evaluate_and_recommend\", structured_summary)\n",
    "            if eval_result['status'] != \"success\":\n",
    "                return self._create_error_response(use_case, \"Evaluation failed\", eval_result)\n",
    "            evaluation = eval_result['result']['evaluation']\n",
    "            \n",
    "            # Step 3: Generate report\n",
    "            self._log(\"Generating recommendation report\")\n",
    "            report_result = self.report_agent.execute_with_recovery(\"generate_report\", structured_summary, evaluation)\n",
    "            if report_result['status'] != \"success\":\n",
    "                return self._create_error_response(use_case, \"Report generation failed\", report_result)\n",
    "            report = report_result['result']['report']\n",
    "            \n",
    "            # Step 4: Validate evaluation\n",
    "            self._log(\"Running validation checks\")\n",
    "            validation = self.validator.validate_all(use_case, structured_summary, evaluation, report)\n",
    "            \n",
    "            # Step 5: Save to knowledge base\n",
    "            self._log(\"Saving evaluation to knowledge base\")\n",
    "            save_result = save_evaluation_to_kb(report, structured_summary)\n",
    "            \n",
    "            # Step 6: Notify stakeholders\n",
    "            self._log(\"Notifying stakeholders\")\n",
    "            notification = notify_stakeholders(\n",
    "                use_case.get('use_case_id', 'unknown'),\n",
    "                evaluation['recommendation'],\n",
    "                use_case.get('submitted_by', 'unknown')\n",
    "            )\n",
    "            \n",
    "            processing_time = time.time() - start_time\n",
    "            \n",
    "            return {\n",
    "                \"status\": \"SUCCESS\",\n",
    "                \"use_case_id\": use_case.get('use_case_id'),\n",
    "                \"processing_time_seconds\": round(processing_time, 2),\n",
    "                \"execution_plan\": execution_plan,\n",
    "                \"structured_summary\": structured_summary,\n",
    "                \"similar_cases\": similar_cases,\n",
    "                \"evaluation\": evaluation,\n",
    "                \"report\": report,\n",
    "                \"validation\": validation,\n",
    "                \"actions_taken\": {\n",
    "                    \"saved_to_kb\": save_result,\n",
    "                    \"notification\": notification\n",
    "                },\n",
    "                \"processing_log\": self.processing_log\n",
    "            }\n",
    "            \n",
    "        except Exception as e:\n",
    "            return self._create_error_response(use_case, f\"System error: {str(e)}\", {\"exception\": str(e)})\n",
    "            \n",
    "    def _log(self, message: str):\n",
    "        log_entry = {\n",
    "            \"timestamp\": datetime.now().isoformat(),\n",
    "            \"message\": message\n",
    "        }\n",
    "        self.processing_log.append(log_entry)\n",
    "        print(f\"[{datetime.now().strftime('%H:%M:%S')}] {message}\")\n",
    "        \n",
    "    def _create_error_response(self, use_case: Dict, error_message: str, details: Dict) -> Dict:\n",
    "        return {\n",
    "            \"status\": \"ERROR\",\n",
    "            \"use_case_id\": use_case.get('use_case_id', 'unknown'),\n",
    "            \"error_message\": error_message,\n",
    "            \"error_details\": details,\n",
    "            \"processing_log\": self.processing_log\n",
    "        }\n",
    "        \n",
    "    def get_system_health(self) -> Dict[str, Any]:\n",
    "        return {\n",
    "            \"agents\": {\n",
    "                \"info_agent\": self.info_agent.get_error_summary(),\n",
    "                \"eval_agent\": self.eval_agent.get_error_summary(),\n",
    "                \"report_agent\": self.report_agent.get_error_summary()\n",
    "            },\n",
    "            \"knowledge_base_records\": len(EVALUATION_DATABASE),\n",
    "            \"system_status\": \"OPERATIONAL\"\n",
    "        }\n",
    "\n",
    "print(\"✓ Complete GenAI Evaluation System defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Execute end-to-end evaluation for all sample use cases\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"COMPLETE END-TO-END GENAI USE-CASE EVALUATION\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Initialize system\n",
    "evaluation_system = GenAIEvaluationSystem()\n",
    "\n",
    "# Process all sample use cases\n",
    "final_results = []\n",
    "\n",
    "for use_case in sample_use_cases:\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"EVALUATING: {use_case['title']}\")\n",
    "    print(f\"{'='*60}\")\n",
    "    \n",
    "    result = evaluation_system.evaluate_use_case(use_case)\n",
    "    final_results.append(result)\n",
    "    \n",
    "    print(f\"\\n--- RESULT ---\")\n",
    "    print(f\"Status: {result['status']}\")\n",
    "    print(f\"Processing Time: {result['processing_time_seconds']}s\")\n",
    "    \n",
    "    if result['status'] == \"SUCCESS\":\n",
    "        print(f\"Recommendation: {result['report']['recommendation']['approach'].upper()}\")\n",
    "        print(f\"Confidence: {result['report']['recommendation']['confidence']}%\")\n",
    "        print(f\"KB Record: {result['actions_taken']['saved_to_kb']['record_id']}\")\n",
    "        \n",
    "        print(f\"\\nValidation: {result['validation']['validation_summary']['overall_status']}\")\n",
    "        print(f\"  Rules Passed: {result['validation']['validation_summary']['passed']}/{result['validation']['validation_summary']['total_rules']}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"EVALUATION SUMMARY\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "print(\"\\nRecommendations:\")\n",
    "for i, result in enumerate(final_results):\n",
    "    if result['status'] == \"SUCCESS\":\n",
    "        rec = result['report']['recommendation']['approach'].upper()\n",
    "        conf = result['report']['recommendation']['confidence']\n",
    "        print(f\"  {sample_use_cases[i]['title']}: {rec} ({conf}% confidence)\")\n",
    "\n",
    "print(\"\\nSystem Health:\")\n",
    "health = evaluation_system.get_system_health()\n",
    "print(f\"  Status: {health['system_status']}\")\n",
    "print(f\"  Knowledge Base Records: {health['knowledge_base_records']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary & Key Takeaways\n",
    "\n",
    "This notebook demonstrated a comprehensive agentic AI implementation for GenAI use-case evaluation, applying all 12 lab techniques from the GAI-3101 course:\n",
    "\n",
    "### Implementation Highlights\n",
    "\n",
    "1. **Simple Python Agents (Lab 1)**: Created 3 specialized agents for the evaluation workflow\n",
    "2. **Multi-Agent Communication (Lab 2)**: Orchestrated agents using AutoGen round-robin chat\n",
    "3. **Deliberative Agents (Lab 4)**: Implemented LangGraph StateGraph for workflow orchestration\n",
    "4. **Observation Tools (Lab 6)**: Built tools for knowledge base search, criteria retrieval, compliance lookup\n",
    "5. **Action Tools (Lab 7)**: Implemented KB persistence, notifications, report export\n",
    "6. **Hierarchical Planning (Lab 8)**: Decomposed evaluation into 10 primitive tasks\n",
    "7. **Rule-Based Reasoning (Lab 9)**: Created 6 deterministic validation rules\n",
    "8. **Error Recovery (Lab 12)**: Added retry logic, circuit breakers, and graceful degradation\n",
    "9. **End-to-End System (Lab 11)**: Integrated all components into `GenAIEvaluationSystem`\n",
    "\n",
    "### Business Impact\n",
    "\n",
    "**Per Evaluation:**\n",
    "- Lead time: 13.75 hours → 4 hours (71% reduction)\n",
    "- Manual effort: 2.75 hours → 1 hour (64% reduction)\n",
    "- Time savings: 1.75 hours per use case\n",
    "\n",
    "**Annual (40 evaluations/month):**\n",
    "- Hours saved: 840 hours/year\n",
    "- Cost savings: $67,200/year\n",
    "- Year 1 net benefit: $7,200\n",
    "- Year 2+ benefit: $47,200/year\n",
    "\n",
    "### Decision Tree Summary\n",
    "\n",
    "The evaluation follows this decision tree:\n",
    "1. **Clear deterministic rules?** → DETERMINISTIC\n",
    "2. **Natural language output?** → GENAI\n",
    "3. **Unstructured data analysis?** → GENAI\n",
    "4. **Mixed requirements?** → HYBRID\n",
    "\n",
    "### Production Considerations\n",
    "\n",
    "1. Replace simulated KB with actual vector database (Pinecone, Weaviate)\n",
    "2. Integrate with ticketing/governance systems\n",
    "3. Add user interface for use case submission\n",
    "4. Implement feedback loop for recommendation accuracy tracking\n",
    "5. Set up monitoring and alerting\n",
    "6. Train teams on self-service evaluation\n",
    "\n",
    "---\n",
    "\n",
    "**Notebook Version:** 1.0  \n",
    "**Last Updated:** November 2025  \n",
    "**Course:** GAI-3101 Custom Agentic AI Solutions"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
