{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "header",
   "metadata": {},
   "source": [
    "# Building a Personal Assistant Agent - SOLUTIONS\n",
    "\n",
    "**Production-Ready Implementation with Logging, Error Handling, and Structured Outputs**\n",
    "\n",
    "This notebook contains complete solutions for Challenge 1 and Challenge 2."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "learning-outcomes",
   "metadata": {},
   "source": [
    "## Learning Outcomes\n",
    "\n",
    "By the end of this lab, you will be able to:\n",
    "\n",
    "- **Design single-agent architectures** for multi-capability applications\n",
    "- **Implement production-grade logging** with Python's `logging` module\n",
    "- **Add comprehensive error handling** with try/except patterns\n",
    "- **Build structured outputs** using Pydantic models\n",
    "- **Visualize agent workflows** with LangGraph's built-in diagram tools\n",
    "- **Manage persistent state** with thread-based memory\n",
    "- **Test and validate** complex agent workflows\n",
    "- **Learn from user corrections** to adapt agent behavior"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "introduction",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "### The Scenario\n",
    "\n",
    "You're a PhD student juggling multiple research projects. You need an agent that can:\n",
    "\n",
    "- Find and organize academic papers\n",
    "- Schedule reading time on your calendar\n",
    "- Maintain your bibliography database\n",
    "- Remember your preferences (morning meetings, favorite journals)\n",
    "- Learn from your corrections\n",
    "\n",
    "### Single-Agent Architecture\n",
    "\n",
    "Unlike multi-agent systems where specialists collaborate, a **single agent** coordinates all capabilities through:\n",
    "\n",
    "1. **Unified state management**: One state tracks all context\n",
    "2. **Tool orchestration**: Agent decides which tools to use when\n",
    "3. **Integrated memory**: Preferences and context persist across sessions\n",
    "4. **Adaptive behavior**: Learns from user feedback\n",
    "\n",
    "**When to use this pattern**:\n",
    "- Tasks are related and share context\n",
    "- Coordination overhead of multiple agents isn't justified\n",
    "- User expects seamless, conversational interaction\n",
    "\n",
    "### Production Patterns You'll Learn\n",
    "\n",
    "This lab teaches **real-world engineering practices**:\n",
    "\n",
    "- **Logging**: Replace `print()` with proper logging for debugging\n",
    "- **Error handling**: Gracefully handle tool failures\n",
    "- **Structured outputs**: Use Pydantic for type-safe tool responses\n",
    "- **Graph visualization**: Understand agent flow with diagrams\n",
    "- **Thread management**: Understand LangGraph's session isolation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "setup-header",
   "metadata": {},
   "source": [
    "## Setup and Installation\n",
    "\n",
    "### Install Required Packages\n",
    "\n",
    "We'll install the **latest stable versions** of:\n",
    "- **LangGraph 1.0+**: Agent workflow framework (currently 1.0.3)\n",
    "- **LangChain 1.0+**: Core LLM abstractions\n",
    "- **LangChain OpenAI**: OpenAI model integration\n",
    "- **Requests**: HTTP client for API calls\n",
    "\n",
    "**Version 1.0 Upgrade Notes**:\n",
    "- LangGraph 1.0 has ZERO breaking changes from 0.6.6 - all APIs remain backward compatible\n",
    "- LangChain 1.0 requires Python 3.10+ and has minor package structure changes\n",
    "- All existing code patterns work without modification in 1.0\n",
    "\n",
    "**Note on version specs**: We use quotes to prevent shell glob expansion issues."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "install",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install -qU \\\n",
    "    langgraph > 1.0.0\\\n",
    "    langchain > 1.0.0 \\\n",
    "    langchain-openai > 1.0.0 \\\n",
    "    requests > 2.0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "96b52c2f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Name: langgraph\n",
      "Version: 1.0.3\n",
      "Summary: Building stateful, multi-actor applications with LLMs\n",
      "Home-page: https://docs.langchain.com/oss/python/langgraph/overview\n",
      "Author: \n",
      "Author-email: \n",
      "License-Expression: MIT\n",
      "Location: /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages\n",
      "Requires: langchain-core, langgraph-checkpoint, langgraph-prebuilt, langgraph-sdk, pydantic, xxhash\n",
      "Required-by: langchain\n",
      "---\n",
      "Name: langchain\n",
      "Version: 1.0.8\n",
      "Summary: Building applications with LLMs through composability\n",
      "Home-page: https://docs.langchain.com/\n",
      "Author: \n",
      "Author-email: \n",
      "License: MIT\n",
      "Location: /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages\n",
      "Requires: langchain-core, langgraph, pydantic\n",
      "Required-by: \n",
      "---\n",
      "Name: langchain-openai\n",
      "Version: 1.0.3\n",
      "Summary: An integration package connecting OpenAI and LangChain\n",
      "Home-page: https://docs.langchain.com/oss/python/integrations/providers/openai\n",
      "Author: \n",
      "Author-email: \n",
      "License: MIT\n",
      "Location: /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages\n",
      "Requires: langchain-core, openai, tiktoken\n",
      "Required-by: \n",
      "---\n",
      "Name: requests\n",
      "Version: 2.32.5\n",
      "Summary: Python HTTP for Humans.\n",
      "Home-page: https://requests.readthedocs.io\n",
      "Author: Kenneth Reitz\n",
      "Author-email: me@kennethreitz.org\n",
      "License: Apache-2.0\n",
      "Location: /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages\n",
      "Requires: certifi, charset_normalizer, idna, urllib3\n",
      "Required-by: CacheControl, cookiecutter, docker, jupyterlab_server, langsmith, nlpia2_wikipedia, poetry, requests-kerberos, requests-toolbelt, sagemaker, sparkmagic, Sphinx, tiktoken\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip show langgraph langchain langchain-openai requests"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "imports-header",
   "metadata": {},
   "source": [
    "### Import Dependencies and Configure Logging\n",
    "\n",
    "**Production Pattern**: Always set up logging at the start of your application.\n",
    "\n",
    "**Why logging over print()**:\n",
    "- **Levels**: DEBUG, INFO, WARNING, ERROR, CRITICAL for filtering\n",
    "- **Timestamps**: Automatic timestamps for debugging\n",
    "- **Configurability**: Easy to redirect to files, disable in production\n",
    "- **Context**: Logger names help identify source of messages\n",
    "\n",
    "**Log levels we'll use**:\n",
    "- `logger.debug()`: Detailed diagnostic information\n",
    "- `logger.info()`: Confirmation that things are working\n",
    "- `logger.warning()`: Something unexpected but recoverable\n",
    "- `logger.error()`: Serious problem, function failed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "imports",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-23 17:07:03 - __main__ - INFO - Dependencies imported successfully\n"
     ]
    }
   ],
   "source": [
    "# Standard library\n",
    "import os\n",
    "import getpass\n",
    "import logging\n",
    "from typing import TypedDict, Annotated, List, Dict, Any, Optional\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "# LangChain/LangGraph\n",
    "from langgraph.graph import StateGraph, START, END\n",
    "from langgraph.graph.message import add_messages\n",
    "from langgraph.checkpoint.memory import MemorySaver\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.messages import HumanMessage, AIMessage, ToolMessage\n",
    "from langchain_core.tools import tool\n",
    "from pydantic import BaseModel, Field\n",
    "\n",
    "# Configure logging\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',\n",
    "    datefmt='%Y-%m-%d %H:%M:%S'\n",
    ")\n",
    "\n",
    "# Create logger for this module\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "logger.info(\"Dependencies imported successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "api-key-header",
   "metadata": {},
   "source": [
    "### Configure API Keys and LLM\n",
    "\n",
    "**Security best practice**: Never hardcode API keys. Always use environment variables or secret management."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "api-setup",
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Enter your OpenAI API key:  ········\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-23 17:07:11 - __main__ - INFO - OpenAI API key configured from user input\n",
      "2025-11-23 17:07:12 - __main__ - INFO - LLM initialized: gpt-4o-mini with temperature=0\n"
     ]
    }
   ],
   "source": [
    "# API key setup\n",
    "if \"OPENAI_API_KEY\" not in os.environ:\n",
    "    os.environ[\"OPENAI_API_KEY\"] = getpass.getpass(\"Enter your OpenAI API key: \")\n",
    "    logger.info(\"OpenAI API key configured from user input\")\n",
    "else:\n",
    "    logger.info(\"OpenAI API key found in environment\")\n",
    "\n",
    "# Initialize LLM\n",
    "try:\n",
    "    llm = ChatOpenAI(model=\"gpt-4o-mini\", temperature=0)\n",
    "    logger.info(\"LLM initialized: gpt-4o-mini with temperature=0\")\n",
    "except Exception as e:\n",
    "    logger.error(f\"Failed to initialize LLM: {e}\")\n",
    "    raise"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "state-header",
   "metadata": {},
   "source": [
    "## Step 1: Design the State Structure\n",
    "\n",
    "### What is State?\n",
    "\n",
    "State is the **memory** of your agent. It persists across multiple turns of conversation and tool executions.\n",
    "\n",
    "### Our State Design\n",
    "\n",
    "For a research assistant, we need to track:\n",
    "\n",
    "1. **`messages`**: All user-agent interactions (conversation history)\n",
    "2. **`research_context`**: Current research topic, active papers, tasks\n",
    "3. **`tool_results`**: Structured outputs from calendar, search, files\n",
    "4. **`preferences`**: Learned behaviors (morning meetings, reading duration)\n",
    "5. **`current_task`**: What the agent is currently working on\n",
    "\n",
    "### Design Decision: Single State vs. Multiple Agents\n",
    "\n",
    "We use a **single state dictionary** instead of multiple agents because:\n",
    "- All tools share the research context\n",
    "- Preferences affect all capabilities (scheduling, file organization)\n",
    "- Conversational flow is unified (one conversation thread)\n",
    "\n",
    "### The `add_messages` Reducer\n",
    "\n",
    "`Annotated[list, add_messages]` is a special LangGraph pattern that:\n",
    "- Automatically appends new messages to the conversation\n",
    "- Handles message deduplication\n",
    "- Maintains chronological order"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "state-definition",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-23 17:07:12 - __main__ - INFO - State structure defined: ResearchAssistantState\n"
     ]
    }
   ],
   "source": [
    "class ResearchAssistantState(TypedDict):\n",
    "    \"\"\"State for academic research assistant agent.\n",
    "    \n",
    "    Attributes:\n",
    "        messages: Conversation history with automatic message appending\n",
    "        research_context: Current research topic, active papers, ongoing tasks\n",
    "        tool_results: Structured results from tool executions\n",
    "        preferences: User preferences learned over time\n",
    "        current_task: Description of what the agent is currently working on\n",
    "        task_queue: Priority-based list of tasks (Challenge 1)\n",
    "        scheduled_events: List of calendar events (Challenge 2)\n",
    "    \"\"\"\n",
    "    messages: Annotated[list, add_messages]\n",
    "    research_context: Dict[str, Any]\n",
    "    tool_results: List[Dict[str, Any]]\n",
    "    preferences: Dict[str, str]\n",
    "    current_task: str\n",
    "    task_queue: List[Dict[str, Any]]  # Challenge 1\n",
    "    scheduled_events: List[Dict[str, Any]]  # Challenge 2\n",
    "\n",
    "\n",
    "def create_initial_state() -> ResearchAssistantState:\n",
    "    \"\"\"Create initial state with default preferences.\n",
    "    \n",
    "    Returns:\n",
    "        ResearchAssistantState with empty messages and default preferences\n",
    "    \"\"\"\n",
    "    logger.debug(\"Creating initial state with default preferences\")\n",
    "    \n",
    "    return {\n",
    "        \"messages\": [],\n",
    "        \"research_context\": {},\n",
    "        \"tool_results\": [],\n",
    "        \"preferences\": {\n",
    "            \"meeting_time\": \"morning\",\n",
    "            \"reading_duration\": \"30min\"\n",
    "        },\n",
    "        \"current_task\": \"\",\n",
    "        \"task_queue\": [],  # Challenge 1\n",
    "        \"scheduled_events\": []  # Challenge 2\n",
    "    }\n",
    "\n",
    "logger.info(\"State structure defined: ResearchAssistantState\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "structured-outputs-header",
   "metadata": {},
   "source": [
    "## Step 2: Define Structured Outputs with Pydantic\n",
    "\n",
    "### Why Pydantic Models?\n",
    "\n",
    "**Production pattern**: Use Pydantic models for tool outputs to ensure:\n",
    "- **Type safety**: Catch errors at definition time, not runtime\n",
    "- **Validation**: Automatic data validation\n",
    "- **Documentation**: Self-documenting code\n",
    "- **IDE support**: Autocomplete and type hints\n",
    "\n",
    "### Our Models\n",
    "\n",
    "We'll define Pydantic models for:\n",
    "1. **Paper**: Represents an academic paper\n",
    "2. **PaperSearchResult**: Structured response from paper search\n",
    "3. **CalendarEvent**: Represents a scheduled event\n",
    "4. **BibliographyEntry**: Citation database entry"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "pydantic-models",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-23 17:07:12 - __main__ - INFO - Pydantic models defined: Paper, PaperSearchResult, CalendarEvent, BibliographyEntry, Task, ConflictCheckResult\n"
     ]
    }
   ],
   "source": [
    "class Paper(BaseModel):\n",
    "    \"\"\"Represents an academic paper.\"\"\"\n",
    "    title: str = Field(description=\"Paper title\")\n",
    "    authors: str = Field(description=\"Comma-separated list of authors\")\n",
    "    year: int = Field(description=\"Publication year\")\n",
    "    abstract: str = Field(description=\"Paper abstract or summary\")\n",
    "    \n",
    "\n",
    "class PaperSearchResult(BaseModel):\n",
    "    \"\"\"Structured result from paper search.\"\"\"\n",
    "    query: str = Field(description=\"Search query used\")\n",
    "    papers: List[Paper] = Field(description=\"List of papers found\")\n",
    "    total_found: int = Field(description=\"Total number of papers found\")\n",
    "    \n",
    "\n",
    "class CalendarEvent(BaseModel):\n",
    "    \"\"\"Represents a calendar event.\"\"\"\n",
    "    title: str = Field(description=\"Event title\")\n",
    "    date: str = Field(description=\"Event date (YYYY-MM-DD)\")\n",
    "    time: str = Field(description=\"Event time (HH:MM AM/PM)\")\n",
    "    duration_min: int = Field(description=\"Duration in minutes\")\n",
    "    \n",
    "\n",
    "class BibliographyEntry(BaseModel):\n",
    "    \"\"\"Represents a bibliography entry.\"\"\"\n",
    "    citation_key: str = Field(description=\"BibTeX citation key\")\n",
    "    title: str = Field(description=\"Paper title\")\n",
    "    authors: str = Field(description=\"Authors\")\n",
    "    year: str = Field(description=\"Publication year\")\n",
    "\n",
    "\n",
    "\n",
    "# Challenge 1: Task Model\n",
    "class Task(BaseModel):\n",
    "    \"\"\"Represents a task with priority.\"\"\"\n",
    "    title: str = Field(description=\"Task description\")\n",
    "    priority: int = Field(description=\"Priority level (1=highest, 5=lowest)\", ge=1, le=5)\n",
    "    status: str = Field(description=\"Task status\", pattern=\"^(pending|in_progress|completed)$\")\n",
    "    created_at: str = Field(description=\"Creation timestamp (ISO format)\")\n",
    "\n",
    "\n",
    "# Challenge 2: Conflict Check Result\n",
    "class ConflictCheckResult(BaseModel):\n",
    "    \"\"\"Result of calendar conflict detection.\"\"\"\n",
    "    has_conflict: bool = Field(description=\"Whether a conflict was detected\")\n",
    "    conflicting_event: Optional[str] = Field(description=\"Title of conflicting event\")\n",
    "    suggested_time: Optional[str] = Field(description=\"Suggested alternative time slot\")\n",
    "    message: str = Field(description=\"Human-readable conflict message\")\n",
    "\n",
    "\n",
    "logger.info(\"Pydantic models defined: Paper, PaperSearchResult, CalendarEvent, BibliographyEntry, Task, ConflictCheckResult\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "tools-header",
   "metadata": {},
   "source": [
    "## Step 3: Build the Tool Suite\n",
    "\n",
    "### Production Tool Pattern\n",
    "\n",
    "Each tool follows this pattern:\n",
    "1. **Structured inputs**: Type hints for all parameters\n",
    "2. **Error handling**: Try/except blocks for all operations\n",
    "3. **Logging**: Log entry, success, and failures\n",
    "4. **Structured outputs**: Return Pydantic models (converted to strings for LLM)\n",
    "5. **Documentation**: Clear docstrings\n",
    "\n",
    "### Our Tool Suite\n",
    "\n",
    "1. **Paper Search**: Find academic papers by query\n",
    "2. **File Organizer**: Create folders, organize files\n",
    "3. **Calendar Manager**: Schedule events, check availability\n",
    "4. **Bibliography Tracker**: Maintain citation database\n",
    "\n",
    "**Note**: These are simulated tools. In production, you'd integrate with:\n",
    "- ArXiv/Semantic Scholar APIs\n",
    "- Google Calendar API\n",
    "- File system operations\n",
    "- SQLite or BibTeX databases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "tool-paper-search",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-23 17:07:12 - __main__ - INFO - Tool registered: search_papers_tool\n"
     ]
    }
   ],
   "source": [
    "@tool\n",
    "def search_papers_tool(query: str, limit: int = 5) -> str:\n",
    "    \"\"\"Search for academic papers on a given topic.\n",
    "    \n",
    "    Args:\n",
    "        query: Search query (e.g., 'transformers', 'neural networks')\n",
    "        limit: Maximum number of papers to return (default: 5)\n",
    "        \n",
    "    Returns:\n",
    "        JSON string of PaperSearchResult with found papers\n",
    "    \"\"\"\n",
    "    logger.info(f\"Searching for papers: query='{query}', limit={limit}\")\n",
    "    \n",
    "    try:\n",
    "        # Simulated paper search (in production: call ArXiv/Semantic Scholar API)\n",
    "        papers = [\n",
    "            Paper(\n",
    "                title=f\"Advances in {query}: A Comprehensive Survey #{i+1}\",\n",
    "                authors=\"Smith, J., Jones, A., Lee, K.\",\n",
    "                year=2024 - i,\n",
    "                abstract=f\"This paper explores {query} using novel methods and provides comprehensive analysis of state-of-the-art approaches.\"\n",
    "            )\n",
    "            for i in range(min(limit, 5))\n",
    "        ]\n",
    "        \n",
    "        result = PaperSearchResult(\n",
    "            query=query,\n",
    "            papers=papers,\n",
    "            total_found=len(papers)\n",
    "        )\n",
    "        \n",
    "        logger.info(f\"Found {len(papers)} papers for query '{query}'\")\n",
    "        return result.model_dump_json(indent=2)\n",
    "        \n",
    "    except Exception as e:\n",
    "        logger.error(f\"Paper search failed: {e}\")\n",
    "        return f\"Error searching for papers: {str(e)}\"\n",
    "\n",
    "logger.info(\"Tool registered: search_papers_tool\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "tool-file-organizer",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-23 17:07:12 - __main__ - INFO - Tool registered: organize_files_tool\n"
     ]
    }
   ],
   "source": [
    "@tool\n",
    "def organize_files_tool(papers: List[str], category: str) -> str:\n",
    "    \"\"\"Organize paper files into categorized folders.\n",
    "    \n",
    "    Args:\n",
    "        papers: List of paper titles to organize\n",
    "        category: Category/folder name (e.g., 'Machine Learning')\n",
    "        \n",
    "    Returns:\n",
    "        Status message with organization details\n",
    "    \"\"\"\n",
    "    logger.info(f\"Organizing {len(papers)} papers into category '{category}'\")\n",
    "    \n",
    "    try:\n",
    "        # Simulated file organization (in production: use pathlib/os)\n",
    "        folder_name = f\"research_{category.lower().replace(' ', '_')}\"\n",
    "        \n",
    "        result = f\"✓ Organized {len(papers)} papers into folder '{folder_name}/':\\n\\n\"\n",
    "        for i, paper in enumerate(papers, 1):\n",
    "            result += f\"  {i}. {paper}\\n\"\n",
    "        \n",
    "        result += f\"\\n✓ Files organized successfully in {folder_name}/\"\n",
    "        \n",
    "        logger.info(f\"Successfully organized {len(papers)} papers\")\n",
    "        return result\n",
    "        \n",
    "    except Exception as e:\n",
    "        logger.error(f\"File organization failed: {e}\")\n",
    "        return f\"Error organizing files: {str(e)}\"\n",
    "\n",
    "logger.info(\"Tool registered: organize_files_tool\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "tool-calendar",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-23 17:07:12 - __main__ - INFO - Tool registered: schedule_event_tool\n"
     ]
    }
   ],
   "source": [
    "@tool\n",
    "def schedule_event_tool(title: str, duration_min: int, preferred_time: str = \"morning\") -> str:\n",
    "    \"\"\"Schedule an event on the calendar.\n",
    "    \n",
    "    Args:\n",
    "        title: Event title\n",
    "        duration_min: Duration in minutes\n",
    "        preferred_time: 'morning', 'afternoon', or 'evening' (default: 'morning')\n",
    "        \n",
    "    Returns:\n",
    "        JSON string of CalendarEvent with scheduled details\n",
    "    \"\"\"\n",
    "    logger.info(f\"Scheduling event: '{title}' for {duration_min} min ({preferred_time})\")\n",
    "    \n",
    "    try:\n",
    "        # Simulated calendar scheduling (in production: call Google Calendar API)\n",
    "        time_map = {\n",
    "            \"morning\": \"10:00 AM\",\n",
    "            \"afternoon\": \"2:00 PM\",\n",
    "            \"evening\": \"5:00 PM\"\n",
    "        }\n",
    "        \n",
    "        time_slot = time_map.get(preferred_time.lower(), \"10:00 AM\")\n",
    "        tomorrow = (datetime.now() + timedelta(days=1)).strftime(\"%Y-%m-%d\")\n",
    "        \n",
    "        event = CalendarEvent(\n",
    "            title=title,\n",
    "            date=tomorrow,\n",
    "            time=time_slot,\n",
    "            duration_min=duration_min\n",
    "        )\n",
    "        \n",
    "        logger.info(f\"Event scheduled: {title} on {tomorrow} at {time_slot}\")\n",
    "        return event.model_dump_json(indent=2)\n",
    "        \n",
    "    except Exception as e:\n",
    "        logger.error(f\"Calendar scheduling failed: {e}\")\n",
    "        return f\"Error scheduling event: {str(e)}\"\n",
    "\n",
    "logger.info(\"Tool registered: schedule_event_tool\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "tool-bibliography",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-23 17:07:12 - __main__ - INFO - Tool registered: update_bibliography_tool\n"
     ]
    }
   ],
   "source": [
    "@tool\n",
    "def update_bibliography_tool(paper_title: str, authors: str, year: str) -> str:\n",
    "    \"\"\"Add a paper to the bibliography database.\n",
    "    \n",
    "    Args:\n",
    "        paper_title: Full title of the paper\n",
    "        authors: Comma-separated list of authors\n",
    "        year: Publication year\n",
    "        \n",
    "    Returns:\n",
    "        JSON string of BibliographyEntry with citation key\n",
    "    \"\"\"\n",
    "    logger.info(f\"Adding to bibliography: '{paper_title}' ({year})\")\n",
    "    \n",
    "    try:\n",
    "        # Simulated bibliography update (in production: use SQLite/BibTeX)\n",
    "        first_author_last = authors.split(',')[0].split()[-1]\n",
    "        citation_key = f\"{first_author_last}{year}\"\n",
    "        \n",
    "        entry = BibliographyEntry(\n",
    "            citation_key=citation_key,\n",
    "            title=paper_title,\n",
    "            authors=authors,\n",
    "            year=year\n",
    "        )\n",
    "        \n",
    "        logger.info(f\"Bibliography entry created: {citation_key}\")\n",
    "        return entry.model_dump_json(indent=2)\n",
    "        \n",
    "    except Exception as e:\n",
    "        logger.error(f\"Bibliography update failed: {e}\")\n",
    "        return f\"Error updating bibliography: {str(e)}\"\n",
    "\n",
    "logger.info(\"Tool registered: update_bibliography_tool\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "tools-binding-header",
   "metadata": {},
   "source": [
    "### Bind Tools to LLM\n",
    "\n",
    "Now we bind all tools to the LLM, enabling it to call them during conversation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "tools-binding",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-23 17:07:12 - __main__ - INFO - Successfully bound 4 tools to LLM\n"
     ]
    }
   ],
   "source": [
    "# Collect all tools\n",
    "tools = [\n",
    "    search_papers_tool,\n",
    "    organize_files_tool,\n",
    "    schedule_event_tool,\n",
    "    update_bibliography_tool\n",
    "]\n",
    "\n",
    "# Bind tools to LLM\n",
    "try:\n",
    "    llm_with_tools = llm.bind_tools(tools)\n",
    "    logger.info(f\"Successfully bound {len(tools)} tools to LLM\")\n",
    "except Exception as e:\n",
    "    logger.error(f\"Failed to bind tools: {e}\")\n",
    "    raise"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "graph-nodes-header",
   "metadata": {},
   "source": [
    "## Step 4: Build the Agent Graph\n",
    "\n",
    "### LangGraph Architecture\n",
    "\n",
    "Our agent follows this workflow:\n",
    "\n",
    "```\n",
    "START → agent → [has tool calls?] → tools → agent → END\n",
    "                      ↓ (no)\n",
    "                     END\n",
    "```\n",
    "\n",
    "**Nodes**:\n",
    "1. **agent**: Reasons about user input and decides which tools to call\n",
    "2. **tools**: Executes tool calls and returns results\n",
    "\n",
    "**Conditional edges**:\n",
    "- If agent produces tool calls → go to **tools** node\n",
    "- If no tool calls → conversation is complete, go to **END**\n",
    "\n",
    "### Error Handling in Nodes\n",
    "\n",
    "Each node wraps operations in try/except blocks to:\n",
    "- Catch and log errors\n",
    "- Return helpful error messages to the LLM\n",
    "- Prevent the agent from crashing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "agent-node",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-23 17:07:12 - __main__ - INFO - Agent node defined\n"
     ]
    }
   ],
   "source": [
    "def agent_node(state: ResearchAssistantState) -> ResearchAssistantState:\n",
    "    \"\"\"Main agent reasoning node.\n",
    "    \n",
    "    Receives current state, invokes LLM with tools, and returns updated state.\n",
    "    \n",
    "    Args:\n",
    "        state: Current agent state\n",
    "        \n",
    "    Returns:\n",
    "        Updated state with agent's response\n",
    "    \"\"\"\n",
    "    logger.debug(\"Agent node: Processing request\")\n",
    "    \n",
    "    try:\n",
    "        messages = state[\"messages\"]\n",
    "        logger.debug(f\"Agent node: {len(messages)} messages in history\")\n",
    "        \n",
    "        # Invoke LLM with bound tools\n",
    "        response = llm_with_tools.invoke(messages)\n",
    "        \n",
    "        # Check if tool calls were made\n",
    "        if hasattr(response, \"tool_calls\") and response.tool_calls:\n",
    "            logger.info(f\"Agent requested {len(response.tool_calls)} tool call(s)\")\n",
    "        else:\n",
    "            logger.info(\"Agent completed without tool calls\")\n",
    "        \n",
    "        return {\"messages\": [response]}\n",
    "        \n",
    "    except Exception as e:\n",
    "        logger.error(f\"Agent node failed: {e}\")\n",
    "        error_msg = AIMessage(content=f\"I encountered an error: {str(e)}. Please try again.\")\n",
    "        return {\"messages\": [error_msg]}\n",
    "\n",
    "logger.info(\"Agent node defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "tools-node",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-23 17:07:12 - __main__ - INFO - Tools node defined\n"
     ]
    }
   ],
   "source": [
    "def tool_node(state: ResearchAssistantState) -> ResearchAssistantState:\n",
    "    \"\"\"Execute tool calls from agent.\n",
    "    \n",
    "    Extracts tool calls from last message, executes each tool,\n",
    "    and returns results as ToolMessages.\n",
    "    \n",
    "    Args:\n",
    "        state: Current agent state\n",
    "        \n",
    "    Returns:\n",
    "        Updated state with tool results\n",
    "    \"\"\"\n",
    "    logger.debug(\"Tools node: Executing tool calls\")\n",
    "    \n",
    "    messages = state[\"messages\"]\n",
    "    last_message = messages[-1]\n",
    "    \n",
    "    tool_results = []\n",
    "    new_messages = []\n",
    "    \n",
    "    # Execute each tool call\n",
    "    for tool_call in last_message.tool_calls:\n",
    "        tool_name = tool_call[\"name\"]\n",
    "        tool_args = tool_call[\"args\"]\n",
    "        tool_call_id = tool_call[\"id\"]\n",
    "        \n",
    "        logger.info(f\"Executing tool: {tool_name} with args: {tool_args}\")\n",
    "        \n",
    "        try:\n",
    "            # Find and execute the tool\n",
    "            tool_map = {tool.name: tool for tool in tools}\n",
    "            \n",
    "            if tool_name not in tool_map:\n",
    "                error_msg = f\"Unknown tool: {tool_name}\"\n",
    "                logger.error(error_msg)\n",
    "                result = error_msg\n",
    "            else:\n",
    "                result = tool_map[tool_name].invoke(tool_args)\n",
    "                logger.info(f\"Tool {tool_name} executed successfully\")\n",
    "            \n",
    "            # Store result\n",
    "            tool_results.append({\n",
    "                \"tool\": tool_name,\n",
    "                \"result\": result,\n",
    "                \"timestamp\": datetime.now().isoformat()\n",
    "            })\n",
    "            \n",
    "            # Create tool message\n",
    "            new_messages.append(\n",
    "                ToolMessage(content=str(result), tool_call_id=tool_call_id)\n",
    "            )\n",
    "            \n",
    "        except Exception as e:\n",
    "            error_msg = f\"Tool {tool_name} failed: {str(e)}\"\n",
    "            logger.error(error_msg)\n",
    "            \n",
    "            # Return error as tool message\n",
    "            new_messages.append(\n",
    "                ToolMessage(content=error_msg, tool_call_id=tool_call_id)\n",
    "            )\n",
    "    \n",
    "    logger.info(f\"Tools node: Executed {len(tool_results)} tool(s)\")\n",
    "    return {\"messages\": new_messages, \"tool_results\": tool_results}\n",
    "\n",
    "logger.info(\"Tools node defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "graph-construction-header",
   "metadata": {},
   "source": [
    "### Construct the Graph\n",
    "\n",
    "Now we'll build the complete workflow graph:\n",
    "\n",
    "1. Define the conditional edge logic\n",
    "2. Create the StateGraph\n",
    "3. Add nodes and edges\n",
    "4. Compile with memory checkpointer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "graph-construction",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-23 17:07:12 - __main__ - INFO - Building agent graph...\n",
      "2025-11-23 17:07:12 - __main__ - INFO - ✓ Agent graph compiled successfully with memory checkpointer\n"
     ]
    }
   ],
   "source": [
    "def should_continue(state: ResearchAssistantState) -> str:\n",
    "    \"\"\"Determine if we should continue to tools or end.\n",
    "    \n",
    "    Args:\n",
    "        state: Current agent state\n",
    "        \n",
    "    Returns:\n",
    "        'tools' if tool calls present, END otherwise\n",
    "    \"\"\"\n",
    "    messages = state[\"messages\"]\n",
    "    last_message = messages[-1]\n",
    "    \n",
    "    # If there are tool calls, continue to tools node\n",
    "    if hasattr(last_message, \"tool_calls\") and last_message.tool_calls:\n",
    "        logger.debug(\"Conditional edge: Routing to tools node\")\n",
    "        return \"tools\"\n",
    "    \n",
    "    # Otherwise, end\n",
    "    logger.debug(\"Conditional edge: Routing to END\")\n",
    "    return END\n",
    "\n",
    "\n",
    "# Create the graph\n",
    "logger.info(\"Building agent graph...\")\n",
    "\n",
    "try:\n",
    "    builder = StateGraph(ResearchAssistantState)\n",
    "    \n",
    "    # Add nodes\n",
    "    builder.add_node(\"agent\", agent_node)\n",
    "    builder.add_node(\"tools\", tool_node)\n",
    "    logger.debug(\"Nodes added: agent, tools\")\n",
    "    \n",
    "    # Define flow\n",
    "    builder.add_edge(START, \"agent\")\n",
    "    builder.add_conditional_edges(\n",
    "        \"agent\",\n",
    "        should_continue,\n",
    "        {\"tools\": \"tools\", END: END}\n",
    "    )\n",
    "    builder.add_edge(\"tools\", \"agent\")\n",
    "    logger.debug(\"Edges added: START->agent, agent->tools/END, tools->agent\")\n",
    "    \n",
    "    # Compile with memory\n",
    "    memory = MemorySaver()\n",
    "    agent = builder.compile(checkpointer=memory)\n",
    "    \n",
    "    logger.info(\"✓ Agent graph compiled successfully with memory checkpointer\")\n",
    "    \n",
    "except Exception as e:\n",
    "    logger.error(f\"Failed to build agent graph: {e}\")\n",
    "    raise"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "graph-visualization-header",
   "metadata": {},
   "source": [
    "## Step 5: Visualize the Agent Workflow\n",
    "\n",
    "### Understanding the Graph\n",
    "\n",
    "LangGraph provides built-in visualization to help you understand the agent's workflow.\n",
    "\n",
    "**What you'll see**:\n",
    "- **Nodes**: Circles representing computation steps (agent, tools)\n",
    "- **Edges**: Arrows showing flow between steps\n",
    "- **Conditional edges**: Diamond shapes showing decision points\n",
    "\n",
    "This visualization is crucial for:\n",
    "- Debugging complex workflows\n",
    "- Understanding agent behavior\n",
    "- Communicating architecture to teammates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "graph-visualization",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-23 17:07:12 - __main__ - INFO - Graph visualization displayed\n"
     ]
    }
   ],
   "source": [
    "from IPython.display import Image, display\n",
    "\n",
    "try:\n",
    "    # Generate graph visualization\n",
    "    graph_image = agent.get_graph().draw_mermaid_png()\n",
    "    \n",
    "    # Display in notebook\n",
    "    display(Image(graph_image))\n",
    "    \n",
    "    logger.info(\"Graph visualization displayed\")\n",
    "    \n",
    "except Exception as e:\n",
    "    logger.warning(f\"Could not display graph visualization: {e}\")\n",
    "    logger.info(\"Graph structure: START -> agent -> [conditional] -> tools -> agent -> END\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "thread-id-header",
   "metadata": {},
   "source": [
    "## Understanding thread_id: Session Isolation\n",
    "\n",
    "### What is thread_id?\n",
    "\n",
    "In LangGraph, `thread_id` is a **session identifier** that isolates conversation state.\n",
    "\n",
    "### How it Works\n",
    "\n",
    "```python\n",
    "    config = {\"configurable\": {\"thread_id\": \"user-123\"}}\n",
    "```\n",
    "\n",
    "- **Same thread_id**: Messages and state are shared (multi-turn conversation)\n",
    "- **Different thread_id**: Completely isolated sessions\n",
    "\n",
    "### Why This Matters\n",
    "\n",
    "**Scenario 1: Same user, same conversation** (example code)\n",
    "```python\n",
    "    config = {\"configurable\": {\"thread_id\": \"user-123\"}}\n",
    "\n",
    "    # Turn 1\n",
    "    chat(\"Find papers on transformers\", config)\n",
    "    # Agent remembers: query=\"transformers\", found 5 papers\n",
    "\n",
    "    # Turn 2 (same thread)\n",
    "    chat(\"Schedule time to read the first one\", config)\n",
    "    # Agent knows which paper: \"Advances in transformers: Survey #1\"\n",
    "```\n",
    "\n",
    "**Scenario 2: Different users** (example code)\n",
    "```python\n",
    "    config_alice = {\"configurable\": {\"thread_id\": \"alice-session\"}}\n",
    "    config_bob = {\"configurable\": {\"thread_id\": \"bob-session\"}}\n",
    "\n",
    "    # Alice's session\n",
    "    chat(\"Find papers on NLP\", config_alice)\n",
    "\n",
    "    # Bob's session (isolated from Alice)\n",
    "    chat(\"Find papers on computer vision\", config_bob)\n",
    "```\n",
    "\n",
    "### Production Use Cases\n",
    "\n",
    "- **Web applications**: `thread_id = user_id` or `session_id`\n",
    "- **Slack bots**: `thread_id = channel_id + thread_ts`\n",
    "- **Multi-tenant systems**: `thread_id = tenant_id + conversation_id`\n",
    "\n",
    "### Best Practices\n",
    "\n",
    "1. **Use meaningful IDs**: `user-123-session-456`, not `thread-1`\n",
    "2. **Document your schema**: What does each thread_id represent?\n",
    "3. **Handle cleanup**: Old threads consume memory (implement expiration)\n",
    "4. **Test isolation**: Verify different threads don't leak state"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "chat-interface-header",
   "metadata": {},
   "source": [
    "## Step 6: Build the Chat Interface\n",
    "\n",
    "### Chat Function Design\n",
    "\n",
    "Our chat function:\n",
    "1. Retrieves or creates state for the given thread\n",
    "2. Adds user message to state\n",
    "3. Invokes agent graph\n",
    "4. Returns agent's response\n",
    "5. Automatically persists state for future turns\n",
    "\n",
    "**Error handling**: We wrap the entire conversation in try/except to gracefully handle failures."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "chat-interface",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-23 17:07:12 - __main__ - INFO - Chat interface ready\n"
     ]
    }
   ],
   "source": [
    "def chat(user_input: str, config: dict) -> str:\n",
    "    \"\"\"Send a message and get response.\n",
    "    \n",
    "    Args:\n",
    "        user_input: User's message\n",
    "        config: Configuration dict with thread_id\n",
    "        \n",
    "    Returns:\n",
    "        Agent's response text\n",
    "    \"\"\"\n",
    "    thread_id = config.get(\"configurable\", {}).get(\"thread_id\", \"unknown\")\n",
    "    logger.info(f\"Chat request [thread={thread_id}]: {user_input[:50]}...\")\n",
    "    \n",
    "    try:\n",
    "        # Get current state or create new\n",
    "        state = agent.get_state(config)\n",
    "        \n",
    "        if state.values:\n",
    "            current_state = state.values\n",
    "            logger.debug(f\"Retrieved existing state for thread {thread_id}\")\n",
    "        else:\n",
    "            current_state = create_initial_state()\n",
    "            logger.debug(f\"Created new state for thread {thread_id}\")\n",
    "        \n",
    "        # Add user message\n",
    "        current_state[\"messages\"].append(HumanMessage(content=user_input))\n",
    "        \n",
    "        # Run agent\n",
    "        result = agent.invoke(current_state, config)\n",
    "        \n",
    "        # Extract last AI message\n",
    "        response = result[\"messages\"][-1].content\n",
    "        \n",
    "        logger.info(f\"Chat response [thread={thread_id}]: {response[:50]}...\")\n",
    "        return response\n",
    "        \n",
    "    except Exception as e:\n",
    "        logger.error(f\"Chat failed [thread={thread_id}]: {e}\")\n",
    "        return f\"I encountered an error: {str(e)}. Please try again.\"\n",
    "\n",
    "logger.info(\"Chat interface ready\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "learning-header",
   "metadata": {},
   "source": [
    "## Step 7: Learn from User Corrections\n",
    "\n",
    "### Adaptive Learning Pattern\n",
    "\n",
    "When users correct the agent, we update preferences:\n",
    "\n",
    "**Example**:\n",
    "```\n",
    "User: \"Schedule a review meeting\"\n",
    "Agent: [schedules for 2pm]\n",
    "User: \"No, I prefer mornings\"\n",
    "Agent: [updates preferences] \"Changed to 10am. I'll remember.\"\n",
    "```\n",
    "\n",
    "### Detection Logic\n",
    "\n",
    "We detect corrections by looking for:\n",
    "1. **Negation words**: \"no\", \"actually\", \"instead\"\n",
    "2. **Preference indicators**: \"prefer\", \"like\", \"better\"\n",
    "3. **Specific values**: \"morning\", \"afternoon\", \"30 minutes\"\n",
    "\n",
    "### Implementation\n",
    "\n",
    "The `detect_correction()` function extracts structured corrections from natural language."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "learning-detection",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-23 17:07:12 - __main__ - INFO - Correction detection function defined\n"
     ]
    }
   ],
   "source": [
    "def detect_correction(message: str) -> Dict[str, str]:\n",
    "    \"\"\"Detect if user is correcting agent behavior.\n",
    "    \n",
    "    Args:\n",
    "        message: User's message\n",
    "        \n",
    "    Returns:\n",
    "        Dictionary of detected corrections (e.g., {'meeting_time': 'morning'})\n",
    "    \"\"\"\n",
    "    logger.debug(f\"Checking for corrections in: {message}\")\n",
    "    \n",
    "    message_lower = message.lower()\n",
    "    corrections = {}\n",
    "    \n",
    "    try:\n",
    "        # Check for time preference corrections\n",
    "        if any(word in message_lower for word in [\"no\", \"actually\", \"prefer\", \"instead\"]):\n",
    "            if \"morning\" in message_lower:\n",
    "                corrections[\"meeting_time\"] = \"morning\"\n",
    "                logger.info(\"Detected preference: morning meetings\")\n",
    "            elif \"afternoon\" in message_lower:\n",
    "                corrections[\"meeting_time\"] = \"afternoon\"\n",
    "                logger.info(\"Detected preference: afternoon meetings\")\n",
    "            elif \"evening\" in message_lower:\n",
    "                corrections[\"meeting_time\"] = \"evening\"\n",
    "                logger.info(\"Detected preference: evening meetings\")\n",
    "        \n",
    "        # Check for duration preferences\n",
    "        if \"30\" in message_lower and \"min\" in message_lower:\n",
    "            corrections[\"reading_duration\"] = \"30min\"\n",
    "            logger.info(\"Detected preference: 30min duration\")\n",
    "        elif \"60\" in message_lower and \"min\" in message_lower:\n",
    "            corrections[\"reading_duration\"] = \"60min\"\n",
    "            logger.info(\"Detected preference: 60min duration\")\n",
    "    \n",
    "    except Exception as e:\n",
    "        logger.error(f\"Correction detection failed: {e}\")\n",
    "    \n",
    "    return corrections\n",
    "\n",
    "logger.info(\"Correction detection function defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "learning-chat",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-23 17:07:12 - __main__ - INFO - Learning-enabled chat interface ready\n"
     ]
    }
   ],
   "source": [
    "def chat_with_learning(user_input: str, config: dict) -> str:\n",
    "    \"\"\"Chat with preference learning.\n",
    "    \n",
    "    Extends basic chat to detect and learn from user corrections.\n",
    "    \n",
    "    Args:\n",
    "        user_input: User's message\n",
    "        config: Configuration dict with thread_id\n",
    "        \n",
    "    Returns:\n",
    "        Agent's response text\n",
    "    \"\"\"\n",
    "    thread_id = config.get(\"configurable\", {}).get(\"thread_id\", \"unknown\")\n",
    "    logger.info(f\"Chat with learning [thread={thread_id}]: {user_input[:50]}...\")\n",
    "    \n",
    "    try:\n",
    "        # Get current state or create new\n",
    "        state = agent.get_state(config)\n",
    "        \n",
    "        if state.values:\n",
    "            current_state = state.values\n",
    "        else:\n",
    "            current_state = create_initial_state()\n",
    "        \n",
    "        # Check for corrections and update preferences\n",
    "        corrections = detect_correction(user_input)\n",
    "        if corrections:\n",
    "            current_state[\"preferences\"].update(corrections)\n",
    "            logger.info(f\"Updated preferences [thread={thread_id}]: {corrections}\")\n",
    "        \n",
    "        # Add user message\n",
    "        current_state[\"messages\"].append(HumanMessage(content=user_input))\n",
    "        \n",
    "        # Run agent\n",
    "        result = agent.invoke(current_state, config)\n",
    "        \n",
    "        # Extract response\n",
    "        response = result[\"messages\"][-1].content\n",
    "        \n",
    "        logger.info(f\"Response [thread={thread_id}]: {response[:50]}...\")\n",
    "        return response\n",
    "        \n",
    "    except Exception as e:\n",
    "        logger.error(f\"Chat with learning failed [thread={thread_id}]: {e}\")\n",
    "        return f\"I encountered an error: {str(e)}. Please try again.\"\n",
    "\n",
    "logger.info(\"Learning-enabled chat interface ready\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "testing-header",
   "metadata": {},
   "source": [
    "## Step 8: Test the Agent\n",
    "\n",
    "### Test Scenarios\n",
    "\n",
    "We'll validate:\n",
    "\n",
    "1. **Basic workflow**: Search papers + organize + add to bibliography\n",
    "2. **Multi-turn context**: Agent remembers previous results\n",
    "3. **Learning from corrections**: Preferences persist across turns\n",
    "4. **Error handling**: Graceful failure recovery\n",
    "\n",
    "### Expected Behavior\n",
    "\n",
    "Each test should:\n",
    "- Execute without exceptions\n",
    "- Log all operations\n",
    "- Return coherent responses\n",
    "- Maintain context across turns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "test1-header",
   "metadata": {},
   "source": [
    "### Test 1: Basic Workflow\n",
    "\n",
    "**Goal**: Verify agent can search papers, organize them, and update bibliography in a single request."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "test1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-23 17:07:12 - __main__ - INFO - ============================================================\n",
      "2025-11-23 17:07:12 - __main__ - INFO - TEST 1: Basic Workflow\n",
      "2025-11-23 17:07:12 - __main__ - INFO - ============================================================\n",
      "2025-11-23 17:07:12 - __main__ - INFO - Chat with learning [thread=test-basic-workflow]: Find 3 papers on transformers and add the first on...\n",
      "2025-11-23 17:07:13 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-11-23 17:07:13 - __main__ - INFO - Agent requested 1 tool call(s)\n",
      "2025-11-23 17:07:13 - __main__ - INFO - Executing tool: search_papers_tool with args: {'query': 'transformers', 'limit': 3}\n",
      "2025-11-23 17:07:13 - __main__ - INFO - Searching for papers: query='transformers', limit=3\n",
      "2025-11-23 17:07:13 - __main__ - INFO - Found 3 papers for query 'transformers'\n",
      "2025-11-23 17:07:13 - __main__ - INFO - Tool search_papers_tool executed successfully\n",
      "2025-11-23 17:07:13 - __main__ - INFO - Tools node: Executed 1 tool(s)\n",
      "2025-11-23 17:07:15 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-11-23 17:07:15 - __main__ - INFO - Agent requested 1 tool call(s)\n",
      "2025-11-23 17:07:15 - __main__ - INFO - Executing tool: update_bibliography_tool with args: {'paper_title': 'Advances in transformers: A Comprehensive Survey #1', 'authors': 'Smith, J., Jones, A., Lee, K.', 'year': '2024'}\n",
      "2025-11-23 17:07:15 - __main__ - INFO - Adding to bibliography: 'Advances in transformers: A Comprehensive Survey #1' (2024)\n",
      "2025-11-23 17:07:15 - __main__ - INFO - Bibliography entry created: Smith2024\n",
      "2025-11-23 17:07:15 - __main__ - INFO - Tool update_bibliography_tool executed successfully\n",
      "2025-11-23 17:07:15 - __main__ - INFO - Tools node: Executed 1 tool(s)\n",
      "2025-11-23 17:07:20 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-11-23 17:07:20 - __main__ - INFO - Agent completed without tool calls\n",
      "2025-11-23 17:07:20 - __main__ - INFO - Response [thread=test-basic-workflow]: I found 3 papers on transformers:\n",
      "\n",
      "1. **Advances i...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "USER: Find 3 papers on transformers and add the first one to my bibliography\n",
      "============================================================\n",
      "AGENT: I found 3 papers on transformers:\n",
      "\n",
      "1. **Advances in transformers: A Comprehensive Survey #1**\n",
      "   - **Authors:** Smith, J., Jones, A., Lee, K.\n",
      "   - **Year:** 2024\n",
      "   - **Abstract:** This paper explores transformers using novel methods and provides comprehensive analysis of state-of-the-art approaches.\n",
      "\n",
      "2. **Advances in transformers: A Comprehensive Survey #2**\n",
      "   - **Authors:** Smith, J., Jones, A., Lee, K.\n",
      "   - **Year:** 2023\n",
      "   - **Abstract:** This paper explores transformers using novel methods and provides comprehensive analysis of state-of-the-art approaches.\n",
      "\n",
      "3. **Advances in transformers: A Comprehensive Survey #3**\n",
      "   - **Authors:** Smith, J., Jones, A., Lee, K.\n",
      "   - **Year:** 2022\n",
      "   - **Abstract:** This paper explores transformers using novel methods and provides comprehensive analysis of state-of-the-art approaches.\n",
      "\n",
      "I have added the first paper to your bibliography with the citation key **Smith2024**.\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "logger.info(\"=\"*60)\n",
    "logger.info(\"TEST 1: Basic Workflow\")\n",
    "logger.info(\"=\"*60)\n",
    "\n",
    "config = {\"configurable\": {\"thread_id\": \"test-basic-workflow\"}}\n",
    "\n",
    "response = chat_with_learning(\n",
    "    \"Find 3 papers on transformers and add the first one to my bibliography\",\n",
    "    config\n",
    ")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"USER: Find 3 papers on transformers and add the first one to my bibliography\")\n",
    "print(\"=\"*60)\n",
    "print(f\"AGENT: {response}\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "test2-header",
   "metadata": {},
   "source": [
    "### Test 2: Multi-Turn Context\n",
    "\n",
    "**Goal**: Verify agent maintains context across multiple conversation turns.\n",
    "\n",
    "**Turn 1**: Search for papers  \n",
    "**Turn 2**: Schedule time to read (agent must remember papers from Turn 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "test2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-23 17:07:20 - __main__ - INFO - ============================================================\n",
      "2025-11-23 17:07:20 - __main__ - INFO - TEST 2: Multi-Turn Context\n",
      "2025-11-23 17:07:20 - __main__ - INFO - ============================================================\n",
      "2025-11-23 17:07:20 - __main__ - INFO - Chat with learning [thread=test-multi-turn]: Find papers on attention mechanisms...\n",
      "2025-11-23 17:07:21 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-11-23 17:07:21 - __main__ - INFO - Agent requested 1 tool call(s)\n",
      "2025-11-23 17:07:21 - __main__ - INFO - Executing tool: search_papers_tool with args: {'query': 'attention mechanisms'}\n",
      "2025-11-23 17:07:21 - __main__ - INFO - Searching for papers: query='attention mechanisms', limit=5\n",
      "2025-11-23 17:07:21 - __main__ - INFO - Found 5 papers for query 'attention mechanisms'\n",
      "2025-11-23 17:07:21 - __main__ - INFO - Tool search_papers_tool executed successfully\n",
      "2025-11-23 17:07:21 - __main__ - INFO - Tools node: Executed 1 tool(s)\n",
      "2025-11-23 17:07:29 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-11-23 17:07:29 - __main__ - INFO - Agent completed without tool calls\n",
      "2025-11-23 17:07:29 - __main__ - INFO - Response [thread=test-multi-turn]: Here are some papers on attention mechanisms:\n",
      "\n",
      "1. ...\n",
      "2025-11-23 17:07:29 - __main__ - INFO - Chat with learning [thread=test-multi-turn]: Schedule 30 minutes to read the top paper tomorrow...\n",
      "2025-11-23 17:07:29 - __main__ - INFO - Detected preference: 30min duration\n",
      "2025-11-23 17:07:29 - __main__ - INFO - Updated preferences [thread=test-multi-turn]: {'reading_duration': '30min'}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "TURN 1\n",
      "============================================================\n",
      "USER: Find papers on attention mechanisms\n",
      "============================================================\n",
      "AGENT: Here are some papers on attention mechanisms:\n",
      "\n",
      "1. **Title:** Advances in attention mechanisms: A Comprehensive Survey #1  \n",
      "   **Authors:** Smith, J., Jones, A., Lee, K.  \n",
      "   **Year:** 2024  \n",
      "   **Abstract:** This paper explores attention mechanisms using novel methods and provides comprehensive analysis of state-of-the-art approaches.\n",
      "\n",
      "2. **Title:** Advances in attention mechanisms: A Comprehensive Survey #2  \n",
      "   **Authors:** Smith, J., Jones, A., Lee, K.  \n",
      "   **Year:** 2023  \n",
      "   **Abstract:** This paper explores attention mechanisms using novel methods and provides comprehensive analysis of state-of-the-art approaches.\n",
      "\n",
      "3. **Title:** Advances in attention mechanisms: A Comprehensive Survey #3  \n",
      "   **Authors:** Smith, J., Jones, A., Lee, K.  \n",
      "   **Year:** 2022  \n",
      "   **Abstract:** This paper explores attention mechanisms using novel methods and provides comprehensive analysis of state-of-the-art approaches.\n",
      "\n",
      "4. **Title:** Advances in attention mechanisms: A Comprehensive Survey #4  \n",
      "   **Authors:** Smith, J., Jones, A., Lee, K.  \n",
      "   **Year:** 2021  \n",
      "   **Abstract:** This paper explores attention mechanisms using novel methods and provides comprehensive analysis of state-of-the-art approaches.\n",
      "\n",
      "5. **Title:** Advances in attention mechanisms: A Comprehensive Survey #5  \n",
      "   **Authors:** Smith, J., Jones, A., Lee, K.  \n",
      "   **Year:** 2020  \n",
      "   **Abstract:** This paper explores attention mechanisms using novel methods and provides comprehensive analysis of state-of-the-art approaches.\n",
      "\n",
      "If you need more information or specific details about any of these papers, let me know!\n",
      "============================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-23 17:07:30 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-11-23 17:07:30 - __main__ - INFO - Agent requested 1 tool call(s)\n",
      "2025-11-23 17:07:30 - __main__ - INFO - Executing tool: schedule_event_tool with args: {'title': \"Read 'Advances in attention mechanisms: A Comprehensive Survey #1'\", 'duration_min': 30, 'preferred_time': 'morning'}\n",
      "2025-11-23 17:07:30 - __main__ - INFO - Scheduling event: 'Read 'Advances in attention mechanisms: A Comprehensive Survey #1'' for 30 min (morning)\n",
      "2025-11-23 17:07:30 - __main__ - INFO - Event scheduled: Read 'Advances in attention mechanisms: A Comprehensive Survey #1' on 2025-11-24 at 10:00 AM\n",
      "2025-11-23 17:07:30 - __main__ - INFO - Tool schedule_event_tool executed successfully\n",
      "2025-11-23 17:07:30 - __main__ - INFO - Tools node: Executed 1 tool(s)\n",
      "2025-11-23 17:07:32 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-11-23 17:07:32 - __main__ - INFO - Agent completed without tool calls\n",
      "2025-11-23 17:07:32 - __main__ - INFO - Response [thread=test-multi-turn]: Your reading session has been scheduled for tomorr...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "TURN 2\n",
      "============================================================\n",
      "USER: Schedule 30 minutes to read the top paper tomorrow morning\n",
      "============================================================\n",
      "AGENT: Your reading session has been scheduled for tomorrow morning:\n",
      "\n",
      "- **Event:** Read 'Advances in attention mechanisms: A Comprehensive Survey #1'\n",
      "- **Date:** November 24, 2025\n",
      "- **Time:** 10:00 AM\n",
      "- **Duration:** 30 minutes\n",
      "\n",
      "If you need any further assistance, feel free to ask!\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "logger.info(\"=\"*60)\n",
    "logger.info(\"TEST 2: Multi-Turn Context\")\n",
    "logger.info(\"=\"*60)\n",
    "\n",
    "config2 = {\"configurable\": {\"thread_id\": \"test-multi-turn\"}}\n",
    "\n",
    "# Turn 1\n",
    "response1 = chat_with_learning(\n",
    "    \"Find papers on attention mechanisms\",\n",
    "    config2\n",
    ")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"TURN 1\")\n",
    "print(\"=\"*60)\n",
    "print(\"USER: Find papers on attention mechanisms\")\n",
    "print(\"=\"*60)\n",
    "print(f\"AGENT: {response1}\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Turn 2\n",
    "response2 = chat_with_learning(\n",
    "    \"Schedule 30 minutes to read the top paper tomorrow morning\",\n",
    "    config2\n",
    ")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"TURN 2\")\n",
    "print(\"=\"*60)\n",
    "print(\"USER: Schedule 30 minutes to read the top paper tomorrow morning\")\n",
    "print(\"=\"*60)\n",
    "print(f\"AGENT: {response2}\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "test3-header",
   "metadata": {},
   "source": [
    "### Test 3: Learning from Corrections\n",
    "\n",
    "**Goal**: Verify agent learns and applies user preferences.\n",
    "\n",
    "**Turn 1**: Schedule meeting (default: morning)  \n",
    "**Turn 2**: User corrects: \"No, I prefer afternoons\"  \n",
    "**Turn 3**: Schedule another meeting (should use afternoon)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "test3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-23 17:07:32 - __main__ - INFO - ============================================================\n",
      "2025-11-23 17:07:32 - __main__ - INFO - TEST 3: Learning from Corrections\n",
      "2025-11-23 17:07:32 - __main__ - INFO - ============================================================\n",
      "2025-11-23 17:07:32 - __main__ - INFO - Chat with learning [thread=test-learning]: Schedule a paper review meeting for 60 minutes...\n",
      "2025-11-23 17:07:32 - __main__ - INFO - Detected preference: 60min duration\n",
      "2025-11-23 17:07:32 - __main__ - INFO - Updated preferences [thread=test-learning]: {'reading_duration': '60min'}\n",
      "2025-11-23 17:07:33 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-11-23 17:07:33 - __main__ - INFO - Agent completed without tool calls\n",
      "2025-11-23 17:07:33 - __main__ - INFO - Response [thread=test-learning]: What preferred time would you like for the meeting...\n",
      "2025-11-23 17:07:33 - __main__ - INFO - Chat with learning [thread=test-learning]: No, I prefer afternoon meetings...\n",
      "2025-11-23 17:07:33 - __main__ - INFO - Detected preference: afternoon meetings\n",
      "2025-11-23 17:07:33 - __main__ - INFO - Updated preferences [thread=test-learning]: {'meeting_time': 'afternoon'}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "TURN 1\n",
      "============================================================\n",
      "USER: Schedule a paper review meeting for 60 minutes\n",
      "============================================================\n",
      "AGENT: What preferred time would you like for the meeting? You can choose from morning, afternoon, or evening.\n",
      "============================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-23 17:07:34 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-11-23 17:07:34 - __main__ - INFO - Agent requested 1 tool call(s)\n",
      "2025-11-23 17:07:34 - __main__ - INFO - Executing tool: schedule_event_tool with args: {'title': 'Paper Review Meeting', 'duration_min': 60, 'preferred_time': 'afternoon'}\n",
      "2025-11-23 17:07:34 - __main__ - INFO - Scheduling event: 'Paper Review Meeting' for 60 min (afternoon)\n",
      "2025-11-23 17:07:34 - __main__ - INFO - Event scheduled: Paper Review Meeting on 2025-11-24 at 2:00 PM\n",
      "2025-11-23 17:07:34 - __main__ - INFO - Tool schedule_event_tool executed successfully\n",
      "2025-11-23 17:07:34 - __main__ - INFO - Tools node: Executed 1 tool(s)\n",
      "2025-11-23 17:07:35 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-11-23 17:07:35 - __main__ - INFO - Agent completed without tool calls\n",
      "2025-11-23 17:07:35 - __main__ - INFO - Response [thread=test-learning]: The paper review meeting has been scheduled for **...\n",
      "2025-11-23 17:07:35 - __main__ - INFO - Chat with learning [thread=test-learning]: Schedule another review meeting for next week...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "TURN 2\n",
      "============================================================\n",
      "USER: No, I prefer afternoon meetings\n",
      "============================================================\n",
      "AGENT: The paper review meeting has been scheduled for **November 24, 2025**, at **2:00 PM** for a duration of **60 minutes**.\n",
      "============================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-23 17:07:36 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-11-23 17:07:36 - __main__ - INFO - Agent requested 1 tool call(s)\n",
      "2025-11-23 17:07:36 - __main__ - INFO - Executing tool: schedule_event_tool with args: {'title': 'Paper Review Meeting', 'duration_min': 60, 'preferred_time': 'afternoon'}\n",
      "2025-11-23 17:07:36 - __main__ - INFO - Scheduling event: 'Paper Review Meeting' for 60 min (afternoon)\n",
      "2025-11-23 17:07:36 - __main__ - INFO - Event scheduled: Paper Review Meeting on 2025-11-24 at 2:00 PM\n",
      "2025-11-23 17:07:36 - __main__ - INFO - Tool schedule_event_tool executed successfully\n",
      "2025-11-23 17:07:36 - __main__ - INFO - Tools node: Executed 1 tool(s)\n",
      "2025-11-23 17:07:38 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-11-23 17:07:38 - __main__ - INFO - Agent requested 1 tool call(s)\n",
      "2025-11-23 17:07:38 - __main__ - INFO - Executing tool: schedule_event_tool with args: {'title': 'Paper Review Meeting', 'duration_min': 60, 'preferred_time': 'afternoon'}\n",
      "2025-11-23 17:07:38 - __main__ - INFO - Scheduling event: 'Paper Review Meeting' for 60 min (afternoon)\n",
      "2025-11-23 17:07:38 - __main__ - INFO - Event scheduled: Paper Review Meeting on 2025-11-24 at 2:00 PM\n",
      "2025-11-23 17:07:38 - __main__ - INFO - Tool schedule_event_tool executed successfully\n",
      "2025-11-23 17:07:38 - __main__ - INFO - Tools node: Executed 1 tool(s)\n",
      "2025-11-23 17:07:41 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-11-23 17:07:41 - __main__ - INFO - Agent requested 1 tool call(s)\n",
      "2025-11-23 17:07:41 - __main__ - INFO - Executing tool: schedule_event_tool with args: {'title': 'Paper Review Meeting', 'duration_min': 60, 'preferred_time': 'afternoon'}\n",
      "2025-11-23 17:07:41 - __main__ - INFO - Scheduling event: 'Paper Review Meeting' for 60 min (afternoon)\n",
      "2025-11-23 17:07:41 - __main__ - INFO - Event scheduled: Paper Review Meeting on 2025-11-24 at 2:00 PM\n",
      "2025-11-23 17:07:41 - __main__ - INFO - Tool schedule_event_tool executed successfully\n",
      "2025-11-23 17:07:41 - __main__ - INFO - Tools node: Executed 1 tool(s)\n",
      "2025-11-23 17:07:42 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-11-23 17:07:42 - __main__ - INFO - Agent completed without tool calls\n",
      "2025-11-23 17:07:42 - __main__ - INFO - Response [thread=test-learning]: It seems that I'm unable to schedule the meeting f...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "TURN 3\n",
      "============================================================\n",
      "USER: Schedule another review meeting for next week\n",
      "============================================================\n",
      "AGENT: It seems that I'm unable to schedule the meeting for next week, as it keeps reverting to the previous date. \n",
      "\n",
      "Would you like me to try a different approach or assist you with something else?\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "logger.info(\"=\"*60)\n",
    "logger.info(\"TEST 3: Learning from Corrections\")\n",
    "logger.info(\"=\"*60)\n",
    "\n",
    "config3 = {\"configurable\": {\"thread_id\": \"test-learning\"}}\n",
    "\n",
    "# Turn 1: Schedule with defaults\n",
    "response1 = chat_with_learning(\n",
    "    \"Schedule a paper review meeting for 60 minutes\",\n",
    "    config3\n",
    ")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"TURN 1\")\n",
    "print(\"=\"*60)\n",
    "print(\"USER: Schedule a paper review meeting for 60 minutes\")\n",
    "print(\"=\"*60)\n",
    "print(f\"AGENT: {response1}\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Turn 2: Correct preference\n",
    "response2 = chat_with_learning(\n",
    "    \"No, I prefer afternoon meetings\",\n",
    "    config3\n",
    ")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"TURN 2\")\n",
    "print(\"=\"*60)\n",
    "print(\"USER: No, I prefer afternoon meetings\")\n",
    "print(\"=\"*60)\n",
    "print(f\"AGENT: {response2}\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Turn 3: Apply learned preference\n",
    "response3 = chat_with_learning(\n",
    "    \"Schedule another review meeting for next week\",\n",
    "    config3\n",
    ")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"TURN 3\")\n",
    "print(\"=\"*60)\n",
    "print(\"USER: Schedule another review meeting for next week\")\n",
    "print(\"=\"*60)\n",
    "print(f\"AGENT: {response3}\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "challenges-header",
   "metadata": {},
   "source": [
    "## Challenges\n",
    "\n",
    "Now that you've built a production-ready research assistant, try these extensions:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "challenge1-header",
   "metadata": {},
   "source": [
    "## Challenge 1: Implement Priority-Based Task Queue\n",
    "\n",
    "### Goal\n",
    "Extend the agent to manage multiple research tasks with priorities.\n",
    "\n",
    "### Requirements\n",
    "\n",
    "1. Add `task_queue: List[Dict]` to `ResearchAssistantState`\n",
    "2. Each task should have:\n",
    "   - `title`: Task description\n",
    "   - `priority`: Integer (1=highest, 5=lowest)\n",
    "   - `status`: \"pending\", \"in_progress\", or \"completed\"\n",
    "3. Create a new tool: `add_task_tool(title, priority)`\n",
    "4. Create a new tool: `show_queue_tool()` that returns sorted queue\n",
    "5. Agent should process high-priority tasks first\n",
    "\n",
    "### Example Usage\n",
    "```\n",
    "User: \"Add reading paper X to my queue (high priority)\"\n",
    "Agent: \"Added to queue with priority 1\"\n",
    "\n",
    "User: \"What's in my queue?\"\n",
    "Agent: \"You have 3 tasks:\n",
    "  1. [Priority 1] Read paper X\n",
    "  2. [Priority 2] Review notes\n",
    "  3. [Priority 3] Update bibliography\"\n",
    "```\n",
    "\n",
    "### Hints\n",
    "\n",
    "- Sort queue by priority: `sorted(tasks, key=lambda t: t['priority'])`\n",
    "- Use Pydantic model for task structure\n",
    "- Add logging for all queue operations\n",
    "- Include error handling for invalid priorities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "challenge1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-23 17:07:42 - __main__ - INFO - Challenge 1 tools registered: add_task_tool, show_queue_tool\n",
      "2025-11-23 17:07:42 - __main__ - INFO - Tools rebound: 6 tools now available (including Challenge 1)\n"
     ]
    }
   ],
   "source": [
    "# Challenge 1 Solution: Priority-Based Task Queue\n",
    "\n",
    "# Shared task queue (in production: store in database or state)\n",
    "global_task_queue = []\n",
    "\n",
    "@tool\n",
    "def add_task_tool(title: str, priority: int, status: str = \"pending\") -> str:\n",
    "    \"\"\"Add a task to the priority-based task queue.\n",
    "    \n",
    "    Args:\n",
    "        title: Task description\n",
    "        priority: Priority level (1=highest, 5=lowest)\n",
    "        status: Task status ('pending', 'in_progress', 'completed')\n",
    "        \n",
    "    Returns:\n",
    "        JSON string of Task with confirmation\n",
    "    \"\"\"\n",
    "    logger.info(f\"Adding task: '{title}' (priority={priority}, status={status})\")\n",
    "    \n",
    "    try:\n",
    "        # Validate priority\n",
    "        if not 1 <= priority <= 5:\n",
    "            error_msg = f\"Invalid priority {priority}. Must be 1-5.\"\n",
    "            logger.error(error_msg)\n",
    "            return error_msg\n",
    "        \n",
    "        # Validate status\n",
    "        valid_statuses = [\"pending\", \"in_progress\", \"completed\"]\n",
    "        if status not in valid_statuses:\n",
    "            error_msg = f\"Invalid status '{status}'. Must be one of: {valid_statuses}\"\n",
    "            logger.error(error_msg)\n",
    "            return error_msg\n",
    "        \n",
    "        # Create task\n",
    "        task = Task(\n",
    "            title=title,\n",
    "            priority=priority,\n",
    "            status=status,\n",
    "            created_at=datetime.now().isoformat()\n",
    "        )\n",
    "        \n",
    "        # Add to queue\n",
    "        global_task_queue.append(task.model_dump())\n",
    "        \n",
    "        logger.info(f\"Task added: {title} (queue size: {len(global_task_queue)})\")\n",
    "        \n",
    "        return f\"✓ Task added successfully:\\n\\n{task.model_dump_json(indent=2)}\\n\\nTotal tasks in queue: {len(global_task_queue)}\"\n",
    "        \n",
    "    except Exception as e:\n",
    "        logger.error(f\"Failed to add task: {e}\")\n",
    "        return f\"Error adding task: {str(e)}\"\n",
    "\n",
    "\n",
    "@tool\n",
    "def show_queue_tool() -> str:\n",
    "    \"\"\"Show the task queue sorted by priority.\n",
    "    \n",
    "    Returns:\n",
    "        Formatted string with sorted task queue\n",
    "    \"\"\"\n",
    "    logger.info(f\"Showing task queue (total tasks: {len(global_task_queue)})\")\n",
    "    \n",
    "    try:\n",
    "        if not global_task_queue:\n",
    "            return \"Task queue is empty.\"\n",
    "        \n",
    "        # Sort by priority (1=highest)\n",
    "        sorted_tasks = sorted(global_task_queue, key=lambda t: t['priority'])\n",
    "        \n",
    "        result = f\"Task Queue ({len(sorted_tasks)} tasks):\\n\"\n",
    "        result += \"=\" * 60 + \"\\n\\n\"\n",
    "        \n",
    "        for i, task in enumerate(sorted_tasks, 1):\n",
    "            status_icon = {\n",
    "                \"pending\": \"⏳\",\n",
    "                \"in_progress\": \"🔄\",\n",
    "                \"completed\": \"✅\"\n",
    "            }.get(task['status'], \"❓\")\n",
    "            \n",
    "            result += f\"{i}. {status_icon} [Priority {task['priority']}] {task['title']}\\n\"\n",
    "            result += f\"   Status: {task['status']} | Created: {task['created_at'][:19]}\\n\\n\"\n",
    "        \n",
    "        logger.info(f\"Queue displayed with {len(sorted_tasks)} tasks\")\n",
    "        return result\n",
    "        \n",
    "    except Exception as e:\n",
    "        logger.error(f\"Failed to show queue: {e}\")\n",
    "        return f\"Error showing queue: {str(e)}\"\n",
    "\n",
    "\n",
    "logger.info(\"Challenge 1 tools registered: add_task_tool, show_queue_tool\")\n",
    "\n",
    "# Rebind tools with Challenge 1 additions\n",
    "tools = [\n",
    "    search_papers_tool,\n",
    "    organize_files_tool,\n",
    "    schedule_event_tool,\n",
    "    update_bibliography_tool,\n",
    "    add_task_tool,              # Challenge 1\n",
    "    show_queue_tool             # Challenge 1\n",
    "]\n",
    "\n",
    "llm_with_tools = llm.bind_tools(tools)\n",
    "logger.info(f\"Tools rebound: {len(tools)} tools now available (including Challenge 1)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "challenge1-test",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-23 17:07:42 - __main__ - INFO - ============================================================\n",
      "2025-11-23 17:07:42 - __main__ - INFO - CHALLENGE 1 TEST: Task Queue\n",
      "2025-11-23 17:07:42 - __main__ - INFO - ============================================================\n",
      "2025-11-23 17:07:42 - __main__ - INFO - Chat with learning [thread=test-task-queue]: Add 'Read paper on transformers' to my queue with ...\n",
      "2025-11-23 17:07:43 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-11-23 17:07:43 - __main__ - INFO - Agent requested 1 tool call(s)\n",
      "2025-11-23 17:07:43 - __main__ - INFO - Executing tool: add_task_tool with args: {'title': 'Read paper on transformers', 'priority': 1}\n",
      "2025-11-23 17:07:43 - __main__ - INFO - Adding task: 'Read paper on transformers' (priority=1, status=pending)\n",
      "2025-11-23 17:07:43 - __main__ - INFO - Task added: Read paper on transformers (queue size: 1)\n",
      "2025-11-23 17:07:43 - __main__ - INFO - Tool add_task_tool executed successfully\n",
      "2025-11-23 17:07:43 - __main__ - INFO - Tools node: Executed 1 tool(s)\n",
      "2025-11-23 17:07:43 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-11-23 17:07:43 - __main__ - INFO - Agent completed without tool calls\n",
      "2025-11-23 17:07:43 - __main__ - INFO - Response [thread=test-task-queue]: The task \"Read paper on transformers\" has been add...\n",
      "2025-11-23 17:07:43 - __main__ - INFO - Chat with learning [thread=test-task-queue]: Add 'Update bibliography' to my queue with priorit...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "TEST 1: Add High-Priority Task\n",
      "============================================================\n",
      "USER: Add 'Read paper on transformers' to my queue with high priority (priority 1)\n",
      "============================================================\n",
      "AGENT: The task \"Read paper on transformers\" has been added to your queue with high priority (priority 1).\n",
      "============================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-23 17:07:44 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-11-23 17:07:44 - __main__ - INFO - Agent requested 1 tool call(s)\n",
      "2025-11-23 17:07:44 - __main__ - INFO - Executing tool: add_task_tool with args: {'title': 'Update bibliography', 'priority': 3}\n",
      "2025-11-23 17:07:44 - __main__ - INFO - Adding task: 'Update bibliography' (priority=3, status=pending)\n",
      "2025-11-23 17:07:44 - __main__ - INFO - Task added: Update bibliography (queue size: 2)\n",
      "2025-11-23 17:07:44 - __main__ - INFO - Tool add_task_tool executed successfully\n",
      "2025-11-23 17:07:44 - __main__ - INFO - Tools node: Executed 1 tool(s)\n",
      "2025-11-23 17:07:45 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-11-23 17:07:45 - __main__ - INFO - Agent completed without tool calls\n",
      "2025-11-23 17:07:45 - __main__ - INFO - Response [thread=test-task-queue]: The task \"Update bibliography\" has been added to y...\n",
      "2025-11-23 17:07:45 - __main__ - INFO - Chat with learning [thread=test-task-queue]: Add 'Organize files' with priority 5...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "TEST 2: Add Medium-Priority Task\n",
      "============================================================\n",
      "USER: Add 'Update bibliography' to my queue with priority 3\n",
      "============================================================\n",
      "AGENT: The task \"Update bibliography\" has been added to your queue with priority 3.\n",
      "============================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-23 17:07:46 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-11-23 17:07:46 - __main__ - INFO - Agent requested 1 tool call(s)\n",
      "2025-11-23 17:07:46 - __main__ - INFO - Executing tool: add_task_tool with args: {'title': 'Organize files', 'priority': 5}\n",
      "2025-11-23 17:07:46 - __main__ - INFO - Adding task: 'Organize files' (priority=5, status=pending)\n",
      "2025-11-23 17:07:46 - __main__ - INFO - Task added: Organize files (queue size: 3)\n",
      "2025-11-23 17:07:46 - __main__ - INFO - Tool add_task_tool executed successfully\n",
      "2025-11-23 17:07:46 - __main__ - INFO - Tools node: Executed 1 tool(s)\n",
      "2025-11-23 17:07:46 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-11-23 17:07:46 - __main__ - INFO - Agent completed without tool calls\n",
      "2025-11-23 17:07:46 - __main__ - INFO - Response [thread=test-task-queue]: The task \"Organize files\" has been added to your q...\n",
      "2025-11-23 17:07:46 - __main__ - INFO - Chat with learning [thread=test-task-queue]: Show me my task queue...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "TEST 3: Add Low-Priority Task\n",
      "============================================================\n",
      "USER: Add 'Organize files' with priority 5\n",
      "============================================================\n",
      "AGENT: The task \"Organize files\" has been added to your queue with priority 5.\n",
      "============================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-23 17:07:47 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-11-23 17:07:47 - __main__ - INFO - Agent requested 1 tool call(s)\n",
      "2025-11-23 17:07:47 - __main__ - INFO - Executing tool: show_queue_tool with args: {}\n",
      "2025-11-23 17:07:47 - __main__ - INFO - Showing task queue (total tasks: 3)\n",
      "2025-11-23 17:07:47 - __main__ - INFO - Queue displayed with 3 tasks\n",
      "2025-11-23 17:07:47 - __main__ - INFO - Tool show_queue_tool executed successfully\n",
      "2025-11-23 17:07:47 - __main__ - INFO - Tools node: Executed 1 tool(s)\n",
      "2025-11-23 17:07:49 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-11-23 17:07:49 - __main__ - INFO - Agent completed without tool calls\n",
      "2025-11-23 17:07:49 - __main__ - INFO - Response [thread=test-task-queue]: Here is your task queue:\n",
      "\n",
      "1. ⏳ [Priority 1] Read p...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "TEST 4: Show Sorted Queue\n",
      "============================================================\n",
      "USER: Show me my task queue\n",
      "============================================================\n",
      "AGENT: Here is your task queue:\n",
      "\n",
      "1. ⏳ [Priority 1] Read paper on transformers  \n",
      "   Status: pending | Created: 2025-11-23T17:07:43\n",
      "\n",
      "2. ⏳ [Priority 3] Update bibliography  \n",
      "   Status: pending | Created: 2025-11-23T17:07:44\n",
      "\n",
      "3. ⏳ [Priority 5] Organize files  \n",
      "   Status: pending | Created: 2025-11-23T17:07:46\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "logger.info(\"=\"*60)\n",
    "logger.info(\"CHALLENGE 1 TEST: Task Queue\")\n",
    "logger.info(\"=\"*60)\n",
    "\n",
    "config_queue = {\"configurable\": {\"thread_id\": \"test-task-queue\"}}\n",
    "\n",
    "# Test 1: Add high-priority task\n",
    "response1 = chat_with_learning(\n",
    "    \"Add 'Read paper on transformers' to my queue with high priority (priority 1)\",\n",
    "    config_queue\n",
    ")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"TEST 1: Add High-Priority Task\")\n",
    "print(\"=\"*60)\n",
    "print(\"USER: Add 'Read paper on transformers' to my queue with high priority (priority 1)\")\n",
    "print(\"=\"*60)\n",
    "print(f\"AGENT: {response1}\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Test 2: Add medium-priority task\n",
    "response2 = chat_with_learning(\n",
    "    \"Add 'Update bibliography' to my queue with priority 3\",\n",
    "    config_queue\n",
    ")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"TEST 2: Add Medium-Priority Task\")\n",
    "print(\"=\"*60)\n",
    "print(\"USER: Add 'Update bibliography' to my queue with priority 3\")\n",
    "print(\"=\"*60)\n",
    "print(f\"AGENT: {response2}\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Test 3: Add low-priority task\n",
    "response3 = chat_with_learning(\n",
    "    \"Add 'Organize files' with priority 5\",\n",
    "    config_queue\n",
    ")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"TEST 3: Add Low-Priority Task\")\n",
    "print(\"=\"*60)\n",
    "print(\"USER: Add 'Organize files' with priority 5\")\n",
    "print(\"=\"*60)\n",
    "print(f\"AGENT: {response3}\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Test 4: Show queue (should be sorted by priority)\n",
    "response4 = chat_with_learning(\n",
    "    \"Show me my task queue\",\n",
    "    config_queue\n",
    ")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"TEST 4: Show Sorted Queue\")\n",
    "print(\"=\"*60)\n",
    "print(\"USER: Show me my task queue\")\n",
    "print(\"=\"*60)\n",
    "print(f\"AGENT: {response4}\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "challenge2-header",
   "metadata": {},
   "source": [
    "## Challenge 2: Smart Scheduling with Conflict Detection\n",
    "\n",
    "### Goal\n",
    "Make the calendar manager detect conflicts and suggest alternatives.\n",
    "\n",
    "### Requirements\n",
    "\n",
    "1. Maintain a list of scheduled events in state: `scheduled_events: List[CalendarEvent]`\n",
    "2. Enhance `schedule_event_tool` to:\n",
    "   - Check for time conflicts with existing events\n",
    "   - If conflict detected, find next available slot\n",
    "   - Respect user's preferred time-of-day\n",
    "   - Return suggestion: \"Conflict at 2pm. Next available: 3pm. Shall I book it?\"\n",
    "3. Add conflict resolution logic:\n",
    "   - Check ±30 minutes around requested time\n",
    "   - Suggest same day if possible, next day otherwise\n",
    "\n",
    "### Example Usage\n",
    "```\n",
    "User: \"Schedule meeting for 2pm\"\n",
    "Agent: \"Conflict detected at 2pm (already have 'Paper review'). \n",
    "        Next available afternoon slot: 3:30pm. Would you like to book it?\"\n",
    "\n",
    "User: \"Yes\"\n",
    "Agent: \"Scheduled for 3:30pm\"\n",
    "```\n",
    "\n",
    "### Hints\n",
    "\n",
    "- Convert times to `datetime` for comparison\n",
    "- Check overlaps: `new_start < existing_end and new_end > existing_start`\n",
    "- Increment time in 30-minute intervals to find next slot\n",
    "- Use logging to track conflict detection\n",
    "- Add error handling for invalid times"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "challenge2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-23 17:07:49 - __main__ - INFO - Challenge 2 tools registered: schedule_event_smart_tool\n",
      "2025-11-23 17:07:49 - __main__ - INFO - Tools rebound: 7 tools now available (including Challenge 1 + 2)\n",
      "2025-11-23 17:07:49 - __main__ - INFO - Agent graph recompiled with all 7 tools\n"
     ]
    }
   ],
   "source": [
    "# Challenge 2 Solution: Smart Scheduling with Conflict Detection\n",
    "\n",
    "# Shared calendar (in production: use Google Calendar API)\n",
    "global_calendar = []\n",
    "\n",
    "def parse_time(time_str: str) -> datetime:\n",
    "    \"\"\"Parse time string to datetime.\n",
    "    \n",
    "    Args:\n",
    "        time_str: Time in format 'HH:MM AM/PM'\n",
    "        \n",
    "    Returns:\n",
    "        datetime object\n",
    "    \"\"\"\n",
    "    try:\n",
    "        return datetime.strptime(time_str, \"%I:%M %p\")\n",
    "    except ValueError:\n",
    "        # Fallback: try without AM/PM\n",
    "        return datetime.strptime(time_str, \"%H:%M\")\n",
    "\n",
    "\n",
    "def has_conflict(new_start: datetime, new_duration_min: int, existing_events: List[Dict]) -> tuple:\n",
    "    \"\"\"Check if new event conflicts with existing events.\n",
    "    \n",
    "    Args:\n",
    "        new_start: Start time of new event\n",
    "        new_duration_min: Duration in minutes\n",
    "        existing_events: List of existing calendar events\n",
    "        \n",
    "    Returns:\n",
    "        Tuple of (has_conflict: bool, conflicting_event: Optional[Dict])\n",
    "    \"\"\"\n",
    "    new_end = new_start + timedelta(minutes=new_duration_min)\n",
    "    \n",
    "    for event in existing_events:\n",
    "        event_start = parse_time(event['time'])\n",
    "        event_end = event_start + timedelta(minutes=event['duration_min'])\n",
    "        \n",
    "        # Check overlap: new_start < existing_end AND new_end > existing_start\n",
    "        if new_start < event_end and new_end > event_start:\n",
    "            logger.info(f\"Conflict detected with '{event['title']}' at {event['time']}\")\n",
    "            return True, event\n",
    "    \n",
    "    return False, None\n",
    "\n",
    "\n",
    "def find_next_slot(preferred_time: str, duration_min: int, existing_events: List[Dict]) -> Optional[str]:\n",
    "    \"\"\"Find next available time slot.\n",
    "    \n",
    "    Args:\n",
    "        preferred_time: 'morning', 'afternoon', or 'evening'\n",
    "        duration_min: Required duration in minutes\n",
    "        existing_events: List of existing calendar events\n",
    "        \n",
    "    Returns:\n",
    "        Time string for next available slot or None\n",
    "    \"\"\"\n",
    "    # Time ranges for each period\n",
    "    time_ranges = {\n",
    "        \"morning\": (9, 12),    # 9am-12pm\n",
    "        \"afternoon\": (13, 17), # 1pm-5pm\n",
    "        \"evening\": (17, 20)    # 5pm-8pm\n",
    "    }\n",
    "    \n",
    "    start_hour, end_hour = time_ranges.get(preferred_time.lower(), (9, 12))\n",
    "    \n",
    "    # Try 30-minute increments\n",
    "    for hour in range(start_hour, end_hour):\n",
    "        for minute in [0, 30]:\n",
    "            test_time = datetime.now().replace(hour=hour, minute=minute, second=0, microsecond=0)\n",
    "            \n",
    "            # Check if this slot is free\n",
    "            conflict, _ = has_conflict(test_time, duration_min, existing_events)\n",
    "            \n",
    "            if not conflict:\n",
    "                return test_time.strftime(\"%I:%M %p\")\n",
    "    \n",
    "    return None\n",
    "\n",
    "\n",
    "@tool\n",
    "def schedule_event_smart_tool(title: str, duration_min: int, preferred_time: str = \"morning\", date: str = None) -> str:\n",
    "    \"\"\"Schedule an event with conflict detection and smart suggestions.\n",
    "    \n",
    "    Args:\n",
    "        title: Event title\n",
    "        duration_min: Duration in minutes\n",
    "        preferred_time: 'morning', 'afternoon', or 'evening' (or specific time 'HH:MM AM/PM')\n",
    "        date: Specific date (YYYY-MM-DD) or None for tomorrow\n",
    "        \n",
    "    Returns:\n",
    "        JSON string of ConflictCheckResult with scheduling details\n",
    "    \"\"\"\n",
    "    logger.info(f\"Smart scheduling: '{title}' for {duration_min} min ({preferred_time})\")\n",
    "    \n",
    "    try:\n",
    "        # Determine date\n",
    "        if date is None:\n",
    "            target_date = (datetime.now() + timedelta(days=1)).strftime(\"%Y-%m-%d\")\n",
    "        else:\n",
    "            target_date = date\n",
    "        \n",
    "        # Map preferred time to actual time\n",
    "        time_map = {\n",
    "            \"morning\": \"10:00 AM\",\n",
    "            \"afternoon\": \"2:00 PM\",\n",
    "            \"evening\": \"5:00 PM\"\n",
    "        }\n",
    "        \n",
    "        if preferred_time.lower() in time_map:\n",
    "            requested_time = time_map[preferred_time.lower()]\n",
    "        else:\n",
    "            requested_time = preferred_time\n",
    "        \n",
    "        # Check for conflicts\n",
    "        requested_start = parse_time(requested_time)\n",
    "        conflict, conflicting_event = has_conflict(requested_start, duration_min, global_calendar)\n",
    "        \n",
    "        if conflict:\n",
    "            # Find next available slot\n",
    "            time_period = preferred_time.lower() if preferred_time.lower() in time_map else \"afternoon\"\n",
    "            next_slot = find_next_slot(time_period, duration_min, global_calendar)\n",
    "            \n",
    "            if next_slot:\n",
    "                result = ConflictCheckResult(\n",
    "                    has_conflict=True,\n",
    "                    conflicting_event=conflicting_event['title'],\n",
    "                    suggested_time=next_slot,\n",
    "                    message=f\"Conflict detected at {requested_time} with '{conflicting_event['title']}'. Next available {time_period} slot: {next_slot}. Would you like to book it?\"\n",
    "                )\n",
    "            else:\n",
    "                result = ConflictCheckResult(\n",
    "                    has_conflict=True,\n",
    "                    conflicting_event=conflicting_event['title'],\n",
    "                    suggested_time=None,\n",
    "                    message=f\"Conflict detected at {requested_time} with '{conflicting_event['title']}'. No available slots in {time_period}. Try a different time period or date.\"\n",
    "                )\n",
    "            \n",
    "            logger.info(f\"Conflict detected, suggested alternative: {next_slot}\")\n",
    "            return result.model_dump_json(indent=2)\n",
    "        \n",
    "        else:\n",
    "            # No conflict, schedule the event\n",
    "            new_event = {\n",
    "                \"title\": title,\n",
    "                \"date\": target_date,\n",
    "                \"time\": requested_time,\n",
    "                \"duration_min\": duration_min\n",
    "            }\n",
    "            \n",
    "            global_calendar.append(new_event)\n",
    "            \n",
    "            result = ConflictCheckResult(\n",
    "                has_conflict=False,\n",
    "                conflicting_event=None,\n",
    "                suggested_time=None,\n",
    "                message=f\"✓ Event '{title}' scheduled successfully on {target_date} at {requested_time} for {duration_min} minutes.\"\n",
    "            )\n",
    "            \n",
    "            logger.info(f\"Event scheduled: {title} on {target_date} at {requested_time}\")\n",
    "            return result.model_dump_json(indent=2)\n",
    "        \n",
    "    except Exception as e:\n",
    "        logger.error(f\"Smart scheduling failed: {e}\")\n",
    "        return f\"Error scheduling event: {str(e)}\"\n",
    "\n",
    "\n",
    "logger.info(\"Challenge 2 tools registered: schedule_event_smart_tool\")\n",
    "\n",
    "# Rebind tools with Challenge 2 additions\n",
    "tools = [\n",
    "    search_papers_tool,\n",
    "    organize_files_tool,\n",
    "    schedule_event_tool,\n",
    "    update_bibliography_tool,\n",
    "    add_task_tool,              # Challenge 1\n",
    "    show_queue_tool,            # Challenge 1\n",
    "    schedule_event_smart_tool   # Challenge 2\n",
    "]\n",
    "\n",
    "llm_with_tools = llm.bind_tools(tools)\n",
    "logger.info(f\"Tools rebound: {len(tools)} tools now available (including Challenge 1 + 2)\")\n",
    "\n",
    "# Rebuild graph with updated tools\n",
    "builder = StateGraph(ResearchAssistantState)\n",
    "builder.add_node(\"agent\", agent_node)\n",
    "builder.add_node(\"tools\", tool_node)\n",
    "builder.add_edge(START, \"agent\")\n",
    "builder.add_conditional_edges(\"agent\", should_continue, {\"tools\": \"tools\", END: END})\n",
    "builder.add_edge(\"tools\", \"agent\")\n",
    "agent = builder.compile(checkpointer=memory)\n",
    "logger.info(\"Agent graph recompiled with all 7 tools\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "challenge2-test",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-23 17:07:49 - __main__ - INFO - ============================================================\n",
      "2025-11-23 17:07:49 - __main__ - INFO - CHALLENGE 2 TEST: Conflict Detection\n",
      "2025-11-23 17:07:49 - __main__ - INFO - ============================================================\n",
      "2025-11-23 17:07:49 - __main__ - INFO - Chat with learning [thread=test-conflicts]: Schedule a paper review meeting for 60 minutes in ...\n",
      "2025-11-23 17:07:49 - __main__ - INFO - Detected preference: afternoon meetings\n",
      "2025-11-23 17:07:49 - __main__ - INFO - Detected preference: 60min duration\n",
      "2025-11-23 17:07:49 - __main__ - INFO - Updated preferences [thread=test-conflicts]: {'meeting_time': 'afternoon', 'reading_duration': '60min'}\n",
      "2025-11-23 17:07:51 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-11-23 17:07:51 - __main__ - INFO - Agent requested 1 tool call(s)\n",
      "2025-11-23 17:07:51 - __main__ - INFO - Executing tool: schedule_event_smart_tool with args: {'title': 'Paper Review Meeting', 'duration_min': 60, 'preferred_time': 'afternoon'}\n",
      "2025-11-23 17:07:51 - __main__ - INFO - Smart scheduling: 'Paper Review Meeting' for 60 min (afternoon)\n",
      "2025-11-23 17:07:51 - __main__ - INFO - Event scheduled: Paper Review Meeting on 2025-11-24 at 2:00 PM\n",
      "2025-11-23 17:07:51 - __main__ - INFO - Tool schedule_event_smart_tool executed successfully\n",
      "2025-11-23 17:07:51 - __main__ - INFO - Tools node: Executed 1 tool(s)\n",
      "2025-11-23 17:07:52 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-11-23 17:07:52 - __main__ - INFO - Agent completed without tool calls\n",
      "2025-11-23 17:07:52 - __main__ - INFO - Response [thread=test-conflicts]: The paper review meeting has been successfully sch...\n",
      "2025-11-23 17:07:52 - __main__ - INFO - Chat with learning [thread=test-conflicts]: Schedule a research discussion for 45 minutes in t...\n",
      "2025-11-23 17:07:52 - __main__ - INFO - Detected preference: afternoon meetings\n",
      "2025-11-23 17:07:52 - __main__ - INFO - Updated preferences [thread=test-conflicts]: {'meeting_time': 'afternoon'}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "TEST 1: First Event (No Conflict)\n",
      "============================================================\n",
      "USER: Schedule a paper review meeting for 60 minutes in the afternoon using the smart scheduler\n",
      "============================================================\n",
      "AGENT: The paper review meeting has been successfully scheduled for **November 24, 2025**, at **2:00 PM** for a duration of **60 minutes**.\n",
      "============================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-23 17:07:53 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-11-23 17:07:53 - __main__ - INFO - Agent requested 1 tool call(s)\n",
      "2025-11-23 17:07:53 - __main__ - INFO - Executing tool: schedule_event_smart_tool with args: {'title': 'Research Discussion', 'duration_min': 45, 'preferred_time': 'afternoon'}\n",
      "2025-11-23 17:07:53 - __main__ - INFO - Smart scheduling: 'Research Discussion' for 45 min (afternoon)\n",
      "2025-11-23 17:07:53 - __main__ - INFO - Conflict detected with 'Paper Review Meeting' at 2:00 PM\n",
      "2025-11-23 17:07:53 - __main__ - INFO - Conflict detected, suggested alternative: 01:00 PM\n",
      "2025-11-23 17:07:53 - __main__ - INFO - Tool schedule_event_smart_tool executed successfully\n",
      "2025-11-23 17:07:53 - __main__ - INFO - Tools node: Executed 1 tool(s)\n",
      "2025-11-23 17:07:54 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-11-23 17:07:54 - __main__ - INFO - Agent completed without tool calls\n",
      "2025-11-23 17:07:54 - __main__ - INFO - Response [thread=test-conflicts]: There is a conflict with the scheduled paper revie...\n",
      "2025-11-23 17:07:54 - __main__ - INFO - Chat with learning [thread=test-conflicts]: Schedule reading time for 30 minutes in the mornin...\n",
      "2025-11-23 17:07:54 - __main__ - INFO - Detected preference: 30min duration\n",
      "2025-11-23 17:07:54 - __main__ - INFO - Updated preferences [thread=test-conflicts]: {'reading_duration': '30min'}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "TEST 2: Conflicting Event\n",
      "============================================================\n",
      "USER: Schedule a research discussion for 45 minutes in the afternoon using the smart scheduler\n",
      "============================================================\n",
      "AGENT: There is a conflict with the scheduled paper review meeting at **2:00 PM**. The next available afternoon slot for the research discussion is **1:00 PM**. Would you like to book it for that time?\n",
      "============================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-23 17:07:55 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-11-23 17:07:55 - __main__ - INFO - Agent requested 1 tool call(s)\n",
      "2025-11-23 17:07:55 - __main__ - INFO - Executing tool: schedule_event_smart_tool with args: {'title': 'Reading Time', 'duration_min': 30, 'preferred_time': 'morning'}\n",
      "2025-11-23 17:07:55 - __main__ - INFO - Smart scheduling: 'Reading Time' for 30 min (morning)\n",
      "2025-11-23 17:07:55 - __main__ - INFO - Event scheduled: Reading Time on 2025-11-24 at 10:00 AM\n",
      "2025-11-23 17:07:55 - __main__ - INFO - Tool schedule_event_smart_tool executed successfully\n",
      "2025-11-23 17:07:55 - __main__ - INFO - Tools node: Executed 1 tool(s)\n",
      "2025-11-23 17:07:56 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-11-23 17:07:56 - __main__ - INFO - Agent completed without tool calls\n",
      "2025-11-23 17:07:56 - __main__ - INFO - Response [thread=test-conflicts]: The reading time has been successfully scheduled f...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "TEST 3: Different Time Slot\n",
      "============================================================\n",
      "USER: Schedule reading time for 30 minutes in the morning using the smart scheduler\n",
      "============================================================\n",
      "AGENT: The reading time has been successfully scheduled for **November 24, 2025**, at **10:00 AM** for a duration of **30 minutes**.\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "logger.info(\"=\"*60)\n",
    "logger.info(\"CHALLENGE 2 TEST: Conflict Detection\")\n",
    "logger.info(\"=\"*60)\n",
    "\n",
    "config_conflict = {\"configurable\": {\"thread_id\": \"test-conflicts\"}}\n",
    "\n",
    "# Test 1: Schedule first event (no conflict)\n",
    "response1 = chat_with_learning(\n",
    "    \"Schedule a paper review meeting for 60 minutes in the afternoon using the smart scheduler\",\n",
    "    config_conflict\n",
    ")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"TEST 1: First Event (No Conflict)\")\n",
    "print(\"=\"*60)\n",
    "print(\"USER: Schedule a paper review meeting for 60 minutes in the afternoon using the smart scheduler\")\n",
    "print(\"=\"*60)\n",
    "print(f\"AGENT: {response1}\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Test 2: Schedule conflicting event (should detect and suggest alternative)\n",
    "response2 = chat_with_learning(\n",
    "    \"Schedule a research discussion for 45 minutes in the afternoon using the smart scheduler\",\n",
    "    config_conflict\n",
    ")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"TEST 2: Conflicting Event\")\n",
    "print(\"=\"*60)\n",
    "print(\"USER: Schedule a research discussion for 45 minutes in the afternoon using the smart scheduler\")\n",
    "print(\"=\"*60)\n",
    "print(f\"AGENT: {response2}\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Test 3: Schedule at suggested time (should work)\n",
    "response3 = chat_with_learning(\n",
    "    \"Schedule reading time for 30 minutes in the morning using the smart scheduler\",\n",
    "    config_conflict\n",
    ")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"TEST 3: Different Time Slot\")\n",
    "print(\"=\"*60)\n",
    "print(\"USER: Schedule reading time for 30 minutes in the morning using the smart scheduler\")\n",
    "print(\"=\"*60)\n",
    "print(f\"AGENT: {response3}\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "summary-header",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "### What You Built\n",
    "\n",
    "You've created a **production-ready single-agent system** using **LangGraph 1.0 + LangChain 1.0** that:\n",
    "\n",
    "✅ **Integrates multiple tools** in a unified architecture  \n",
    "✅ **Maintains conversational context** across multiple turns  \n",
    "✅ **Learns from user feedback** to improve over time  \n",
    "✅ **Handles errors gracefully** with comprehensive logging  \n",
    "✅ **Persists state** across sessions with thread-based memory  \n",
    "✅ **Uses structured outputs** with Pydantic for type safety  \n",
    "✅ **Visualizes workflows** with graph diagrams  \n",
    "\n",
    "### Version 1.0 Compatibility\n",
    "\n",
    "This lab is fully compatible with **LangGraph 1.0.3+ and LangChain 1.0+**:\n",
    "- ✅ Zero breaking changes from LangGraph 0.6.6\n",
    "- ✅ All APIs backward compatible\n",
    "- ✅ Works with latest OpenAI models (gpt-4o-mini)\n",
    "- ✅ Python 3.10+ required (LangChain 1.0 requirement)\n",
    "\n",
    "### Production Patterns You Learned\n",
    "\n",
    "1. **Logging**: Replace `print()` with proper logging levels\n",
    "2. **Error handling**: Try/except blocks in all tool operations\n",
    "3. **Structured outputs**: Pydantic models for type safety\n",
    "4. **Graph visualization**: Understanding agent flow\n",
    "5. **Thread isolation**: Session management with thread_id\n",
    "6. **Adaptive learning**: Detecting and applying user corrections\n",
    "\n",
    "### When to Use Single-Agent Architecture\n",
    "\n",
    "✅ **Good for**:\n",
    "- Related capabilities sharing context\n",
    "- Conversational user interaction\n",
    "- Centralized state management\n",
    "- Learning from user behavior\n",
    "\n",
    "❌ **Not ideal for**:\n",
    "- Highly specialized, independent tasks\n",
    "- Parallel execution required\n",
    "- Different reasoning strategies per task\n",
    "- Clear separation of concerns (use multi-agent instead)\n",
    "\n",
    "### Next Steps\n",
    "\n",
    "- **Lab 2**: Build a Data Analysis Agent with code generation\n",
    "- **Lab 3**: Add production-grade error recovery and retries\n",
    "- **Lab 4**: Implement multi-agent collaboration\n",
    "- **Capstone**: Integrate all patterns into a complete system\n",
    "\n",
    "### Key Takeaways\n",
    "\n",
    "1. **Always log, never print**: Logging is essential for debugging production systems\n",
    "2. **Handle all errors**: Every tool call should be wrapped in try/except\n",
    "3. **Use structured outputs**: Pydantic ensures type safety and validation\n",
    "4. **Visualize your graphs**: Understanding flow is crucial for debugging\n",
    "5. **Test thoroughly**: Multi-turn conversations reveal edge cases\n",
    "6. **Document thread_id usage**: Session isolation prevents data leaks\n",
    "\n",
    "**Great work!** You've built a real-world agent with production engineering practices using the latest stable versions of LangGraph and LangChain."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
